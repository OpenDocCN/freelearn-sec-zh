# 第二章：Kali Linux 和 ELK 堆栈

现在我们已经基本了解了网络安全作为一个专业领域的演变，让我们开始逐步拆解 Kali Purple 工具集。你会记得我们之前解释的红色和蓝色在色轮上会混合成紫色。这是因为 Kali Purple 的血统是一个双重用途的工具集，来自两套技术工具，一套与**红队**相关，另一套与**蓝队**相关。我们在上一章提供了每个分组的概述。那些工具列表并不是一个详尽无遗的工具清单——只是重点展示了一些工具。

在本章中，我们将简要解释 Kali Linux，供那些可能第一次接触 Linux 的读者了解。Kali Purple 的防御性安全功能已经成为一种流行现象，促使一些人在职业生涯中第一次接触 Linux**操作系统**（**OS**）。然而，大多数阅读本书的人可能已经有 Linux 经验，甚至有些人可能有 Kali 经验。因此，在简要介绍 Kali Linux 时，我们将主要集中于解构**安全信息与事件管理**（**SIEM**）系统的初步元素——**ELK 堆栈**。

**ELK**代表**Elasticsearch、Logstash 和 Kibana**。它们是三款独特的开源软件，通常组合在一起，用于与日志管理、数据存储、搜索/查询、数据分析和数据可视化相关的技术解决方案。它们本身并不定义一个 SIEM。然而，当与其他一些核心组件结合使用时，如**Beats**和**X-Pack**，ELK 堆栈将为我们提供我们正在追求的信息和事件管理系统。

Kali Purple 中有多个工具可用，但我们将重点关注这个工具集中的主要强大引擎——分布式分析引擎 Elasticsearch。为了理解我们将使用的工具，我们需要关注以下 Elastic 元素：

+   Elasticsearch

+   Logstash

+   Kibana

+   Beats

+   X-Pack

由于 SIEM 是你在 SOC 中找到的核心解决方案，因此我们必须充分探索并理解这个工具，了解它如何工作，它如何以及从哪里获取数据，以及我们如何通过不同的方式处理这些数据以实现我们的安全目标。典型的 SIEM 会从多个其他来源获取数据，对其进行标准化、丰富、组织和存储，然后以统一的格式呈现给网络安全分析员进行人工评估和行动。组织依赖它来集中管理防御性安全操作。在本章结束时，你将对这些概念有一个扎实的理解。

在接下来的三章中，这些理解将派上用场，因为我们将在准备 Kali Purple 的技术时，接着是获取、安装和配置我们的技术以及 Kali Purple 本身。了解 ELK 堆栈与 Kali Linux 环境相关的数据流将为你提供基础，以应对在我们解包*SOC-in-a-Box*解决方案时可能出现的任何异常。

本章涵盖以下内容：

+   Kali Linux 的演变

+   Elasticsearch、Logstash 和 Kibana（ELK 堆栈）

+   代理和监控

# Kali Linux 的演变

尽管是一项突破性技术，Kali Purple 的现代根源追溯到 1969 年。正是在 AT&T 的贝尔实验室，Ken Thompson 和 Dennis Ritchie 共同创造了 UNIX 操作系统。Ritchie 也因创造 C 编程语言而闻名。几乎所有现代计算技术都归功于 Ritchie 的两项重大贡献。最初的 Windows 操作系统就是用 C 语言编写的。UNIX 最终以开源和商业版本的形式发布。商业 UNIX 的两个最大客户是美国**国防部**（**DOD**）和——请鼓掌——苹果公司。没错，Mac OS X 就是基于 UNIX 构建的。那就只剩下一个主要的角色了……Linux。

芬兰赫尔辛基大学的计算机科学家 Linus Torvalds 拿到了一个开源版本的 UNIX，并开始加入自己的风格，创建一个全新的基于 UNIX 的操作系统。他随后公开发布了这个操作系统，免费供人使用。*Linus + UNIX = Linux*。由于 Torvalds 先生发布了代码并允许任何人修改和重新发布自己的版本，Linux 迅速流行起来，今天世界上有无数种 Linux 操作系统版本。

当 Dennis Ritchie 离开贝尔实验室去加利福尼亚大学伯克利分校工作时，他第二次加强了 Kali Linux 的基因。正是在那里，他最终创建了 UNIX 的一个新版本，命名为**伯克利软件分发版**（**BSD**）。正是这个版本的 UNIX，Debian Linux——基于 Torvalds 最初贡献的操作系统——用来增强其强大的操作系统。

享有盛誉的美国信息安全公司 Offensive Security 资助了 Kali Linux 的开发，以支持通过数字取证和渗透测试的道德网络安全需求。Offensive Security 的员工 Mati Aharoni 和 Devon Kearns 将公司之前的一个 Linux 发行版进行了重新设计，创造出了现在被称为 Kali Linux 的 Debian Linux 衍生版。

如前所述，SOC 的旗舰产品是 SIEM，而 Kali Linux 通过在 2022 年 12 月发布的 Kali Purple 为我们提供了将其整合在一起的工具。让我们开始检查这些工具中最强大的——ELK 堆栈。

# Elasticsearch、Logstash 和 Kibana（ELK 堆栈）

ELK 堆栈是指三个软件组件，它们协同工作以增强、索引和可视化数据进行分析。有些人会将 Beats 系列数据收集代理视为 ELK 堆栈的一部分。我们将在下一节中介绍 Beats。

## Elasticsearch

Elasticsearch 是 ELK 堆栈的核心引擎。正如我们稍后会讨论的，Elasticsearch 从技术上来说是一种非关系型 NoSQL 数据库。如果没有有效地搜索我们收集的数据，我们怎么可能判断是否有什么内容需要发出警报呢？Kibana 将无法继续前进，无法正确展示它提供的可视化威胁。这是 ELK 堆栈中一个非常强大的组件，因为它设计用于处理在大量查询频繁发生的情况下的大量数据。

Elasticsearch 通过使用一种被称为**分片**（sharding）的技术来实现这一目标。这是数据库术语，简单来说，就是通过将数据拆分成小块来结构化和组织数据。这使得该技术能够将数据的各个部分水平分布到多个分区/节点和服务器上，同时通过索引保持与相似数据的关系。这些较小的数据块称为分片。数据被分割成分片的过程发生在索引过程中。

创建这些分片的价值在于，Elasticsearch 可以使用一种叫做**并行处理**的技术来同时访问多个分片。并行处理是指多个处理单元——例如 CPU——在多个节点上并行工作。它的效果就像是有多台计算机同时处理同一任务，从而加快完成工作的速度。这通常应用于数据库技术中。

Elasticsearch 提供的一个有益功能是支持创建分片的副本——**副本**（replicas）。顾名思义，这些是原始分片的副本。这样做是为了支持网络安全框架中的*可用性*组件，通过创建冗余，从而实现容错。它通过将副本分片放置在数据集群的不同节点中来创建这一安全网。就像原始分片在索引过程中创建一样，副本的创建也是如此。

虽然对于不熟悉搜索技术的人来说，可能听起来不算什么值得吹嘘的事情，但 Elasticsearch 提供了**全文搜索**功能。这可是大新闻！许多搜索/查询技术仅基于匹配的关键词或嵌入的元数据进行搜索。全文搜索意味着 Elasticsearch 会根据文档和文本的完整内容进行搜索并检索结果。这意味着需要考虑文档的全部内容。

Elasticsearch 利用六个关键特性来支持这种搜索方式：

+   **分词**：分词在应用于全文搜索时，也可以被视为一种文本清理方式。其含义是，Elasticsearch 将文本拆分为单独的单元，并将这些小单元或词元存储在倒排索引中——有时也叫做**倒排索引**。这样做的目的是允许基于关键词或简单术语进行搜索。分词过程的一部分包括将所有大写字符转换为小写字符、去除标点符号，并将输入拆分成更小的单元。

+   **基于术语的查询**：基于术语的查询顾名思义，就是允许用户在文档中指定他们希望 Elasticsearch 查找的关键词或非常具体的术语。它通过使用前述的分词方法来实现，帮助缩小结果范围，确保结果与搜索词直接相关。

+   **全文匹配**：Elasticsearch 利用数学算法通过一种叫做全文匹配的技术来辅助结果的准确性。这与我们稍后会谈到的全文搜索不同。Elasticsearch 会尝试通过计算文档中与搜索词匹配的术语出现次数来确定文档与搜索查询的相关性。它还会使用如**术语频率-逆文档频率**（**TF-IDF**）和**最佳匹配 25**（**BM25**）等算法来评估这些搜索词在整个文档集合中的重要性，因为它会衡量这些出现次数。

+   **全文搜索选项**（**FTSO**）：另一种被称为 FTSO 的样式是一个相关概念，它在焦点和整体功能上有所不同。匹配样式侧重于精确匹配，而 FTSO 则更进一步，不仅考虑词语的相对位置、术语频率、语言分析、相关性评分，还包括部分匹配、同义词和模糊匹配。它允许在数据库语言中常见的布尔运算符（**AND**、**OR**和**NOT**）。简而言之，匹配样式通常只关注提供词语完全匹配的结果，而不太考虑这些结果是否相关。FTSO 则更加注重确保结果不仅仅是词语匹配，而是内容匹配。

+   **语言分析器**：你有没有想过，像 Elasticsearch 这样的技术如何处理它在非英语语言中找到的信息？信不信由你，它能够完成这个任务，因为 Elasticsearch 集成了各种语言分析器。这包括一些不仅强调口语语言特征的分析器。以下是 Elasticsearch 使用的几种流行语言分析器的示例：

    +   **标准分析器**：基于语法的分词方法，适用于大多数语言

    +   **简单分析器**：侧重于非字母字符和小写规范化

    +   **空白分析器**：对每个由空格分隔的单词进行标记化处理

    +   **关键字分析器**：对整个输入字符串进行标记化处理

    +   **口语语言**：英语、法语、德语、西班牙语等多种语言

+   **分面搜索和聚合**：最后，Elasticsearch 提供了一种称为分面搜索和聚合的高级搜索样式，涉及复杂的数据分析。分面搜索有时也称为**分面导航**。这是一种根据数据的不同属性进行数据过滤的方法。这些不同的过滤维度有时称为分面，因此得名。这些分面的一些示例可能是颜色、价格范围和购物类别等内容。这种样式为用户提供了一种确定他们希望如何细化搜索的途径。它允许用户深入到非常精确的搜索级别。

    与分面搜索共同工作的是聚合搜索。这是在搜索开始时进行的，以建立最丰富的潜在相关信息供分面搜索发现用户最需要的信息。Elasticsearch 使用三种主要类型的聚合：

    +   **桶聚合**：使用逻辑分区来组合具有相关属性的文档

    +   **度量聚合**：在每个桶内对数据执行计算

    +   **管道聚合**：基于其他聚合输出运行的次要聚合

除了对搜索考虑因素更多之外，Elasticsearch 还提供几乎实时的数据索引功能。它用来提供这一功能的方法之一被称为**倒排索引**。传统索引是将数据存储在数据库中，并根据整个文档进行组织。而倒排索引是根据文档中的单词和术语组织数据。索引是在初始解析和分析时构建的，然后放入优化的数据结构进行存储。这个结构就是索引本身。

为了进一步支持近实时索引，Elasticsearch 在索引过程开始之前首先将文档写入称为**预写式日志**（**WAL**）的事务日志。这样做是为了在系统故障时保护数据。即使机器功能出现短暂的故障，也可能会损坏索引过程。因此，通过使用 WAL，Elasticsearch 可以立即恢复数据并重新开始索引过程。

为了确保数据准确性，Elasticsearch 每秒刷新一次索引。当发生这种情况时，它会加强索引中信息的可用性，以便进行搜索操作。这个过程使得接近实时的搜索成为可能，从而使得 Elasticsearch 检索信息的整体过程在本质上非常高效。这是任何依赖快速访问信息进行分析的应用程序成功的必要组件，例如 SIEM。

## Logstash

Logstash 是 ELK 堆栈的一部分，具备数据摄取、必要时进行数据转换/丰富，并将其从一个地方传输到另一个地方的能力。虽然 Elasticsearch 是 ELK 堆栈的核心，但 Logstash 提供了一些并行的功能，并充当着提供 Elasticsearch 数据的中介。

Logstash 可以从各种数据源中获取数据。以下是一些可能的数据源：

+   日志文件

+   数据库

+   **应用程序编程** **接口**（**APIs**）

+   消息队列

+   **物联网**（**IoT**）设备

在摄取数据后，Logstash 提供了所谓的**数据转换**和**数据丰富**功能。这是任何依赖异常警报的分析应用程序的关键特性。它通过使用过滤器来执行这些操作。这些过滤器允许用户操作传入的数据，然后将其发送到目标位置，供 Elasticsearch 访问。

过滤器有很多，大约有 50 个，但我们将在这里重点介绍一些较简单且受欢迎的过滤器。随着你开发和提升 ELK 堆栈技能，可以随时深入研究并了解新的过滤器。与此同时，让我们先覆盖一些大家开始学习和训练时会接触到的。

其中一个过滤器是**Grok 过滤器**。Logstash 使用它来解析任何类型的结构化日志数据——这些数据是由 Grok 从非结构化数据中派生出来的——它似乎遵循某种模式。此操作还允许 Logstash 从任何非结构化或半结构化的日志消息中提取它认为有意义的字段。它使用正则表达式尝试识别并定义模式。

**日期过滤器**是另一个转换工具。正如你所预期的，这个工具用于解析和标准化传入的日志数据中的日期和时间。你可以自定义它要解析的日期格式模式。这个过滤器对于包含任何日期或时间戳的日志文件非常有用。任何正确生成的日志文件都应该包含这样的时间戳。

还有一个**翻译过滤器**，它通过外部映射文件进行数据查找。该过滤器利用这些信息来丰富通过日志传入的数据。该过滤器允许你在字典文件中定义键值对。然后，过滤器通过匹配并用相应的字典值替换特定值，从而丰富数据。

名副其实的**Mutate 过滤器**为 Logstash 提供了多种操作，可以用来操控和修改数据。使用这个过滤器，你可以重命名字段，添加或删除字段。你还可以转换字段的类型并修改字段中的值。甚至可以做一些诸如拆分和修剪字符串，或将一个字符串与另一个字符串连接的操作。

如果你想向数据添加地理信息，可以使用**GeoIP 过滤器**。它通过执行 IP 地址查询来实现这一点。通过将 IP 地址映射到相应的地理位置，你可以为数据增添有用的信息，如城市、州、国家以及纬度和经度。

Logstash 使用**用户代理过滤器**来解析通常来自网页日志的字符串，这使得它能够提取关于端点上使用的工具的重要细节。这可能包括设备类型、操作系统、网页浏览器品牌和版本。拥有这些信息，可以让分析人员深入了解数据流两端使用的设备和浏览器。

**JavaScript 对象表示法**（**JSON**）是防御性安全工具中最常见的代码相关语言格式之一。你几乎不可能找到一个主流的 SIEM 或其他网络安全产品，它不兼容 JSON 格式，而且至少有一两种方式可以将应用程序使用的一些信息以 JSON 格式呈现。Logstash 也不例外。它可以轻松处理 JSON 格式的数据，甚至能够处理嵌套的 JSON 结构。嵌套特性是在任何编程或编码环境中，当一个特性被包含在其自身之内时，称为嵌套。

喜欢使用电子表格的朋友会很高兴得知，Logstash 提供了**CSV 过滤器**。这个过滤器允许 Logstash 解析和处理逗号分隔值。这是电子表格用户提取和/或存储可以以电子表格方式呈现的数据时使用的常见格式。此过滤器可以处理多种配置，例如分隔符、头部、行和列映射。

另一种常见且跨系统兼容的格式是**可扩展标记语言**（**XML**）。Logstash 有一个 XML 过滤器，可以解析 XML 格式的数据。这个过滤器可以从 XML 文档中提取特定的元素和/或属性，然后将它们转换为可以用于后续处理的结构化字段。

完整的过滤器集合超出了这个列表。这些只是最常见和最可能被 Logstash 使用的过滤器。完整的过滤器集合使 Logstash 成为一个非常强大的数据、标准化和增值工具。正如你现在可能已经发现的那样，Logstash 采用了一种设计，使其能够处理结构化和非结构化数据。

在处理所有这些数据和我们讨论的 Logstash 如何操作和丰富这些数据的不同方式时，有一个至关重要的编码风格特性，用于控制数据流。那些有编码和/或软件开发经验的人都知道这种数据控制流叫做**条件语句**。条件语句允许用户根据特定的条件和标准控制数据流。一些人可能会认识到这就是**if-else** 语法。

这是一个有趣的例子：

```
if (paycheck(onTime)){        // Set the condition. Am I paid on Time?
         doSomething;          // If I am, the condition is met
         beProductive;        // I will do something and be productive
} else {                    // Otherwise
     doNothing;            // I wasn't paid on time. Condition not met
     beLazy;               // I will be lazy and do nothing
}
```

正如我们之前提到的，Logstash 有点像是一个中介——一个中间人或中介者。这意味着它不仅仅被用来获取日志信息，还被大量依赖于对这些数据进行丰富并转发，主要的输出通道是 Elasticsearch。它可以以多种格式输出数据，但最常见的格式是 CSV 和 JSON，正如我们之前讨论过的那样。

Logstash 有一个插件，C 程序员会立刻认出来，叫做**stdout**，它代表**标准输出**。这个插件允许数据直接发送到控制台应用程序的命令行中。以这种方式输出数据的主要原因是允许在 Logstash 实例上工作的工程师能够立即识别某些数据元素，方便进行调试和应用测试。

另一个常见的输出目的地是**Apache Kafka**，它是一个分布式事件流平台应用程序，最初由流行的社交媒体平台 LinkedIn 开发。它作为开源软件在 Apache 软件基金会下发布。它以高效、容错可靠而著称，能够处理大量数据。

之前，我们讨论了在 Elasticsearch 中组织数据的方式，叫做**桶**（**bucket**）。Logstash 也可以输出到采用这种数据组织原则并大规模推广的组织：亚马逊。亚马逊通过其**简单存储服务**（**S3**）桶来实现这一点。它在亚马逊的云服务中非常受欢迎，并且与**亚马逊网络服务**（**AWS**）完全集成。

Logz.io 是一个流行的基于云的日志管理平台，Logstash 可以将数据输出到这个平台。虽然它是一个日志管理和分析平台，但它并不专注于实时监控、原始数据与丰富数据源之间的关联以及安全事件分析。因此，它距离被认为是一个完整的 SIEM 系统还差一个神经元。

另一个 Logstash 输出集成是**Java 数据库连接**（**JDBC**）。JDBC 是一个 Java API，允许开发者使用 SQL 数据库语言与关系数据库进行交互。它提供了标准的接口和类，用于执行数据库交互操作，如连接、执行 SQL 查询、更新或处理结果。

**Redis** 是一个类似的存储选项，可以用作数据库、缓存或消息代理。Logstash 也会输出这种内存数据结构存储。Redis 的独特之处在于，它将主要数据存储在 RAM 中，以实现快速的读写操作。它支持非常广泛的数据类型，并采用常见的数据库方法，如键值对模型。这种模型将数据作为一个键值对存储，每个键都有一个唯一的标识符。它特别重视键和数据结构的使用，如字符串、列表、集合、哈希和有序集合。Redis 的吸引力在于其处理、存储和回调数据的极高速度和效率。

## Kibana

Kibana 是 ELK 堆栈中的工具，帮助用户探索、可视化和分析其他组件收集和聚合的数据。它专门设计用于与 Elasticsearch 配合使用，提供一个用户友好的界面。最直接的术语来描述 Kibana 就是 **数据可视化**。

Kibana 中的数据可视化采用多种方法和风格，但有七种主要方法，具体如下：

+   **图表**：图表可能是这里最符合逻辑的预期。它们包括折线图、柱状图、面积图、饼图、散点图等。这为那些更偏视觉学习的用户提供了一个从不同角度发现潜在异常的机会。

+   **地图**：虽然许多软件应用程序都使用地图，但 Kibana 将这一功能提升到了更高的水平。用户可以创建分级图、符号图以及基于网格坐标的地图。**分级图**是一种专题地图，专注于某个特定的主题或话题。分级图的风格是使用不同的颜色或图案来表示地理区域/地区，通常是国家、省/州或县/区。你可能在童年课堂上看到过这些类型的地图。符号图与之类似，但它更侧重于使用符号来识别和表示特定的地理特征，而网格坐标地图在与经纬度对齐时尤为常见。

+   **Timelion**：顾名思义，Timelion 是一个用于 Kibana 的数据可视化插件，允许最终用户处理和分析基于时间的数据。像大多数与 ELK 堆栈相关的工具一样，Timelion 可以用于数据转换、聚合和数学计算。你可以执行平均值、百分位数、求和等功能。它支持使用变量，这对于处理不规则的时间序列非常有用。Timelion 创建的典型可视化图表通常以折线图的形式出现。当应用于技术时，折线图几乎总是以 *X* 和 *Y* 轴的形式存在，其中 *X* 轴表示时间，*Y* 轴表示数据值。

+   **时间序列可视化构建器**（**TSVB**）：TSVB 是 Kibana 中的一个可视化选项，侧重于最终用户的使用舒适度。它允许用户通过简单的拖放界面构建复杂的时间序列可视化。它提供了多种图表类型、数学聚合和自定义样式的访问权限。

+   **仪表盘和指标**：Kibana 版本的仪表盘和指标可视化旨在提供代表特定范围内单一值的视觉效果。这允许用户以多种方式呈现这些简单的值，包括火花图（sparklines）。**火花图**是小型的紧凑线条柱状图，或者有时是简单的折线图，旨在显示数据的变化或趋势，适合在紧凑的空间中展示。它们旨在快速提供数据的*形状*或趋势视觉。

+   **热力图**：在今天的商业世界中，热力图是最受欢迎的数据可视化之一。热力图是二维的，通常以全球或其他大型地理地图的形式呈现。它们通常用于显示趋势、关联性和模式，帮助分析师解决复杂和/或广泛的问题。在网络安全领域，热力图的一个常见用途是展示网络攻击的来源或源 IP 位置。通常，热力图会精心集成颜色方案，当多个实体交叉时，颜色的强度会增加，比如当一些手机 GPS 地图根据特定地点的交通拥堵情况将路线颜色从蓝色逐渐加深到黄色，再到红色。热力图使用标签和注释，但最好的热力图会保持简洁，限制这些信息，以便使可视化对观察者来说保持清晰。

+   **标签云**：标签云，也叫做**词云**，是文本数据的可视化，通常通过按类别和突出性显示不同大小和颜色的单词。它们通常根据频率来衡量单词的重要性。生成这些图形的算法通常会在决定最终输出之前执行一些预处理操作，去除一些不太可能与数据相关的常见词汇，比如介词和代词。这是一种文本清理形式，类似于 Elasticsearch 中的分词方法。

Kibana 提供的另一种可视化类型是**仪表板创建**。这是与 Kibana 最相关的总体元素，也是利用我们刚刚探索的不同图表和其他可视化的一个功能。Kibana 仪表板提供了一个集中视图，可以将多个可视化组合在一起，并可以应用查询和过滤器，创建一个完全自定义的布局。它们有助于近实时跟踪业务世界中所称的**关键绩效指标**（**KPI**）。

Kibana 支持用户执行临时搜索。这是在 Elasticsearch 中索引数据的基础上进行的，且具有可视化的搜索界面。如果你愿意，可以使用简单的搜索语法来构造查询。更高级的用户则可以选择使用 Lucene 查询语法，这对精确搜索非常有帮助。搜索结果会立即显示，之后你可以进一步细化、过滤和排序数据，继续深入分析特定的子集信息。

说到查询和过滤器，Kibana 提供了一种可视化构建查询和过滤器的方式，帮助你深入探索数据。Kibana 的 **领域特定语言**（**DSL**）允许用户构建复杂的查询和聚合，以便从数据中提取深刻且有意义的见解。你可以使用过滤器，根据特定条件（如时间范围、属性或自定义定义的条件）来缩小数据范围。

Kibana 的一大优势是其在基于时间的分析上的卓越表现。这使你能够在一定时间范围内更深入地探索数据趋势、模式和异常。你可以调整时间范围，设置自定义时间间隔，甚至应用带有日期的直方图，从而在不同的粒度级别上分析数据。时间选择器使你能够指定你希望分析的时间范围。

**地理空间分析**是 Kibana 提供的另一个功能，允许你可视化和分析基于位置的数据。你可以使用此功能在交互式地图上绘制数据，地理编码 IP 地址，应用地理空间聚合，并使用热力图可视化密度。Kibana 还支持多个地图层、自定义底图和地理形状文件，这是一大亮点。它还提供地理空间扩展，其中最常见的是 Elastic Maps Service。

Kibana 允许用户设置警报和监视器，以实时跟踪特定事件或条件。你可以根据多种因素定义警报规则，包括数据阈值、模式或其他条件。它还可以与外部通知系统（如电子邮件和流行的 webhook 等）协同工作，触发警报。正是这套功能，使得 ELK 堆栈在很多日志摄取和处理工具中更接近于一个真正的 SIEM 环境。

Kibana 不仅仅是一个数据可视化工具——它还提供了安全性功能。它提供了一套非常强大的安全功能来控制对数据的访问。它通过支持身份验证和授权机制、**基于角色的访问控制**（**RBAC**），以及与外部身份验证提供商（如 **轻量目录访问协议**（**LDAP**）和/或 **安全断言标记语言**（**SAML**））的集成，来实现这一点。

ELK 堆栈到目前为止，主要用于数据处理/增强、可视化以及整体处理。然而，我们首先需要一种方式来收集这些信息并将其发送到 ELK 堆栈。这个过程就需要 **Beats**。

# 代理与监控

还有一些与 ELK 堆栈相关的突出的附加应用程序。它们是开源的 Beats 和商业化的 X-Pack。正是这两个组件的加入，使 ELK 堆栈成为一个完整的 SIEM 系统。它们共同提供数据收集与传输、警报与通知、用于检测隐藏异常行为的机器学习，以及自动化报告功能。

## Beats

Beats 是一组数据收集和传输代理，通常被称为**数据运输工具**。它们是轻量级的——微型——应用程序，安装在终端上，包括个人计算机、服务器和其他网络设备，专门用于收集数据并将其发送到 Elasticsearch 和 Logstash 以便实时进一步处理。Beats 从其安装的设备收集操作数据。它从不同的来源获取数据，包括日志、网络流量以及其他相关信息。Beats 家族包括各种专门的数据运输工具，例如用于日志文件的 **Filebeat**，用于机器指标的 **Metricbeat**，用于网络数据的 **Packetbeat**，以及用于审计事件的 **Auditbeat** 等。Beats 使用安装在服务器或系统上的轻量级代理，高效地收集数据，并最大限度地减少资源使用。让我们详细了解一下这些：

+   **Filebeat**：Filebeat 旨在收集和传输日志文件。它可以监控日志目录和文件。它将收集的数据发送到 Elasticsearch 或 Logstash，在那里对数据进行增强和/或进一步处理与分析。Filebeat 支持不同的日志文件格式，包括纯文本日志、JSON 日志、syslog 等。

+   **Metricbeat**：该 Beats 收集系统级别的指标和统计信息，例如 CPU 使用率、内存利用率、磁盘 I/O 以及网络指标等。它可以从各种来源收集指标，如操作系统、服务、容器、数据库和云平台。与 Filebeat 的操作方式相同，Metricbeat 将收集到的指标发送到 Elasticsearch 或 Logstash 进行增强、进一步处理、存储、分析和可视化。

+   **Packetbeat**：如果能够从网络数据中捕获这些类型的指标，那该有多好？这正是 Packetbeat 的作用。它捕获网络数据并分析各种协议，包括 HTTP、DNS、MySQL 和 Redis 以及其他多种协议，从而提取元数据和行为洞察。Packetbeat 的数据收集目标意味着它可以提供近实时的网络流量可视化。这可能包括事务细节、请求和响应数据，或网络错误。Packetbeat 也可以用于故障排除、性能问题诊断、安全威胁识别和网络活动监控。

+   **Auditbeat**：这个工具被设计用来收集并发送来自 Linux 审计框架和 Windows 事件日志的审计事件。Auditbeat 监控系统日志中的审计事件，以便提供用户活动的洞察，并通过监控权限变更、文件系统修改等，查找未经授权的特权升级。Auditbeat 在识别被攻陷的网络和/或设备方面具有潜力。它是一个不可或缺的工具，有助于合规性监控、安全分析以及检测可疑活动。

作为一个软件包，Beats 允许进行一些数据转换功能，以便为分析准备收集到的数据。Beats 提供了轻量级的功能，这些功能在数据发送到 Elasticsearch 或 Logstash 之前应用到事件上。这些功能被称为 Beats 处理器。它们可以丰富、过滤和修改收集到的数据。这些处理器可以执行如过滤字段、用元数据丰富数据、重命名字段等操作，当然还有更多功能。

对于商业技术来说，一个关键的考量是能够应对突如其来的、或意外的快速业务增长。幸运的是，Beats 被设计成能够处理可扩展性。不仅如此，它还被设计成能够在高流量和分布式环境中运行。Beats 可以通过在不同服务器或系统上部署多个 Beats 代理来轻松实现水平扩展。这些代理提供了可配置选项来处理负载均衡、容错和故障转移，确保数据收集具有高可用性和弹性。

作为 Elastic Stack 的重要组成部分，Beats 的设计使其能够与 ELK Stack 的其他组件——Elasticsearch、Logstash 和 Kibana 无缝集成。它们实现了数据的高效收集、传输和处理，然后将数据存储在 Elasticsearch 中进行进一步分析和可视化。Beats 可以配置为直接将数据发送到 Elasticsearch 或 Logstash，以进行额外的数据转换和丰富。

最后一部分提出了一个我们应该讨论的话题。我们已经讨论过几次，数据可以直接发送到 Elasticsearch 或 Logstash，而 Beats 也可以将数据发送到任一位置。分析这些场景非常重要，这样在我们职业生涯的发展过程中，才能在设置数据传输工具时选择合适的配置。

话虽如此，我们也要考虑直接将数据发送到 Elasticsearch 或通过 Logstash 作为中介的优缺点。

直接将数据发送到 Elasticsearch 的优点如下：

+   **简洁性**：将数据直接发送到 Elasticsearch 意味着数据在到达目的地之前少了一个停靠点。通过去除 Logstash 作为中介，我们简化了数据管道。

+   **可扩展性**：正如我们刚才提到的，现代商业技术需要能够适应突如其来或意外的业务增长。通过直接将数据发送到 Elasticsearch，我们为水平扩展提供了可能。这将使得可以在多个 Elasticsearch 节点之间分配工作负载，从而显著提高性能，并让我们能够处理更大规模的数据量。

+   **效率**：每当从数据流中移除一个停止点时，所需的处理开销将减少。由于数据能够继续流动并超越原本应该存在停止点的位置，也会减少延迟/滞后。这使得我们能够实现 Elasticsearch 所知名的近实时数据摄取和数据索引。这反过来又加快了搜索和分析的可用性。

+   **简化的架构**：为了支持更高效率的论点，Beats 或任何数据传输工具与 Elasticsearch 的直接集成意味着我们有一个更紧凑、更直接的架构。这保持了整个数据流的简洁性，除了提升性能外，还意味着减少了出现故障的可能性。

直接将数据发送到 Elasticsearch 的缺点如下：

+   **有限的数据转换**：虽然我们可能会提高效率，但也失去了 Logstash 的优势。这意味着，通过直接将数据发送到 Elasticsearch，我们不再拥有 Logstash 所提供的数据丰富和转换功能。Elasticsearch 虽然也具备这些功能，但不如 Logstash 强大。Logstash 提供了更强大的过滤、丰富和数据解析能力，可以在将数据索引之前进行预处理和转换。

+   **某些场景中的复杂性**：在某些情况下，额外的数据预处理将简化 Elasticsearch 最终需要完成的工作。因此，有时候某些数据丰富工作更适合由 Logstash 完成。尤其是那些涉及多个数据源或更复杂转换的超复杂数据管道，Logstash 的灵活性和功能会带来更好的帮助。

+   **与非 Elasticsearch 系统的兼容性**：有时可能会出现除了 Logstash 或 Beats 以外的其他系统将数据发送到 Elasticsearch 的情况。如果这些系统与 Elasticsearch 的集成不完全，可能会在数据规范化过程中遇到困难。来自 Logstash 的任何数据都肯定与 Elasticsearch 兼容。另一方面，Logstash 配备了多种插件，可以帮助将数据转换为与 Elasticsearch 兼容的格式。

使用 Logstash 作为中介的优点如下：

+   **数据转换和丰富**：Logstash 拥有非常强大的数据处理能力，其规模和范围远超 Elasticsearch。Logstash 能够进行高级过滤、数据丰富和解析。它还具有一定的警报功能。凭借其大量的插件和过滤器，Logstash 可以支持更为复杂的处理方式。综合这些功能，Logstash 能够在将数据传递给 Elasticsearch 之前，先将其转换为兼容的格式。

+   **兼容性和集成**：Logstash 中存在大量的插件和过滤器，使其能够与许多数据源和目标进行集成。由于这一点，Logstash 可以支持多种不同的格式、协议和系统。这也使其能够兼容许多非 Elasticsearch 系统，从中获取数据并为 Elasticsearch 进行转换。这赋予了我们所谓的数据管道灵活性。

+   **复杂场景中的灵活性**：虽然 Elasticsearch 并不适合极其复杂的场景，但 Logstash 在这一领域表现出色。这些领域包括复杂的数据管道，以及需要从多个数据源收集数据、转换、组合或拆分后再传递给 Elasticsearch 的情况。这提高了处理多样化需求和数据操作时的整体粒度和灵活性。

使用 Logstash 作为中介的缺点如下：

+   **增加的复杂性**：虽然 Logstash 增强了将多种不同类型数据标准化以便 Elasticsearch 使用的能力，但它也通过在数据流中增加一个新组件而带来了额外的停靠点。这意味着需要配置、管理和优化更多的内容。对于较小的部署，这可能会增加系统的复杂性和开销，特别是当这些部署的需求较简单时。

+   **潜在的性能影响**：每当一个额外组件被添加到技术管道中，就意味着需要额外的处理和开销。这将降低效率，并增加数据流的延迟/滞后。在数据流量很大和/或需要大量转换和丰富的情况下，性能可能会受到影响。

+   **开销和资源利用**：由于有额外的技术，资源的需求也会增加。这意味着与将数据直接发送到 Elasticsearch 集成相比，对 CPU、内存和磁盘空间的压力会更大。在涉及非常高数据量或上述资源有限的场景中，这可能更为有价值。

Elasticsearch 与 Logstash 数据摄取的基本区别在于，直接将数据发送到 Elasticsearch 提高了效率，但会减少某些功能；而首先将数据发送到 Logstash 则恰恰相反，增加了功能但降低了效率。Elasticsearch 大大提高了整体效率，减少了预处理开销。Logstash 提供了丰富的数据转换功能，能够处理原生格式下与 Elasticsearch 不兼容的数据。然后，当它处理完成后，会将其作为兼容的摄取平台传送到 Elasticsearch。最终，关于数据发送位置的决策将取决于你对数据管道复杂度、数据转换需求以及与环境中其他系统兼容性的预期。

## X-Pack

我们迄今讨论的 ELK 堆栈中的四个组件都是免费的开源应用，而 X-Pack 是一个部分免费的扩展，提供额外的付费服务。它比完全免费的应用提供了增强的功能。

它提供五个独立的功能：

+   **安全性**：X-Pack 通过多种措施提供安全性。其中一项是 RBAC（基于角色的访问控制），它允许你定义和管理用户的角色和权限，从而对文档、索引和其他资源进行严格的控制。它为客户端与 Elasticsearch 之间的安全通信提供 **传输层安全性** (**TLS**) 加密。X-Pack 还通过使用 LDAP、Active Directory 和本地认证等方法提供安全的用户认证，帮助保持对 Elasticsearch 集群的安全访问。它还具有审计日志功能，能够记录与安全相关的事件，帮助你监控和追踪用户与系统行为中的异常情况。

+   **监控**：X-Pack 提供了对 Elasticsearch 集群的详细监控。它监控的一些问题包括节点健康、资源使用情况和索引速率。这是为了让你能够识别和应对性能问题和瓶颈。它还会查看查询性能，识别响应缓慢的查询和低效的搜索模式。你可以根据预定义的条件和阈值设置监控警报。如果满足预定义条件或指标，这些警报将会通知你。

+   **警报**：X-Pack 最关键的组件之一是其警报功能。它使用名为 **Watcher** 的应用程序来定义和安排基于查询结果和聚合的警报规则。像所有 SIEM 技术的主要产品一样，它可以执行诸如提供自动电子邮件通知或 Slack 消息等操作。它还可以执行自定义脚本，赋予分析师主动响应关键事件的能力。

+   **机器学习（ML）**：X-Pack 通过其 ML 功能提供异常检测。这使得时间序列数据中的异常能够被自动检测出来，帮助分析师识别不寻常的模式、偏差或其他异常行为。它还使用机器学习来根据历史模式预测未来的趋势，这些趋势否则可能很难被发现或完全隐藏。X-Pack 的 ML 特别之处在于，它能够帮助识别出它认为是导致异常行为的最有影响力的因素。这对于调查人员和事件响应人员进行根本原因分析非常有帮助。

+   **报告**：最后，X-Pack 允许你创建和安排报告，这是一项功能全面的特性，类似于警报功能，它将帮助你将 ELK 堆栈转变为一个完整的 SIEM。它提供多种格式的报告，如 **.pdf**、**.csv** 和 **.xls**，这些格式与 Elasticsearch 中的数据格式非常契合。你可以按需生成这些报告，或者将它们安排在预定的时间间隔内运行。

通过以上内容，我们已经了解了一整套被称为 Elastic SIEM 的应用程序。我们讨论了可以用来收集、传输、丰富和可视化安全数据的工具。现在，是时候开始设置我们的技术，实践这些内容了。

# 总结

在本章中，我们了解了 Kali Linux 的发展历程，并开始拆解该操作系统的紫色变种（Purple variant）提供的最流行工具集之一。我们深入探讨了 Elastic Stack，通常被称为 ELK 堆栈，因为它将成为本书的主要焦点之一，作为 Kali Purple 的核心。我们深入了解了 Elasticsearch 和 Logstash 如何处理数据，从数据的摄取到丰富再到聚合。我们还看到了如何通过 Kibana 将这些数据以可视化的方式呈现出来。

在彻底分析了 ELK 堆栈如何处理数据之后，我们开始检查原始数据源以及如何通过 Beats 从中提取数据并传递给 Elasticsearch，通常通过 Logstash 作为中间停留点，但并非总是如此。我们研究了直接将数据发送到 Elasticsearch 与通过 Logstash 发送数据之间的差异。我们还了解了 ELK 堆栈的商业组件 X-Pack，并能看到当 X-Pack 告警功能被纳入公式时，ELK 堆栈能变得多么像一个完整的 SIEM。

现在我们已经对 Kali Purple 的核心功能有了扎实的理解，接下来可以进入学习过程的下一个阶段——实践应用。在下一章中，我们将开始设置我们的技术，以便获取、安装和配置我们自己的 Kali Purple 实例。是时候将我们所学的知识付诸实践了！

# 问题

回答以下问题，测试你对本章内容的理解：

1.  什么是 ELK 堆栈？

    1.  一群野鹿站在彼此之上

    1.  一组协同工作的开源软件

    1.  环境 Linux 知识

1.  正确还是错误：Beats 是一个商业产品，属于 ELK 的一部分，但使用它需要付费。

    1.  正确

    1.  错误

1.  管道与其他聚合的主要区别是什么？

    1.  这种类型的聚合利用其他聚合的结果

    1.  其他聚合依赖于首先执行这个聚合

    1.  这种聚合围绕一组长而细的线性标准展开

    1.  如果发生管道泄漏，必须通知 EPA

1.  在 Elastic 堆栈管道中，数据在哪里得到增强（你可以选择多个选项）？

    1.  当其中一个 Beats 代理在数据发货之前收集数据时

    1.  Kibana 只有在数据通过 Logstash 和 Elasticsearch 时才会丰富数据

    1.  Logstash

    1.  Elasticsearch

1.  哪个 ELK 堆栈组件帮助用户可视化数据？

    1.  Elasticsearch

    1.  Logstash

    1.  Kibana

    1.  Beats

    1.  X-Pack

1.  条件语句在 ELK 堆栈中是什么？

    1.  一种处理气流的设备，用以保持其冷却

    1.  Kali Purple 实例的操作员与其客户之间的协议

    1.  一个 Elasticsearch 进程，只有在满足所有需求的情况下才会执行

    1.  一组命令，用于根据预定标准控制数据流

# 深入阅读

要了解本章所涵盖的主题，可以查看**BM25** **算法**：[`www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables`](https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables)。
