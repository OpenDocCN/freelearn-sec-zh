- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Kali Linux and the ELK Stack
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kali Linux和ELK堆栈
- en: Now that we’ve gained a basic understanding of the evolution of cybersecurity
    as a professional field of study and practice, let’s begin to unpack the Kali
    Purple toolset. You’ll recall our explanation of red and blue colors creating
    purple on the color wheel. That’s because Kali Purple’s genealogy is a double-pronged
    utility coming from two suites of technical tools, one associated with the **red
    team** and the other with the **blue team**. We provided an overview of each grouping
    in the previous chapter. Those lists of tools were not nearly an exhaustive, or
    complete, list of tools – just the highlights.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经基本了解了网络安全作为一个专业领域的演变，让我们开始逐步拆解Kali Purple工具集。你会记得我们之前解释的红色和蓝色在色轮上会混合成紫色。这是因为Kali
    Purple的血统是一个双重用途的工具集，来自两套技术工具，一套与**红队**相关，另一套与**蓝队**相关。我们在上一章提供了每个分组的概述。那些工具列表并不是一个详尽无遗的工具清单——只是重点展示了一些工具。
- en: In this chapter, we are going to briefly explain Kali Linux for those who might
    be delving into Linux for the first time. A popular phenomenon has been developed
    with Kali Purple in that its defensive security offerings are causing some people
    to pursue experience with the Linux **operating system** (**OS**) for the first
    time in their careers. However, most folks who’ve picked up this book likely already
    have Linux experience, and some may even have Kali experience. So, while briefly
    covering Kali Linux, we will focus mostly on unpacking the initial elements of
    the **security information and event management** (**SIEM**) system, the **ELK
    stack**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将简要解释Kali Linux，供那些可能第一次接触Linux的读者了解。Kali Purple的防御性安全功能已经成为一种流行现象，促使一些人在职业生涯中第一次接触Linux**操作系统**（**OS**）。然而，大多数阅读本书的人可能已经有Linux经验，甚至有些人可能有Kali经验。因此，在简要介绍Kali
    Linux时，我们将主要集中于解构**安全信息与事件管理**（**SIEM**）系统的初步元素——**ELK堆栈**。
- en: '**ELK** stands for **Elasticsearch, Logstash, and Kibana**. They are three
    unique open source software offerings that are usually pasted together for technology
    solutions related to log management, data storage, search/query, data analysis,
    and data visualization. They alone do not define a SIEM. However, when combined
    with certain other core components, such as **Beats** and **X-Pack**, the ELK
    stack will deliver unto us the information and event management system we are
    questing for.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**ELK**代表**Elasticsearch、Logstash和Kibana**。它们是三款独特的开源软件，通常组合在一起，用于与日志管理、数据存储、搜索/查询、数据分析和数据可视化相关的技术解决方案。它们本身并不定义一个SIEM。然而，当与其他一些核心组件结合使用时，如**Beats**和**X-Pack**，ELK堆栈将为我们提供我们正在追求的信息和事件管理系统。'
- en: 'There are multiple tools available in Kali Purple, but we will be focusing
    on the main powerhouse of the suite, the distributed analytics engine known as
    Elasticsearch. For us to understand the tools we will be working with, we will
    need to focus on the following Elastic elements:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Kali Purple中有多个工具可用，但我们将重点关注这个工具集中的主要强大引擎——分布式分析引擎Elasticsearch。为了理解我们将使用的工具，我们需要关注以下Elastic元素：
- en: Elasticsearch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elasticsearch
- en: Logstash
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Logstash
- en: Kibana
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kibana
- en: Beats
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beats
- en: X-Pack
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X-Pack
- en: Since the SIEM is the core solution you will find within a SOC, it is critical
    that we fully explore and understand this tool, how it works, how and where it
    gets its data, and the different ways we can work with that data to accomplish
    our security objectives. The typical SIEM takes data from multiple other sources,
    normalizes it, enriches it, organizes it, stores it, and then presents it to cybersecurity
    analysts in a unified format for human evaluation and action. Organizations rely
    on it to centralize their defensive security operations. By the end of this chapter,
    you will have a solid understanding of those concepts.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 由于SIEM是你在SOC中找到的核心解决方案，因此我们必须充分探索并理解这个工具，了解它如何工作，它如何以及从哪里获取数据，以及我们如何通过不同的方式处理这些数据以实现我们的安全目标。典型的SIEM会从多个其他来源获取数据，对其进行标准化、丰富、组织和存储，然后以统一的格式呈现给网络安全分析员进行人工评估和行动。组织依赖它来集中管理防御性安全操作。在本章结束时，你将对这些概念有一个扎实的理解。
- en: This understanding will come in handy throughout the next three chapters as
    we negotiate the process of preparing our technology for Kali Purple and then
    acquiring, installing, and configuring our technology along with Kali Purple itself.
    Knowing the data flow as it relates to the ELK stack and Kali Linux environment
    will provide the foundation you need to confront any anomalies that might arise
    when we unpack our *SOC-in-a-Box* solution.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的三章中，这些理解将派上用场，因为我们将在准备Kali Purple的技术时，接着是获取、安装和配置我们的技术以及Kali Purple本身。了解ELK堆栈与Kali
    Linux环境相关的数据流将为你提供基础，以应对在我们解包*SOC-in-a-Box*解决方案时可能出现的任何异常。
- en: 'This chapter covers the following topics:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下内容：
- en: The evolution of Kali Linux
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kali Linux的演变
- en: Elasticsearch, Logstash, and Kibana (ELK stack)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elasticsearch、Logstash和Kibana（ELK堆栈）
- en: Agents and monitoring
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理和监控
- en: The evolution of Kali Linux
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kali Linux的演变
- en: Though a groundbreaking technology, Kali Purple has its modern roots back in
    the year 1969\. It was then in AT&T’s Bell Laboratories that Ken Thompson and
    Dennis Ritchie co-created the UNIX OS. Ritchie is also famously known for creating
    the C programming language. Nearly all modern computing exists because of Ritchie’s
    two biggest contributions. The original Windows was programmed in C. UNIX eventually
    became available in both open source and commercial varieties. The two biggest
    customers of commercial UNIX were the United States **Department of Defense**
    (**DOD**) and – drumroll please – Apple Computer. That’s right. The Mac OS X is
    built from UNIX. That leaves only one major player to account for… Linux.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管是一项突破性技术，Kali Purple的现代根源追溯到1969年。正是在AT&T的贝尔实验室，Ken Thompson和Dennis Ritchie共同创造了UNIX操作系统。Ritchie也因创造C编程语言而闻名。几乎所有现代计算技术都归功于Ritchie的两项重大贡献。最初的Windows操作系统就是用C语言编写的。UNIX最终以开源和商业版本的形式发布。商业UNIX的两个最大客户是美国**国防部**（**DOD**）和——请鼓掌——苹果公司。没错，Mac
    OS X就是基于UNIX构建的。那就只剩下一个主要的角色了……Linux。
- en: A computer scientist at the University of Helsinki in Finland, Linus Torvalds
    grabbed one of the open source versions of UNIX and began to add his style to
    create a brand-new OS based on UNIX. He then publicly released the OS for free.
    *Linus + UNIX = Linux.* Because Mr. Torvalds released the code and a license allowing
    anybody to take it, modify it, and redistribute their own versions of it, Linux
    rapidly grew in popularity, with countless varieties of the OS available in the
    world today.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 芬兰赫尔辛基大学的计算机科学家Linus Torvalds拿到了一个开源版本的UNIX，并开始加入自己的风格，创建一个全新的基于UNIX的操作系统。他随后公开发布了这个操作系统，免费供人使用。*Linus
    + UNIX = Linux*。由于Torvalds先生发布了代码并允许任何人修改和重新发布自己的版本，Linux迅速流行起来，今天世界上有无数种Linux操作系统版本。
- en: The Kali Linux genealogy was reinforced by Dennis Ritchie a second time when
    he left Bell Labs to go work for the University of California at Berkley. It was
    there that he eventually created a newer version of UNIX that he dubbed **Berkeley
    Software Distribution** (**BSD**). It was this version of UNIX that Debian Linux
    – the OS based directly on Torvald’s initial contributions – utilized to beef
    up its own powerful OS.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当Dennis Ritchie离开贝尔实验室去加利福尼亚大学伯克利分校工作时，他第二次加强了Kali Linux的基因。正是在那里，他最终创建了UNIX的一个新版本，命名为**伯克利软件分发版**（**BSD**）。正是这个版本的UNIX，Debian
    Linux——基于Torvalds最初贡献的操作系统——用来增强其强大的操作系统。
- en: Highly reputable American information security company Offensive Security funded
    the development of Kali Linux to support ethical cybersecurity needs through digital
    forensics and penetration testing. Offensive Security employees Mati Aharoni and
    Devon Kearns took one of the company’s previous Linux distributions and revamped
    it to give the world the Debian Linux derivative now known as Kali Linux.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 享有盛誉的美国信息安全公司Offensive Security资助了Kali Linux的开发，以支持通过数字取证和渗透测试的道德网络安全需求。Offensive
    Security的员工Mati Aharoni和Devon Kearns将公司之前的一个Linux发行版进行了重新设计，创造出了现在被称为Kali Linux的Debian
    Linux衍生版。
- en: As previously mentioned, the flagship of the SOC is the SIEM, and Kali Linux
    gave us the tools to put that together with the release of Kali Purple in December
    2022\. Let’s start examining the most potent of these tools, the ELK stack.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，SOC的旗舰产品是SIEM，而Kali Linux通过在2022年12月发布的Kali Purple为我们提供了将其整合在一起的工具。让我们开始检查这些工具中最强大的——ELK堆栈。
- en: Elasticsearch, Logstash, and Kibana (ELK stack)
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Elasticsearch、Logstash和Kibana（ELK堆栈）
- en: The ELK stack refers to the three software components that collaboratively enrich,
    index, and visualize data for analysis. Some folks will include the Beats family
    of data collection agents as part of the ELK stack. We will cover Beats in the
    next section.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ELK堆栈是指三个软件组件，它们协同工作以增强、索引和可视化数据进行分析。有些人会将Beats系列数据收集代理视为ELK堆栈的一部分。我们将在下一节中介绍Beats。
- en: Elasticsearch
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Elasticsearch
- en: Elasticsearch is the central engine of the ELK stack. As we’ll discuss in a
    moment, Elasticsearch is technically a type of non-relational, NoSQL database.
    Without the ability to effectively search through the data we have collected,
    how on Earth would we ever be able to decide if there is something that should
    be alerted to? Kibana would have no path forward to properly display the visualization
    threats it provides to us. It is a very powerful component of the ELK stack in
    that it is designed to handle very large volumes of data in scenarios where a
    lot of querying frequently occurs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch是ELK堆栈的核心引擎。正如我们稍后会讨论的，Elasticsearch从技术上来说是一种非关系型NoSQL数据库。如果没有有效地搜索我们收集的数据，我们怎么可能判断是否有什么内容需要发出警报呢？Kibana将无法继续前进，无法正确展示它提供的可视化威胁。这是ELK堆栈中一个非常强大的组件，因为它设计用于处理在大量查询频繁发生的情况下的大量数据。
- en: Elasticsearch accomplishes this by using a technique known as **sharding**.
    This is a database terminology that, in simple terms, allows data to be structured
    and organized by breaking it into pieces. This allows the technology to spread
    pieces of data horizontally across multiple partitions/nodes and servers while
    maintaining its relationship with similar data through indexing. These smaller
    chunks of data are called shards. The process of placing the data into shards
    occurs during the indexing process.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch通过使用一种被称为**分片**（sharding）的技术来实现这一目标。这是数据库术语，简单来说，就是通过将数据拆分成小块来结构化和组织数据。这使得该技术能够将数据的各个部分水平分布到多个分区/节点和服务器上，同时通过索引保持与相似数据的关系。这些较小的数据块称为分片。数据被分割成分片的过程发生在索引过程中。
- en: The value of creating these shards is that Elasticsearch can then use a technique
    called **parallel processing** to access multiple shards simultaneously. Parallel
    processing is when more than one processing unit – a CPU, for example – works
    concurrently across multiple nodes. It provides the effect of having more than
    one computer working on the same thing at the same time to get the job done quicker.
    This is typically employed in database technology.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 创建这些分片的价值在于，Elasticsearch可以使用一种叫做**并行处理**的技术来同时访问多个分片。并行处理是指多个处理单元——例如CPU——在多个节点上并行工作。它的效果就像是有多台计算机同时处理同一任务，从而加快完成工作的速度。这通常应用于数据库技术中。
- en: Something beneficial Elasticsearch offers is the support for the creation of
    duplicates – **replicas** – of the shards. As implied, these are copies of the
    original shards. This is done to support the *availability* component of the cybersecurity
    framework by creating redundancy, which, in turn, serves the purpose of fault
    tolerance. It creates this safety net by placing the replica shards in a different
    node within the data cluster. Just as the original shards are created during the
    indexing process, so too are the creation of the duplicates.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch提供的一个有益功能是支持创建分片的副本——**副本**（replicas）。顾名思义，这些是原始分片的副本。这样做是为了支持网络安全框架中的*可用性*组件，通过创建冗余，从而实现容错。它通过将副本分片放置在数据集群的不同节点中来创建这一安全网。就像原始分片在索引过程中创建一样，副本的创建也是如此。
- en: Although it may not sound brag-worthy to someone unfamiliar with searching technology,
    Elasticsearch offers **full-text searching**. That’s a big deal! Many searching/querying
    technologies only perform searches based on matching extracted keywords or embedded
    metadata. Full-text searching means that Elasticsearch will search for and retrieve
    results based on the entire contents of a document and text. That means considering
    the full contents of the document.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对于不熟悉搜索技术的人来说，可能听起来不算什么值得吹嘘的事情，但Elasticsearch提供了**全文搜索**功能。这可是大新闻！许多搜索/查询技术仅基于匹配的关键词或嵌入的元数据进行搜索。全文搜索意味着Elasticsearch会根据文档和文本的完整内容进行搜索并检索结果。这意味着需要考虑文档的全部内容。
- en: 'There are six key features Elasticsearch utilizes to support this style of
    searching:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch利用六个关键特性来支持这种搜索方式：
- en: '**Tokenization**: Tokenization, when applied to full-text searching, might
    also be considered a form of sanitizing the text. What this means is that Elasticsearch
    breaks the text up into individual units. It then stores the small units, or tokens,
    in a reverse index – sometimes called an **inverted index**. That is done to also
    allow searching based on keywords or simple terms. Part of the tokenization process
    involves steps such as converting all uppercase characters into lowercase characters,
    removing punctuation, and dividing the input into smaller units.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词**：分词在应用于全文搜索时，也可以被视为一种文本清理方式。其含义是，Elasticsearch将文本拆分为单独的单元，并将这些小单元或词元存储在倒排索引中——有时也叫做**倒排索引**。这样做的目的是允许基于关键词或简单术语进行搜索。分词过程的一部分包括将所有大写字符转换为小写字符、去除标点符号，并将输入拆分成更小的单元。'
- en: '**Term-based queries**: Term-based queries are just as they sound – a search
    that allows the users to state keywords or very specific terms they want Elasticsearch
    to seek out within a document. It accomplishes this by utilizing the tokenization
    method described previously to help narrow down the results, making sure they
    are directly relevant to the search terms.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于术语的查询**：基于术语的查询顾名思义，就是允许用户在文档中指定他们希望Elasticsearch查找的关键词或非常具体的术语。它通过使用前述的分词方法来实现，帮助缩小结果范围，确保结果与搜索词直接相关。'
- en: '**Full-text matching**: Elasticsearch utilizes mathematical algorithms to assist
    in result accuracy by employing a technique known as full-text matching. This
    is not to be confused with full-text searching, which we will talk about in a
    moment. Elasticsearch will attempt to determine the relevance of a document as
    it relates to the search query by counting the number of occurrences of terms
    in a document that match one of the search terms. It will also use algorithms
    such as **Term Frequency-Inverse Documents Frequency** (**TF-IDF**) and **Best
    Match 25** (**BM25**) to assess the importance of these search terms within the
    entire collection of documents searched because of measuring those occurrences.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全文匹配**：Elasticsearch利用数学算法通过一种叫做全文匹配的技术来辅助结果的准确性。这与我们稍后会谈到的全文搜索不同。Elasticsearch会尝试通过计算文档中与搜索词匹配的术语出现次数来确定文档与搜索查询的相关性。它还会使用如**术语频率-逆文档频率**（**TF-IDF**）和**最佳匹配25**（**BM25**）等算法来评估这些搜索词在整个文档集合中的重要性，因为它会衡量这些出现次数。'
- en: '**Full-text search options** (**FTSO**): Another style called FTSO is a related
    concept that differs in its focus and overall functionality. While the matching
    style focuses on exact matching, FTSO takes this a step further by also considering
    word proximity, term frequency, language analysis, relevance scoring as well as
    partial matches, synonyms, and fuzzy matches. It allows for Boolean operators
    commonly found in database languages (**AND**, **OR**, and **NOT**). In simple
    terms, the matching style tends to focus only on providing results where the words
    are an exact match with less of a consideration as to whether those results are
    relevant. FTSO places more effort into making sure the results aren’t simply word
    matches but relevant content matches.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全文搜索选项**（**FTSO**）：另一种被称为FTSO的样式是一个相关概念，它在焦点和整体功能上有所不同。匹配样式侧重于精确匹配，而FTSO则更进一步，不仅考虑词语的相对位置、术语频率、语言分析、相关性评分，还包括部分匹配、同义词和模糊匹配。它允许在数据库语言中常见的布尔运算符（**AND**、**OR**和**NOT**）。简而言之，匹配样式通常只关注提供词语完全匹配的结果，而不太考虑这些结果是否相关。FTSO则更加注重确保结果不仅仅是词语匹配，而是内容匹配。'
- en: '**Language analyzers**: Have you ever wondered how technologies such as Elasticsearch
    deal with information that it finds in a language other than its native English?
    Believe it or not, it can accomplish that feat because Elasticsearch has integrated
    a variety of language analyzers. That includes analyzers that emphasize features
    other than spoken languages. Here are a few examples of some of the more popular
    language analyzers Elasticsearch uses:'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言分析器**：你有没有想过，像Elasticsearch这样的技术如何处理它在非英语语言中找到的信息？信不信由你，它能够完成这个任务，因为Elasticsearch集成了各种语言分析器。这包括一些不仅强调口语语言特征的分析器。以下是Elasticsearch使用的几种流行语言分析器的示例：'
- en: '**Standard analyzer**: Grammar-based tokenization for most languages'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准分析器**：基于语法的分词方法，适用于大多数语言'
- en: '**Simple analyzer**: Focuses on non-letter characters and lowercase normalization'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单分析器**：侧重于非字母字符和小写规范化'
- en: '**Whitespace analyzer**: Tokenizes each word that is separated by whitespace'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**空白分析器**：对每个由空格分隔的单词进行标记化处理'
- en: '**Keyword analyzer**: Tokenizes the entire input string'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关键字分析器**：对整个输入字符串进行标记化处理'
- en: '**Spoken languages**: English, French, German, Spanish, and many others'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**口语语言**：英语、法语、德语、西班牙语等多种语言'
- en: '**Faceted searching and aggregations**: Finally, Elasticsearch offers an advanced
    searching style known as faceted searching and aggregations, which involves complex
    data analysis. Faceted searching is sometimes called **faceted navigation**. This
    is a method that filters data based on different attributes of the data. These
    different filtering dimensions are sometimes called facets, hence the name. Some
    examples of these facets could be things such as color, price range, and shopping
    category. This style provides an avenue for the user to determine how they might
    like to refine their search. It allows the user to drill down to very precise
    levels of searching.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分面搜索和聚合**：最后，Elasticsearch提供了一种称为分面搜索和聚合的高级搜索样式，涉及复杂的数据分析。分面搜索有时也称为**分面导航**。这是一种根据数据的不同属性进行数据过滤的方法。这些不同的过滤维度有时称为分面，因此得名。这些分面的一些示例可能是颜色、价格范围和购物类别等内容。这种样式为用户提供了一种确定他们希望如何细化搜索的途径。它允许用户深入到非常精确的搜索级别。'
- en: 'Working together with faceted searching is aggregated searching. This is done
    at the beginning of the search to build the most robust supply of potentially
    relevant information for the faceted search to surgically discover the information
    most needed by the user. Three primary types of aggregations are utilized by Elasticsearch:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与分面搜索共同工作的是聚合搜索。这是在搜索开始时进行的，以建立最丰富的潜在相关信息供分面搜索发现用户最需要的信息。Elasticsearch使用三种主要类型的聚合：
- en: '**Bucket aggregations**: Using a logical partition to group documents with
    related attributes'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**桶聚合**：使用逻辑分区来组合具有相关属性的文档'
- en: '**Metric aggregations**: Performs calculations on the data within each bucket'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**度量聚合**：在每个桶内对数据执行计算'
- en: '**Pipeline aggregations**: A secondary aggregation that operates based on output
    from other aggregations'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道聚合**：基于其他聚合输出运行的次要聚合'
- en: In addition to having a larger pool of considerations for the search, Elasticsearch
    also offers data indexing in near real time. One of the methods it uses to deliver
    that feature is something called **inverted indexing**. Whereas traditional indexing
    is when data is stored in a database and is organized based on the total document,
    inverted indexing organizes the data based on the words and terms within the documents.
    The index is built while they are initially parsed and analyzed before being placed
    into an optimized data structure for storage. That structure is the index itself.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对搜索考虑因素更多之外，Elasticsearch还提供几乎实时的数据索引功能。它用来提供这一功能的方法之一被称为**倒排索引**。传统索引是将数据存储在数据库中，并根据整个文档进行组织。而倒排索引是根据文档中的单词和术语组织数据。索引是在初始解析和分析时构建的，然后放入优化的数据结构进行存储。这个结构就是索引本身。
- en: To further support near-real-time indexing, Elasticsearch first writes a document
    to a transaction log called a **Write Ahead Log** (**WAL**) immediately before
    the indexing process begins. This is done to protect the data in the case of a
    system failure. Even a brief hiccup in machine function could corrupt the indexing
    process. So, by using a WAL, Elasticsearch can immediately recover the data and
    begin the indexing process again.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步支持近实时索引，Elasticsearch在索引过程开始之前首先将文档写入称为**预写式日志**（**WAL**）的事务日志。这样做是为了在系统故障时保护数据。即使机器功能出现短暂的故障，也可能会损坏索引过程。因此，通过使用WAL，Elasticsearch可以立即恢复数据并重新开始索引过程。
- en: For the sake of data accuracy, Elasticsearch refreshes the indexes every second.
    When this occurs, it reinforces the availability of the information in the index
    for search operations. That process then gives way to near-real-time searching,
    making the overall process of retrieving information by Elasticsearch incredibly
    efficient in nature. This is a necessary component for the success of any application
    that depends on quick access to information for analytics, such as an SIEM.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保数据准确性，Elasticsearch 每秒刷新一次索引。当发生这种情况时，它会加强索引中信息的可用性，以便进行搜索操作。这个过程使得接近实时的搜索成为可能，从而使得
    Elasticsearch 检索信息的整体过程在本质上非常高效。这是任何依赖快速访问信息进行分析的应用程序成功的必要组件，例如 SIEM。
- en: Logstash
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Logstash
- en: Logstash is part of the ELK stack with capabilities for ingesting data, transforming/enriching
    it if needed, as well as transporting it from one place to another. While Elasticsearch
    is the core of the ELK stack, Logstash offers some parallel abilities and serves
    as the intermediary that supplies Elasticsearch.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash 是 ELK 堆栈的一部分，具备数据摄取、必要时进行数据转换/丰富，并将其从一个地方传输到另一个地方的能力。虽然 Elasticsearch
    是 ELK 堆栈的核心，但 Logstash 提供了一些并行的功能，并充当着提供 Elasticsearch 数据的中介。
- en: 'Logstash can ingest data from a vast array of data sources. Some of these sources
    might include the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash 可以从各种数据源中获取数据。以下是一些可能的数据源：
- en: Log files
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志文件
- en: Databases
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据库
- en: '**Application programming** **interfaces** (**APIs**)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用程序编程** **接口**（**APIs**）'
- en: Message queues
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息队列
- en: '**Internet of Things** (**IoT**) devices'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物联网**（**IoT**）设备'
- en: After ingesting the data, Logstash offers something called **data transformation**
    and **data enrichment**. This is a critical feature for any analytic application
    that alerts on anomalies. It performs these actions through the use of filters.
    These filters allow users to manipulate incoming data and then send it to the
    destination, where it will become accessible to Elasticsearch.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在摄取数据后，Logstash 提供了所谓的**数据转换**和**数据丰富**功能。这是任何依赖异常警报的分析应用程序的关键特性。它通过使用过滤器来执行这些操作。这些过滤器允许用户操作传入的数据，然后将其发送到目标位置，供
    Elasticsearch 访问。
- en: There are many filters, around 50, but we are going to focus on some of the
    simpler and more popular filters here. Feel free to dig in and research new filters
    as you develop and hone your ELK stack skillset. In the meantime, let’s cover
    a few that folks will start learning and training with.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤器有很多，大约有 50 个，但我们将在这里重点介绍一些较简单且受欢迎的过滤器。随着你开发和提升 ELK 堆栈技能，可以随时深入研究并了解新的过滤器。与此同时，让我们先覆盖一些大家开始学习和训练时会接触到的。
- en: One such filter is the **Grok Filter**. Logstash uses this to parse any type
    of structured log data – which Grok derives from unstructured data – that seems
    to follow a pattern. This action allows Logstash to also extract fields it deems
    meaningful from any unstructured or semi-structured log messages. It uses regular
    expressions to try and recognize and define patterns.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个过滤器是**Grok 过滤器**。Logstash 使用它来解析任何类型的结构化日志数据——这些数据是由 Grok 从非结构化数据中派生出来的——它似乎遵循某种模式。此操作还允许
    Logstash 从任何非结构化或半结构化的日志消息中提取它认为有意义的字段。它使用正则表达式尝试识别并定义模式。
- en: The **Date Filter** is another transformation tool. As you might expect, this
    tool is used to parse and standardize incoming log data with date and time. You
    can customize the date format patterns you want it to parse. This filter is quite
    useful for any log files that contain any sort of date or timestamp. Any properly
    generated log file should have such stamps on them.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**日期过滤器**是另一个转换工具。正如你所预期的，这个工具用于解析和标准化传入的日志数据中的日期和时间。你可以自定义它要解析的日期格式模式。这个过滤器对于包含任何日期或时间戳的日志文件非常有用。任何正确生成的日志文件都应该包含这样的时间戳。'
- en: There is also a **Translate Filter**, which performs data lookups against external
    mapping files. The filter uses this information to enrich the data coming in through
    the logs. This filter allows you to define key-value pairs within a dictionary
    file. The filter can then enrich the data by matching and replacing specific values
    with corresponding dictionary values.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个**翻译过滤器**，它通过外部映射文件进行数据查找。该过滤器利用这些信息来丰富通过日志传入的数据。该过滤器允许你在字典文件中定义键值对。然后，过滤器通过匹配并用相应的字典值替换特定值，从而丰富数据。
- en: The aptly named **Mutate Filter** provides Logstash with a variety of operations
    that it can use to manipulate and modify data. With this filter, you can rename
    fields and add or remove them. You can also convert their types and modify the
    values contained within the fields. You can even do things such as split and trim
    strings or concatenate one string with another.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 名副其实的**Mutate 过滤器**为 Logstash 提供了多种操作，可以用来操控和修改数据。使用这个过滤器，你可以重命名字段，添加或删除字段。你还可以转换字段的类型并修改字段中的值。甚至可以做一些诸如拆分和修剪字符串，或将一个字符串与另一个字符串连接的操作。
- en: If you want to add geographical information to the data, you will use the **GeoIP
    Filter**. It accomplishes this by performing IP address lookups. By mapping IP
    addresses to corresponding geographic locations, you can enrich your data with
    helpful information such as city, state, and country, as well as latitude and
    longitude.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想向数据添加地理信息，可以使用**GeoIP 过滤器**。它通过执行 IP 地址查询来实现这一点。通过将 IP 地址映射到相应的地理位置，你可以为数据增添有用的信息，如城市、州、国家以及纬度和经度。
- en: Logstash uses a **User-Agent Filter** to parse strings typically coming from
    weblogs, which enables it to extract important details about the tools used on
    the endpoint. This could include device type, OS, web browser brand, and version.
    Having this information can allow an analyst to gain a powerful insight into the
    devices and browsers used on either end of the data flow.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash 使用**用户代理过滤器**来解析通常来自网页日志的字符串，这使得它能够提取关于端点上使用的工具的重要细节。这可能包括设备类型、操作系统、网页浏览器品牌和版本。拥有这些信息，可以让分析人员深入了解数据流两端使用的设备和浏览器。
- en: '**JavaScript Object Notation** (**JSON**) is one of the most commonplace code-related
    language formats found in defensive security tools. You are not likely to ever
    find a mainstream SIEM or other cybersecurity product that doesn’t have compatibility
    with JSON and for that matter, at least an option or two to present some of the
    information that’s utilized by the application in JSON format. Logstash is no
    exception. It can easily handle JSON formatted data and is even capable of handling
    nested JSON structures. A nested feature in any programming or coding environment
    is when such a feature is found included within itself.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**JavaScript 对象表示法**（**JSON**）是防御性安全工具中最常见的代码相关语言格式之一。你几乎不可能找到一个主流的 SIEM 或其他网络安全产品，它不兼容
    JSON 格式，而且至少有一两种方式可以将应用程序使用的一些信息以 JSON 格式呈现。Logstash 也不例外。它可以轻松处理 JSON 格式的数据，甚至能够处理嵌套的
    JSON 结构。嵌套特性是在任何编程或编码环境中，当一个特性被包含在其自身之内时，称为嵌套。'
- en: Spreadsheet fans will be excited to learn that Logstash offers a **CSV Filter**.
    This filter allows Logstash to parse and manipulate comma-separated values. That
    is the common format that’s used when a spreadsheet user extracts and/or stores
    data that can be presented in a spreadsheet manner. This filter handles a variety
    of configurations, such as delimiter, header, row, and column mapping.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 喜欢使用电子表格的朋友会很高兴得知，Logstash 提供了**CSV 过滤器**。这个过滤器允许 Logstash 解析和处理逗号分隔值。这是电子表格用户提取和/或存储可以以电子表格方式呈现的数据时使用的常见格式。此过滤器可以处理多种配置，例如分隔符、头部、行和列映射。
- en: Another format that is both commonly found and compatible across different systems
    is **eXtensible Markup Language** (**XML**). Logstash has an XML filter to allow
    it to parse data that comes in XML formats. This filter can extract specific elements
    and/or attributes out of XML documents and then convert them into structured fields
    that can be used later for processing.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见且跨系统兼容的格式是**可扩展标记语言**（**XML**）。Logstash 有一个 XML 过滤器，可以解析 XML 格式的数据。这个过滤器可以从
    XML 文档中提取特定的元素和/或属性，然后将它们转换为可以用于后续处理的结构化字段。
- en: The full collection of filters exceeds this list. These are just the most common
    and most likely to be used by Logstash. The complete filter collection makes Logstash
    a very robust data, normalization, and enrichment utility. As you might have picked
    up by now, Logstash is designed in a manner that allows it to deal with both structured
    and unstructured data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的过滤器集合超出了这个列表。这些只是最常见和最可能被 Logstash 使用的过滤器。完整的过滤器集合使 Logstash 成为一个非常强大的数据、标准化和增值工具。正如你现在可能已经发现的那样，Logstash
    采用了一种设计，使其能够处理结构化和非结构化数据。
- en: In dealing with all this data and all the different ways we’ve discussed how
    Logstash manipulates and enriches it, there is one critical coding style of a
    feature that it uses to control the data flow. Those who’ve worked with coding
    and/or software development before know of this type of data control flow as **conditionals**.
    Conditionals allow the user to control the flow based on certain specific conditions
    and criteria. Some might recognize these as an **if-else** syntax.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理所有这些数据和我们讨论的 Logstash 如何操作和丰富这些数据的不同方式时，有一个至关重要的编码风格特性，用于控制数据流。那些有编码和/或软件开发经验的人都知道这种数据控制流叫做**条件语句**。条件语句允许用户根据特定的条件和标准控制数据流。一些人可能会认识到这就是**if-else**
    语法。
- en: 'Here’s a fun example:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有趣的例子：
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As we’ve already touched on, Logstash is a bit of a liaison – a middleman or
    intermediary. That means it isn’t only relied upon for ingesting log information,
    it’s also heavily relied upon to enrich and forward that data with the primary
    output highway being Elasticsearch. It can output data in various formats but
    the most common are CSV and JSON, which we discussed earlier.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，Logstash 有点像是一个中介——一个中间人或中介者。这意味着它不仅仅被用来获取日志信息，还被大量依赖于对这些数据进行丰富并转发，主要的输出通道是
    Elasticsearch。它可以以多种格式输出数据，但最常见的格式是 CSV 和 JSON，正如我们之前讨论过的那样。
- en: There is a plugin for Logstash that C programmers will immediately recognize
    called **stdout** which stands for **standard output**. This plugin allows the
    data to be sent directly to the command line in console applications. The main
    reason for outputting data in that manner is to allow engineers working the Logstash
    instance to have the ability to immediately recognize certain data elements for
    debugging and application testing purposes.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash 有一个插件，C 程序员会立刻认出来，叫做**stdout**，它代表**标准输出**。这个插件允许数据直接发送到控制台应用程序的命令行中。以这种方式输出数据的主要原因是允许在
    Logstash 实例上工作的工程师能够立即识别某些数据元素，方便进行调试和应用测试。
- en: Another popular output destination is **Apache Kafka**, which is a distributed
    event streaming platform application that was originally developed by the popular
    social media platform LinkedIn. It was released as open source software under
    the Apache Software Foundation. It has earned a reputation for being a credible
    fault-tolerant and efficient utility capable of handling large volumes of data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的输出目的地是**Apache Kafka**，它是一个分布式事件流平台应用程序，最初由流行的社交媒体平台 LinkedIn 开发。它作为开源软件在
    Apache 软件基金会下发布。它以高效、容错可靠而著称，能够处理大量数据。
- en: 'Earlier, we talked about a means of data organization within Elasticsearch
    called the **bucket**. Logstash can also output to the organization that took
    this data organization principle and popularized it on a grand scale: Amazon.
    They did this via their **Simple Storage Service** (**S3**) buckets. It is extremely
    popular in Amazon’s cloud services and is fully integrated with **Amazon Web**
    **Services** (**AWS**).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们讨论了在 Elasticsearch 中组织数据的方式，叫做**桶**（**bucket**）。Logstash 也可以输出到采用这种数据组织原则并大规模推广的组织：亚马逊。亚马逊通过其**简单存储服务**（**S3**）桶来实现这一点。它在亚马逊的云服务中非常受欢迎，并且与**亚马逊网络服务**（**AWS**）完全集成。
- en: Logz.io is a popular cloud-based log management platform that Logstash is compatible
    with outputting to. While it is a log management and analytics platform, it does
    not focus on real-time monitoring, correlation between raw and enriched data sources,
    and security event analysis. Therefore, it falls just a neuron short of being
    considered a full-fledged SIEM.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Logz.io 是一个流行的基于云的日志管理平台，Logstash 可以将数据输出到这个平台。虽然它是一个日志管理和分析平台，但它并不专注于实时监控、原始数据与丰富数据源之间的关联以及安全事件分析。因此，它距离被认为是一个完整的
    SIEM 系统还差一个神经元。
- en: Another Logstash output integration is **Java Database Connectivity** (**JDBC**).
    JDBC is a Java API that allows developers to interact with relational databases
    using the SQL DB language. It provides interfaces and classes that are standard
    for performing database interactions such as connecting, executing SQL queries,
    updating, or processing results.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个 Logstash 输出集成是**Java 数据库连接**（**JDBC**）。JDBC 是一个 Java API，允许开发者使用 SQL 数据库语言与关系数据库进行交互。它提供了标准的接口和类，用于执行数据库交互操作，如连接、执行
    SQL 查询、更新或处理结果。
- en: '**Redis** is a similar storage option in that it can be used as a database,
    cache, or message broker. Logstash outputs this in-memory data structure storage
    as well. Something unique about Redis is that it stores its primary data in RAM
    to allow for lightning-fast reading and writing operations. It supports a very
    wide range of data types. It utilizes common database methods such as a key-value
    pair model. This is where the data is stored as a pair with a unique key to identify
    them. It focuses heavily on the use of keys and data structures such as strings,
    lists, sets, hashes, and sorted sets. The appeal of Redis is the incredibly fast
    and efficient way it manipulates, stores, and recalls data.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**Redis** 是一个类似的存储选项，可以用作数据库、缓存或消息代理。Logstash 也会输出这种内存数据结构存储。Redis 的独特之处在于，它将主要数据存储在
    RAM 中，以实现快速的读写操作。它支持非常广泛的数据类型，并采用常见的数据库方法，如键值对模型。这种模型将数据作为一个键值对存储，每个键都有一个唯一的标识符。它特别重视键和数据结构的使用，如字符串、列表、集合、哈希和有序集合。Redis
    的吸引力在于其处理、存储和回调数据的极高速度和效率。'
- en: Kibana
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kibana
- en: Kibana is the ELK stack utility that helps users explore, visualize, and analyze
    the data that’s been collected and aggregated by the other components. It is designed
    to specifically work with Elasticsearch to provide a user-friendly interface for
    the end user. The most direct term to describe Kibana is simply **data visualization**.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 是 ELK 堆栈中的工具，帮助用户探索、可视化和分析其他组件收集和聚合的数据。它专门设计用于与 Elasticsearch 配合使用，提供一个用户友好的界面。最直接的术语来描述
    Kibana 就是 **数据可视化**。
- en: 'Data visualization in Kibana occurs from many methods and styles but there
    are seven primary methods, as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 中的数据可视化采用多种方法和风格，但有七种主要方法，具体如下：
- en: '**Charts**: Charts are probably the most logical expectation here. They include
    line charts, bar charts, area charts, pie charts, scatter plots, and others. This
    gives the users who are more visual learners an opportunity to see potential anomalies
    from a different perspective.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图表**：图表可能是这里最符合逻辑的预期。它们包括折线图、柱状图、面积图、饼图、散点图等。这为那些更偏视觉学习的用户提供了一个从不同角度发现潜在异常的机会。'
- en: '**Maps**: While many software applications will make use of maps, Kibana takes
    this to a higher level. Users can create choropleth maps, symbol maps, and maps
    based on grid coordinates. A **choropleth map** is a type of thematic map that
    focuses on a specific theme or topic. The choropleth style is to use different
    colors or patterns to represent geographical areas/regions. Usually, that would
    be countries, states/provinces, or counties/shires. You have probably seen many
    of these types of maps in your childhood classrooms. Symbol maps are similar but
    instead focus on using symbols to identify and represent specific geographical
    features, while grid coordinate maps are popular when aligned with latitude and
    longitude.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**地图**：虽然许多软件应用程序都使用地图，但 Kibana 将这一功能提升到了更高的水平。用户可以创建分级图、符号图以及基于网格坐标的地图。**分级图**是一种专题地图，专注于某个特定的主题或话题。分级图的风格是使用不同的颜色或图案来表示地理区域/地区，通常是国家、省/州或县/区。你可能在童年课堂上看到过这些类型的地图。符号图与之类似，但它更侧重于使用符号来识别和表示特定的地理特征，而网格坐标地图在与经纬度对齐时尤为常见。'
- en: '**Timelion**: Timelion, as the name suggests, is a data visualization plugin
    for Kibana that allows end users to manipulate and analyze time-based data. Like
    most utilities associated with the ELK stack, Timelion can be used for data transformation,
    aggregations, and mathematical calculations. You can perform averages, percentiles,
    and sums, among other functions. It allows the use of variables, which is handy
    in dealing with irregular time series. The typical visualization that’s created
    by Timelion comes in the form of a line chart. When applied to technology, line
    charts are nearly always present in the form of an *X* and *Y*-axis, where the
    *X*-axis is representative of time and the *Y*-axis represents the data values.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Timelion**：顾名思义，Timelion 是一个用于 Kibana 的数据可视化插件，允许最终用户处理和分析基于时间的数据。像大多数与 ELK
    堆栈相关的工具一样，Timelion 可以用于数据转换、聚合和数学计算。你可以执行平均值、百分位数、求和等功能。它支持使用变量，这对于处理不规则的时间序列非常有用。Timelion
    创建的典型可视化图表通常以折线图的形式出现。当应用于技术时，折线图几乎总是以 *X* 和 *Y* 轴的形式存在，其中 *X* 轴表示时间，*Y* 轴表示数据值。'
- en: '**Time Series Visual Builder** (**TSVB**): The TSVB is a visualization option
    in Kibana that focuses on the end user’s comfort. It allows them to build complicated
    time series visualizations with a simple drag-and-drop interface. It gives access
    to a variety of chart types, mathematical aggregations, and custom styling.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间序列可视化构建器**（**TSVB**）：TSVB是Kibana中的一个可视化选项，侧重于最终用户的使用舒适度。它允许用户通过简单的拖放界面构建复杂的时间序列可视化。它提供了多种图表类型、数学聚合和自定义样式的访问权限。'
- en: '**Gauge and metric**: Kibana’s version of gauge and metric visualizations is
    directed at providing visuals that represent a single value within a specific
    range. This is to allow users to present such simple values in a variety of ways,
    including sparklines. **Sparklines** are small and condensed line bar charts or
    sometimes simple line charts that are meant to display variations in data or trends
    within a condensed space. They’re meant to give a quick visual *shape* or look
    at the data.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仪表盘和指标**：Kibana版本的仪表盘和指标可视化旨在提供代表特定范围内单一值的视觉效果。这允许用户以多种方式呈现这些简单的值，包括火花图（sparklines）。**火花图**是小型的紧凑线条柱状图，或者有时是简单的折线图，旨在显示数据的变化或趋势，适合在紧凑的空间中展示。它们旨在快速提供数据的*形状*或趋势视觉。'
- en: '**Heat maps**: One of the more popular data visualizations in the business
    world today is heat maps. Heat maps are two-dimensional and often present in the
    form of a global or other large geographical map. They are popularly used to display
    trends, correlations, and patterns to help analysts solve complicated and/or wide-ranging
    problems. One of the more popular heat map usages in the world of cybersecurity
    is to show the origins of cyberattacks or source IP locations. Typically, heat
    maps have carefully integrated color schemes so that when multiple entities cross
    paths, it increases the intensity of the color, such as when some mobile phone
    GPS maps darken the route color from blue to yellow and eventually to red, depending
    on how congested the traffic is in a particular location. Heat maps use labels
    and annotations, but the best ones will keep this concise and limited to keep
    the visualization appearing clean to the observer.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**热力图**：在今天的商业世界中，热力图是最受欢迎的数据可视化之一。热力图是二维的，通常以全球或其他大型地理地图的形式呈现。它们通常用于显示趋势、关联性和模式，帮助分析师解决复杂和/或广泛的问题。在网络安全领域，热力图的一个常见用途是展示网络攻击的来源或源IP位置。通常，热力图会精心集成颜色方案，当多个实体交叉时，颜色的强度会增加，比如当一些手机GPS地图根据特定地点的交通拥堵情况将路线颜色从蓝色逐渐加深到黄色，再到红色。热力图使用标签和注释，但最好的热力图会保持简洁，限制这些信息，以便使可视化对观察者来说保持清晰。'
- en: '**Tag clouds**: Tag clouds, also known as **word clouds**, are visualizations
    of text data that is usually shown with words in different sizes and colors based
    on category and prominence. They tend to measure word prominence based on frequency.
    The algorithms that generate these sorts of maps usually perform a bit of pre-processing
    before deciding on the final output. They do this to remove common words that
    are not likely related to data, such as prepositions and pronouns. This is a form
    of text sanitization, similar to tokenization, which is seen in Elasticsearch
    text-matching methods.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签云**：标签云，也叫做**词云**，是文本数据的可视化，通常通过按类别和突出性显示不同大小和颜色的单词。它们通常根据频率来衡量单词的重要性。生成这些图形的算法通常会在决定最终输出之前执行一些预处理操作，去除一些不太可能与数据相关的常见词汇，比如介词和代词。这是一种文本清理形式，类似于Elasticsearch中的分词方法。'
- en: Another type of visualization offered by Kibana is **dashboard creation**. This
    is the overarching element that is most associated with Kibana and the one that
    makes use of the different charting and other visualizations we’ve just explored.
    A Kibana dashboard provides a centralized view where multiple visualizations can
    be combined, and queries and filters can be applied to create a fully customized
    layout. They help track what is known in the business world as **key performance
    metrics** (**KPIs**) in near real time.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana提供的另一种可视化类型是**仪表板创建**。这是与Kibana最相关的总体元素，也是利用我们刚刚探索的不同图表和其他可视化的一个功能。Kibana仪表板提供了一个集中视图，可以将多个可视化组合在一起，并可以应用查询和过滤器，创建一个完全自定义的布局。它们有助于近实时跟踪业务世界中所称的**关键绩效指标**（**KPI**）。
- en: Kibana provides support for users to perform ad hoc searches. This is done across
    indexed data in Elasticsearch but with the benefit of a visual search interface.
    You can construct queries using simple search syntax if you like. The more advanced
    users have the option of using the Lucene query syntax, which is helpful for precise
    searches. The search results are displayed instantly and then you can further
    refine, filter, and sort the data if you wish to continue drilling down into specific
    subsets of information.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 支持用户执行临时搜索。这是在 Elasticsearch 中索引数据的基础上进行的，且具有可视化的搜索界面。如果你愿意，可以使用简单的搜索语法来构造查询。更高级的用户则可以选择使用
    Lucene 查询语法，这对精确搜索非常有帮助。搜索结果会立即显示，之后你可以进一步细化、过滤和排序数据，继续深入分析特定的子集信息。
- en: Speaking of queries and filters, Kibana provides a way to visually build queries
    and filters so that you can deeply explore data. Kibana’s **domain-specific language**
    (**DSL**) allows users to construct complex queries and aggregations to extract
    very deep and meaningful insights from the data. You can use filters to narrow
    down data based on specific criteria, such as time range, attributes, or custom-defined
    conditions.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 说到查询和过滤器，Kibana 提供了一种可视化构建查询和过滤器的方式，帮助你深入探索数据。Kibana 的 **领域特定语言**（**DSL**）允许用户构建复杂的查询和聚合，以便从数据中提取深刻且有意义的见解。你可以使用过滤器，根据特定条件（如时间范围、属性或自定义定义的条件）来缩小数据范围。
- en: One of the greater strengths Kibana has is its ability to excel at time-based
    analysis. This allows you to explore data trends, patterns, and anomalies more
    thoroughly over a certain period. You can adjust the time ranges, set custom time
    intervals, and even apply histograms with dates so that you can analyze data at
    different levels of granularity. The time picker enables you to specify the time
    range you wish to analyze.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 的一大优势是其在基于时间的分析上的卓越表现。这使你能够在一定时间范围内更深入地探索数据趋势、模式和异常。你可以调整时间范围，设置自定义时间间隔，甚至应用带有日期的直方图，从而在不同的粒度级别上分析数据。时间选择器使你能够指定你希望分析的时间范围。
- en: '**Geospatial analysis** is another feature that’s offered that allows you to
    visualize and analyze location-based data. You can use this feature to plot data
    on interactive maps, geocode IP addresses, apply geospatial aggregations, and
    visualize density using heatmaps. A nice bonus from Kibana is the support for
    multiple map layers, custom base maps, and geographic shape files. It also offers
    geospatial extensions, the most common being the Elastic Maps Service.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**地理空间分析**是 Kibana 提供的另一个功能，允许你可视化和分析基于位置的数据。你可以使用此功能在交互式地图上绘制数据，地理编码 IP 地址，应用地理空间聚合，并使用热力图可视化密度。Kibana
    还支持多个地图层、自定义底图和地理形状文件，这是一大亮点。它还提供地理空间扩展，其中最常见的是 Elastic Maps Service。'
- en: Kibana allows users to set up alerts and monitors to track specific events or
    conditions in real-time data. You can define alert rules based on a variety of
    factors, including data thresholds, patterns, or other conditions. It can also
    work with external notifications such as email and the popular webhook, among
    others, as the alerts are triggered. It is this set of features that takes the
    ELK stack one step closer to an authentic SIEM environment than most log ingestion
    and manipulation suites.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 允许用户设置警报和监视器，以实时跟踪特定事件或条件。你可以根据多种因素定义警报规则，包括数据阈值、模式或其他条件。它还可以与外部通知系统（如电子邮件和流行的
    webhook 等）协同工作，触发警报。正是这套功能，使得 ELK 堆栈在很多日志摄取和处理工具中更接近于一个真正的 SIEM 环境。
- en: Kibana isn’t only a data visualization tool – it also offers an element of security.
    It provides a very robust set of security features to control access to data.
    It does this by supporting authentication and authorization mechanisms, **role-based
    access control** (**RBAC**), and integration with external authentication providers
    such as **Lightweight Directory Access Protocol** (**LDAP**) and/or the **Security
    Assertion Markup** **Language** (**SAML**).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 不仅仅是一个数据可视化工具——它还提供了安全性功能。它提供了一套非常强大的安全功能来控制对数据的访问。它通过支持身份验证和授权机制、**基于角色的访问控制**（**RBAC**），以及与外部身份验证提供商（如
    **轻量目录访问协议**（**LDAP**）和/或 **安全断言标记语言**（**SAML**））的集成，来实现这一点。
- en: The ELK stack, as presented thus far, works with data manipulation/enrichment,
    visualization, and overall processing. However, we need to have a way of gathering
    this information and sending it to the ELK stack in the first place. Enter Beats.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ELK 堆栈到目前为止，主要用于数据处理/增强、可视化以及整体处理。然而，我们首先需要一种方式来收集这些信息并将其发送到 ELK 堆栈。这个过程就需要
    **Beats**。
- en: Agents and monitoring
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代理与监控
- en: There are a couple of prominent additional applications that are associated
    with the ELK stack. They are the open source Beats and the commercially available
    X-Pack. It is the addition of these two components that transitions the ELK stack
    into a fully functioning SIEM. Together, they provide data collection and shipping,
    alerting and notification, machine learning for detecting hidden anomalous behavior,
    and automated reporting.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些与 ELK 堆栈相关的突出的附加应用程序。它们是开源的 Beats 和商业化的 X-Pack。正是这两个组件的加入，使 ELK 堆栈成为一个完整的
    SIEM 系统。它们共同提供数据收集与传输、警报与通知、用于检测隐藏异常行为的机器学习，以及自动化报告功能。
- en: Beats
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Beats
- en: 'Beats is a group of data collection and transportation agents. These are sometimes
    referred to as **data shippers**. They are lightweight – miniature – applications
    that are installed on endpoints so that they include personal computers, servers,
    and other network devices for the sole purpose of collecting data to ship off
    to Elasticsearch and Logstash for further processing in real time. Beats collect
    operational data from the devices they are installed on. It gets it from different
    sources, including logs and network traffic, along with other relevant information.
    The Beats family includes various specialized data shippers, such as **Filebeat**
    for log files, **Metricbeat** for machine metrics, **Packetbeat** for network
    data, and **Auditbeat** for audit events, among others. Beats uses lightweight
    agents installed on servers or systems to efficiently collect data with minimal
    resource usage. Let’s look at these in some detail:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Beats 是一组数据收集和传输代理，通常被称为**数据运输工具**。它们是轻量级的——微型——应用程序，安装在终端上，包括个人计算机、服务器和其他网络设备，专门用于收集数据并将其发送到
    Elasticsearch 和 Logstash 以便实时进一步处理。Beats 从其安装的设备收集操作数据。它从不同的来源获取数据，包括日志、网络流量以及其他相关信息。Beats
    家族包括各种专门的数据运输工具，例如用于日志文件的 **Filebeat**，用于机器指标的 **Metricbeat**，用于网络数据的 **Packetbeat**，以及用于审计事件的
    **Auditbeat** 等。Beats 使用安装在服务器或系统上的轻量级代理，高效地收集数据，并最大限度地减少资源使用。让我们详细了解一下这些：
- en: '**Filebeat**: Filebeat is designed for collecting and shipping log files. It
    can monitor log directories and files. It takes the data that it collects and
    ships it off to Elasticsearch or Logstash, where it will be enriched and/or further
    processed and analyzed. Filebeat supports different log file formats, including
    plain text logs, JSON logs, syslog, and others.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Filebeat**：Filebeat 旨在收集和传输日志文件。它可以监控日志目录和文件。它将收集的数据发送到 Elasticsearch 或 Logstash，在那里对数据进行增强和/或进一步处理与分析。Filebeat
    支持不同的日志文件格式，包括纯文本日志、JSON 日志、syslog 等。'
- en: '**Metricbeat**: This beat collects system-level metrics and statistics, such
    as CPU usage, memory utilization, disk I/O, and network metrics among others.
    It can gather metrics from various sources, such as OSs, services, containers,
    databases, and cloud platforms. Just like Filebeat operates, Metricbeat ships
    the collected metrics to Elasticsearch or Logstash for enrichment and further
    processing, storage, analysis, and visualization.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Metricbeat**：该 Beats 收集系统级别的指标和统计信息，例如 CPU 使用率、内存利用率、磁盘 I/O 以及网络指标等。它可以从各种来源收集指标，如操作系统、服务、容器、数据库和云平台。与
    Filebeat 的操作方式相同，Metricbeat 将收集到的指标发送到 Elasticsearch 或 Logstash 进行增强、进一步处理、存储、分析和可视化。'
- en: '**Packetbeat**: Wouldn’t it be great to capture these types of metrics from
    network data? That’s where Packetbeat comes into play. It captures network data
    and analyzes various protocols, including HTTP, DNS, MySQL, and Redis, along with
    many other protocols so that it can extract metadata and behavioral insights.
    Packetbeat’s data collection targets mean that it can provide near-real-time visibility
    into network traffic. This might include transaction details, request and response
    data, or network errors. Packetbeat can also be used for troubleshooting performance
    issues, identifying security threats, and monitoring network activity.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Packetbeat**：如果能够从网络数据中捕获这些类型的指标，那该有多好？这正是 Packetbeat 的作用。它捕获网络数据并分析各种协议，包括
    HTTP、DNS、MySQL 和 Redis 以及其他多种协议，从而提取元数据和行为洞察。Packetbeat 的数据收集目标意味着它可以提供近实时的网络流量可视化。这可能包括事务细节、请求和响应数据，或网络错误。Packetbeat
    也可以用于故障排除、性能问题诊断、安全威胁识别和网络活动监控。'
- en: '**Auditbeat**: This one is designed to collect and ship audit events from the
    Linux Audit Framework and Windows Event Logs. Auditbeat monitors system logs for
    audit events so that it can provide insight into user activities and look for
    unauthorized escalation of privileges by monitoring privilege changes, filesystem
    modifications, and others. Auditbeat has the potential to be a key player in identifying
    a compromised network and/or device. It is an invaluable tool that helps with
    compliance monitoring, security analysis, and detecting suspicious activities.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Auditbeat**：这个工具被设计用来收集并发送来自 Linux 审计框架和 Windows 事件日志的审计事件。Auditbeat 监控系统日志中的审计事件，以便提供用户活动的洞察，并通过监控权限变更、文件系统修改等，查找未经授权的特权升级。Auditbeat
    在识别被攻陷的网络和/或设备方面具有潜力。它是一个不可或缺的工具，有助于合规性监控、安全分析以及检测可疑活动。'
- en: As a package, Beats allows for some data transformation capabilities so that
    it can prepare the collected data for analysis. Beats provides lightweight functions
    that are applied to events before they’re sent on to Elasticsearch or Logstash.
    These functions are called Beats processors. They can enrich, filter, and modify
    the collected data. These processors allow operations such as filtering fields,
    enriching data with metadata, renaming fields, and much, much more.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个软件包，Beats 允许进行一些数据转换功能，以便为分析准备收集到的数据。Beats 提供了轻量级的功能，这些功能在数据发送到 Elasticsearch
    或 Logstash 之前应用到事件上。这些功能被称为 Beats 处理器。它们可以丰富、过滤和修改收集到的数据。这些处理器可以执行如过滤字段、用元数据丰富数据、重命名字段等操作，当然还有更多功能。
- en: One critical consideration for business technologies is the ability to adapt
    to sudden and/or unexpected rapid business growth. Thankfully, Beats is built
    to handle scalability. Not only that, but it is also designed to operate in high-volume
    and distributed environments. Beats can be easily scaled horizontally through
    the deployment of multiple Beats agents across different servers or systems. These
    agents provide configurable options to deal with load balancing, fault tolerance,
    and failover. This ensures data collection with high availability and resilience.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于商业技术来说，一个关键的考量是能够应对突如其来的、或意外的快速业务增长。幸运的是，Beats 被设计成能够处理可扩展性。不仅如此，它还被设计成能够在高流量和分布式环境中运行。Beats
    可以通过在不同服务器或系统上部署多个 Beats 代理来轻松实现水平扩展。这些代理提供了可配置选项来处理负载均衡、容错和故障转移，确保数据收集具有高可用性和弹性。
- en: Being an integral contribution to the Elastic stack, Beats’ design allows it
    to seamlessly integrate with the other components of the ELK stack – Elasticsearch,
    Logstash, and Kibana. They enable the efficient collection, transport, and processing
    of data before it is stored in Elasticsearch for further analysis and visualization.
    Beats can be configured to send data directly to Elasticsearch or Logstash for
    additional data transformation and enrichment.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 Elastic Stack 的重要组成部分，Beats 的设计使其能够与 ELK Stack 的其他组件——Elasticsearch、Logstash
    和 Kibana 无缝集成。它们实现了数据的高效收集、传输和处理，然后将数据存储在 Elasticsearch 中进行进一步分析和可视化。Beats 可以配置为直接将数据发送到
    Elasticsearch 或 Logstash，以进行额外的数据转换和丰富。
- en: That last part brings up a topic we should address. We’ve discussed a few times
    now that data can be sent directly to either Elasticsearch or Logstash and that
    Beats is also capable of sending it to either one. It’s important to examine these
    scenarios so that as we advance throughout our careers, we can select the appropriate
    configurations when setting up the data shippers.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一部分提出了一个我们应该讨论的话题。我们已经讨论过几次，数据可以直接发送到Elasticsearch或Logstash，而Beats也可以将数据发送到任一位置。分析这些场景非常重要，这样在我们职业生涯的发展过程中，才能在设置数据传输工具时选择合适的配置。
- en: That said, let’s consider that sending data directly to Elasticsearch or instead
    to Logstash as an intermediary both has its advantages and disadvantages.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们也要考虑直接将数据发送到Elasticsearch或通过Logstash作为中介的优缺点。
- en: 'The advantages of sending data directly to Elasticsearch are as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 直接将数据发送到Elasticsearch的优点如下：
- en: '**Simplicity**: Sending data directly to Elasticsearch means there’s one less
    stop the data needs to make before reaching its destination. By eliminating Logstash
    as an intermediary, we are keeping the data pipeline simplified.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简洁性**：将数据直接发送到Elasticsearch意味着数据在到达目的地之前少了一个停靠点。通过去除Logstash作为中介，我们简化了数据管道。'
- en: '**Scalability**: As we talked about just a short while ago, modern technology
    in business needs to be able to adjust to sudden and/or unexpected business growth.
    By directly sending data to Elasticsearch, we are creating an allowance for horizontal
    scaling as well. That will make it possible to distribute the workload across
    multiple Elasticsearch nodes. This significantly improves performance and gives
    us the ability to handle larger volumes of data.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：正如我们刚才提到的，现代商业技术需要能够适应突如其来或意外的业务增长。通过直接将数据发送到Elasticsearch，我们为水平扩展提供了可能。这将使得可以在多个Elasticsearch节点之间分配工作负载，从而显著提高性能，并让我们能够处理更大规模的数据量。'
- en: '**Efficiency**: Any time a stop is removed from a data flow, less processing
    overhead will be required. There is also going to be less lag/latency due to the
    ability of the data to keep moving past the point where a stop would otherwise
    be located. This gives us the near-real-time data ingestion and indexing of data
    that Elasticsearch is known for. That, in turn, leads to faster availability for
    search and analysis.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**效率**：每当从数据流中移除一个停止点时，所需的处理开销将减少。由于数据能够继续流动并超越原本应该存在停止点的位置，也会减少延迟/滞后。这使得我们能够实现Elasticsearch所知名的近实时数据摄取和数据索引。这反过来又加快了搜索和分析的可用性。'
- en: '**Streamlined architecture**: Supporting the greater efficiency argument, having
    Beats, or any data shipper for that matter, make a direct integration with Elasticsearch
    means we have a tighter and more straightforward architecture. That keeps the
    entire data flow simpler, which, in addition to improved performance, also means
    less surface for breaks and flaws.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简化的架构**：为了支持更高效率的论点，Beats或任何数据传输工具与Elasticsearch的直接集成意味着我们有一个更紧凑、更直接的架构。这保持了整个数据流的简洁性，除了提升性能外，还意味着减少了出现故障的可能性。'
- en: 'The disadvantages of sending data directly to Elasticsearch are as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 直接将数据发送到Elasticsearch的缺点如下：
- en: '**Limited data transformation**: We may be more efficient but we are also missing
    out on the benefits of Logstash. This means that by sending data directly to Elasticsearch,
    we no longer have the data enrichment and transformation capabilities that Logstash
    has to offer. Elasticsearch still has these values but not at the same level as
    Logstash. Logstash provides much more powerful filtering, enrichment, and data
    parsing capabilities so that it can preprocess and transform the data before indexing
    it.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限的数据转换**：虽然我们可能会提高效率，但也失去了Logstash的优势。这意味着，通过直接将数据发送到Elasticsearch，我们不再拥有Logstash所提供的数据丰富和转换功能。Elasticsearch虽然也具备这些功能，但不如Logstash强大。Logstash提供了更强大的过滤、丰富和数据解析能力，可以在将数据索引之前进行预处理和转换。'
- en: '**Complexity in certain scenarios**: There are going to be scenarios where
    the additional preprocessing of data would simplify the work Elasticsearch needs
    to do in the end. So, there are times when there will be enrichment that is better
    suited for Logstash. Ultra-complex data pipelines, especially those involving
    many data sources or more advanced transformations, would benefit better from
    the flexibility and capabilities of Logstash.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**某些场景中的复杂性**：在某些情况下，额外的数据预处理将简化 Elasticsearch 最终需要完成的工作。因此，有时候某些数据丰富工作更适合由
    Logstash 完成。尤其是那些涉及多个数据源或更复杂转换的超复杂数据管道，Logstash 的灵活性和功能会带来更好的帮助。'
- en: '**Compatibility with non-Elasticsearch systems**: There could be times when
    systems other than Logstash or possibly even other than Beats will ship data to
    Elasticsearch. If any of these systems do not integrate fully with Elasticsearch,
    there could be difficulties with normalizing the data being transferred. Any data
    coming from Logstash is certain to be compatible with Elasticsearch. Logstash,
    on the other hand, is set up with a multitude of plugins to assist with transforming
    data into a format that is compatible with Elasticsearch.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与非 Elasticsearch 系统的兼容性**：有时可能会出现除了 Logstash 或 Beats 以外的其他系统将数据发送到 Elasticsearch
    的情况。如果这些系统与 Elasticsearch 的集成不完全，可能会在数据规范化过程中遇到困难。来自 Logstash 的任何数据都肯定与 Elasticsearch
    兼容。另一方面，Logstash 配备了多种插件，可以帮助将数据转换为与 Elasticsearch 兼容的格式。'
- en: 'The advantages of using Logstash as an intermediary are as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Logstash 作为中介的优点如下：
- en: '**Data transformation and enrichment**: Logstash possesses very robust data
    processing capabilities that are simply greater in size and scope than Elasticsearch
    has to offer. Logstash is capable of advanced filtering, enriching, and parsing
    of data. It also has some alerting capabilities. It can support the more advanced
    styles due to its vast array of plugins and filters. Collectively, these abilities
    allow Logstash to transform data into a compatible format before passing it onto
    Elasticsearch.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据转换和丰富**：Logstash 拥有非常强大的数据处理能力，其规模和范围远超 Elasticsearch。Logstash 能够进行高级过滤、数据丰富和解析。它还具有一定的警报功能。凭借其大量的插件和过滤器，Logstash
    可以支持更为复杂的处理方式。综合这些功能，Logstash 能够在将数据传递给 Elasticsearch 之前，先将其转换为兼容的格式。'
- en: '**Compatibility and integration**: The vast array of plugins and filters that
    exist in Logstash allows for integration with many data sources and destinations.
    Logstash can support many different formats, protocols, and systems because of
    this. That also makes it compatible with many non-Elasticsearch systems, which
    it can grab data from and translate for Elasticsearch. This gives us what some
    would call data pipeline versatility.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**兼容性和集成**：Logstash 中存在大量的插件和过滤器，使其能够与许多数据源和目标进行集成。由于这一点，Logstash 可以支持多种不同的格式、协议和系统。这也使其能够兼容许多非
    Elasticsearch 系统，从中获取数据并为 Elasticsearch 进行转换。这赋予了我们所谓的数据管道灵活性。'
- en: '**Flexibility in complex scenarios**: Whereas Elasticsearch is not the best
    suited for extremely complex scenarios, Logstash excels in this area. These areas
    include complex data pipelines, and situations where data needs to be collected
    from multiple sources, transformed, combined, or split before being passed on
    to Elasticsearch. This increases overall granularity and flexibility when dealing
    with diverse requirements and data manipulation.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂场景中的灵活性**：虽然 Elasticsearch 并不适合极其复杂的场景，但 Logstash 在这一领域表现出色。这些领域包括复杂的数据管道，以及需要从多个数据源收集数据、转换、组合或拆分后再传递给
    Elasticsearch 的情况。这提高了处理多样化需求和数据操作时的整体粒度和灵活性。'
- en: 'The disadvantages of using Logstash as an intermediary are as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Logstash 作为中介的缺点如下：
- en: '**Increased complexity**: While Logstash does expand the ability to normalize
    many different types of data for Elasticsearch, it also introduces an additional
    stop in the data flow by adding a new component to the pipeline. That’s one more
    thing that must be configured, managed, and refined. This can increase the complexity
    and overhead of the system, which is especially concerning for smaller deployments
    that likely have simpler requirements.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增加的复杂性**：虽然 Logstash 增强了将多种不同类型数据标准化以便 Elasticsearch 使用的能力，但它也通过在数据流中增加一个新组件而带来了额外的停靠点。这意味着需要配置、管理和优化更多的内容。对于较小的部署，这可能会增加系统的复杂性和开销，特别是当这些部署的需求较简单时。'
- en: '**Potential performance impact**: Anytime an additional component is added
    to any technological pipeline, this means there will be an additional need for
    processing and overhead. This will reduce efficiency and increase the latency/lag
    of the data flow. In cases where there is a large volume of data flow and/or a
    large number of transformations and enrichments needed, there is likely to be
    a performance hit.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在的性能影响**：每当一个额外组件被添加到技术管道中，就意味着需要额外的处理和开销。这将降低效率，并增加数据流的延迟/滞后。在数据流量很大和/或需要大量转换和丰富的情况下，性能可能会受到影响。'
- en: '**Overhead and resource utilization**: Because there is extra technology, there
    is also going to be extra demand for the resources. That means more stress on
    the CPU, memory, and disk space compared to sending the data directly to the Elasticsearch
    integration. This might be more valuable while considering scenarios involving
    very high volumes of data or if the previously mentioned resources are limited.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开销和资源利用**：由于有额外的技术，资源的需求也会增加。这意味着与将数据直接发送到 Elasticsearch 集成相比，对 CPU、内存和磁盘空间的压力会更大。在涉及非常高数据量或上述资源有限的场景中，这可能更为有价值。'
- en: The general gist of Elasticsearch versus Logstash data ingestion is that sending
    data directly to Elasticsearch improves efficiency while reducing capabilities
    and sending it to Logstash first has the exact opposite effect of increasing capabilities
    while reducing efficiency. Elasticsearch greatly improves the overall efficiency
    and reduces preprocessing overhead. Logstash offers a plethora of data transformations
    where it can take data not compatible with Elasticsearch in its native format.
    Then, when it’s done processing it, it delivers it to Elasticsearch as a compatible
    ingestion platform. In the end, the decision about where to send the data is going
    to depend on how you feel about the expected complexity of the data pipeline,
    the need for data transformation, and compatibility with other systems in your
    environment.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 与 Logstash 数据摄取的基本区别在于，直接将数据发送到 Elasticsearch 提高了效率，但会减少某些功能；而首先将数据发送到
    Logstash 则恰恰相反，增加了功能但降低了效率。Elasticsearch 大大提高了整体效率，减少了预处理开销。Logstash 提供了丰富的数据转换功能，能够处理原生格式下与
    Elasticsearch 不兼容的数据。然后，当它处理完成后，会将其作为兼容的摄取平台传送到 Elasticsearch。最终，关于数据发送位置的决策将取决于你对数据管道复杂度、数据转换需求以及与环境中其他系统兼容性的预期。
- en: X-Pack
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: X-Pack
- en: Whereas the four components of the ELK stack we’ve discussed thus far are all
    free, open source applications, X-Pack is a partially free extension that offers
    additional paid services. It offers enhanced capabilities over the completely
    free applications.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今讨论的 ELK 堆栈中的四个组件都是免费的开源应用，而 X-Pack 是一个部分免费的扩展，提供额外的付费服务。它比完全免费的应用提供了增强的功能。
- en: 'It has five distinct offerings:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 它提供五个独立的功能：
- en: '**Security**: X-Pack delivers security through several actions. One offering
    is RBAC, which allows you to define and manage roles and privileges for users.
    This provides a strict level of control over documents, indexes, and other resources.
    It provides **Transport Layer Security** (**TLS**) encryption for secure communication
    between clients and Elasticsearch. X-Pack also offers secure user authentication
    by utilizing methods such as LDAP, Active Directory, and native authentication.
    This helps keep access to the Elasticsearch cluster safe and secure. It also has
    an audit logging feature so that it can log security-related events to help you
    monitor and track user and system actions for anomalous behavior.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：X-Pack 通过多种措施提供安全性。其中一项是 RBAC（基于角色的访问控制），它允许你定义和管理用户的角色和权限，从而对文档、索引和其他资源进行严格的控制。它为客户端与
    Elasticsearch 之间的安全通信提供 **传输层安全性** (**TLS**) 加密。X-Pack 还通过使用 LDAP、Active Directory
    和本地认证等方法提供安全的用户认证，帮助保持对 Elasticsearch 集群的安全访问。它还具有审计日志功能，能够记录与安全相关的事件，帮助你监控和追踪用户与系统行为中的异常情况。'
- en: '**Monitoring**: X-Pack provides detailed monitoring of Elasticsearch clusters.
    Some of the issues it monitors include node health, resource usage, and indexing
    rates. This is so that you can identify and respond to performance issues and
    bottlenecks. It will also look at query performance, identifying slow-responding
    queries and inefficient search patterns. You can set up monitoring alerts based
    on predefined conditions and thresholds. These alerts will provide notifications
    to you if a predefined condition or metric is met.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控**：X-Pack 提供了对 Elasticsearch 集群的详细监控。它监控的一些问题包括节点健康、资源使用情况和索引速率。这是为了让你能够识别和应对性能问题和瓶颈。它还会查看查询性能，识别响应缓慢的查询和低效的搜索模式。你可以根据预定义的条件和阈值设置监控警报。如果满足预定义条件或指标，这些警报将会通知你。'
- en: '**Alerting**: One of the most critical components of X-Pack is its alerting
    capabilities. It uses an application called **Watcher** to define and schedule
    alert rules based on query results and aggregations. Like all the big players
    in SIEM technology, it can perform actions such as providing automatic email notifications
    or Slack messages. It can also execute custom scripts, giving analysts the power
    to proactively respond to critical events.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**警报**：X-Pack 最关键的组件之一是其警报功能。它使用名为 **Watcher** 的应用程序来定义和安排基于查询结果和聚合的警报规则。像所有
    SIEM 技术的主要产品一样，它可以执行诸如提供自动电子邮件通知或 Slack 消息等操作。它还可以执行自定义脚本，赋予分析师主动响应关键事件的能力。'
- en: '**Machine learning (ML)**: X-Pack offers anomaly detection through its ML capability.
    This enables the automatic detection of anomalies in time series data, which assists
    analysts in identifying unusual patterns, deviations, or other anomalous behavior.
    It also uses ML to forecast future trends based on historical patterns that would
    otherwise be hard to detect or are hidden altogether. Something interesting about
    X-Pack’s ML is that it helps to identify what it believes to be the most influential
    factors that contribute to anomalous behavior. This helps investigators and incident
    responders with their examination of root cause analysis.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习（ML）**：X-Pack 通过其 ML 功能提供异常检测。这使得时间序列数据中的异常能够被自动检测出来，帮助分析师识别不寻常的模式、偏差或其他异常行为。它还使用机器学习来根据历史模式预测未来的趋势，这些趋势否则可能很难被发现或完全隐藏。X-Pack
    的 ML 特别之处在于，它能够帮助识别出它认为是导致异常行为的最有影响力的因素。这对于调查人员和事件响应人员进行根本原因分析非常有帮助。'
- en: '**Reporting**: Finally, X-Pack allows you to create and schedule reports, which
    is a well-rounded feature that, like alerting, will help transition your ELK stack
    into a full-fledged SIEM. It offers these reports in a variety of formats, such
    as **.pdf**, **.csv**, and **.xls**, which works wonderfully with the data formats
    that you’ll find in Elasticsearch. These reports can either be generated on-demand
    or they can be scheduled to run at a predefined interval.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**报告**：最后，X-Pack 允许你创建和安排报告，这是一项功能全面的特性，类似于警报功能，它将帮助你将 ELK 堆栈转变为一个完整的 SIEM。它提供多种格式的报告，如
    **.pdf**、**.csv** 和 **.xls**，这些格式与 Elasticsearch 中的数据格式非常契合。你可以按需生成这些报告，或者将它们安排在预定的时间间隔内运行。'
- en: With that, we’ve learned about an entire group of applications that make up
    what some call the Elastic SIEM. We’ve discussed the utilities we can use to collect,
    transport, enrich, and visually present security data. Now, it’s time to start
    setting up our technology to put it all into practice.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以上内容，我们已经了解了一整套被称为 Elastic SIEM 的应用程序。我们讨论了可以用来收集、传输、丰富和可视化安全数据的工具。现在，是时候开始设置我们的技术，实践这些内容了。
- en: Summary
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about the genealogy of Kali Linux and began to unpack
    one of the most popular toolsets offered by the Purple variant of that OS. We
    started to delve deeply into the Elastic Stack, sometimes called the ELK stack,
    as that will become one of the primary focal points of this book, being the core
    of Kali Purple. We gained a healthy understanding of how Elasticsearch and Logstash
    work with data, from ingesting it to enriching it to aggregating it. We also saw
    how this data can be presented visually through Kibana.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了 Kali Linux 的发展历程，并开始拆解该操作系统的紫色变种（Purple variant）提供的最流行工具集之一。我们深入探讨了
    Elastic Stack，通常被称为 ELK 堆栈，因为它将成为本书的主要焦点之一，作为 Kali Purple 的核心。我们深入了解了 Elasticsearch
    和 Logstash 如何处理数据，从数据的摄取到丰富再到聚合。我们还看到了如何通过 Kibana 将这些数据以可视化的方式呈现出来。
- en: After thoroughly examining how the ELK stack handles data, we started to examine
    the original data sources and how we glean data from there through Beats and pass
    it on to Elasticsearch, usually through Logstash as an intermediate stop, but
    not always. We studied the difference between sending data directly to Elasticsearch
    versus Logstash. We also peeked at the commercial component of the ELK stack,
    X-Pack, and were able to see how much of a full-fledged SIEM the ELK stack can
    become once X-Pack alerting is factored into the formula.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在彻底分析了 ELK 堆栈如何处理数据之后，我们开始检查原始数据源以及如何通过 Beats 从中提取数据并传递给 Elasticsearch，通常通过
    Logstash 作为中间停留点，但并非总是如此。我们研究了直接将数据发送到 Elasticsearch 与通过 Logstash 发送数据之间的差异。我们还了解了
    ELK 堆栈的商业组件 X-Pack，并能看到当 X-Pack 告警功能被纳入公式时，ELK 堆栈能变得多么像一个完整的 SIEM。
- en: Now that we have a robust understanding of the core abilities of Kali Purple,
    we can move on to the next phase of the learning process – practical application.
    In the next chapter, we will begin to set up our technology so that we can acquire,
    install, and configure our very own instance of Kali Purple. It’s time to take
    what we’ve been studying and put it into action!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对 Kali Purple 的核心功能有了扎实的理解，接下来可以进入学习过程的下一个阶段——实践应用。在下一章中，我们将开始设置我们的技术，以便获取、安装和配置我们自己的
    Kali Purple 实例。是时候将我们所学的知识付诸实践了！
- en: Questions
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Answer the following questions to test your knowledge of this chapter:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 回答以下问题，测试你对本章内容的理解：
- en: What is the ELK stack?
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是 ELK 堆栈？
- en: A herd of wild deer standing on top of each other
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一群野鹿站在彼此之上
- en: A group of open source software working together
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一组协同工作的开源软件
- en: Environmental Linux knowledge
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 环境 Linux 知识
- en: 'True or false: Beats is a commercial product that is part of the ELK but costs
    money to use.'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确还是错误：Beats 是一个商业产品，属于 ELK 的一部分，但使用它需要付费。
- en: 'True'
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确
- en: 'False'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误
- en: What is the primary difference between a pipeline versus other aggregations?
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 管道与其他聚合的主要区别是什么？
- en: This type of aggregation utilizes the results of other aggregations
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这种类型的聚合利用其他聚合的结果
- en: Other aggregations depend on this one being conducted first
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其他聚合依赖于首先执行这个聚合
- en: This aggregation revolves around a long, thin linear set of criteria
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这种聚合围绕一组长而细的线性标准展开
- en: The EPA must be notified in the event of a pipeline breach
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果发生管道泄漏，必须通知 EPA
- en: Where is data enriched within the Elastic stack pipeline (you may select more
    than one)?
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Elastic 堆栈管道中，数据在哪里得到增强（你可以选择多个选项）？
- en: When one of the Beats agents collects the data before shipping it
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当其中一个 Beats 代理在数据发货之前收集数据时
- en: Kibana only enriches the data if it is passed through both Logstash and Elasticsearch
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kibana 只有在数据通过 Logstash 和 Elasticsearch 时才会丰富数据
- en: Logstash
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Logstash
- en: Elasticsearch
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Elasticsearch
- en: Which ELK stack component helps the user visualize the data?
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个 ELK 堆栈组件帮助用户可视化数据？
- en: Elasticsearch
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Elasticsearch
- en: Logstash
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Logstash
- en: Kibana
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kibana
- en: Beats
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Beats
- en: X-Pack
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: X-Pack
- en: What are conditionals as they relate to the ELK stack?
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 条件语句在 ELK 堆栈中是什么？
- en: A device that processes the airflow, keeping it cool
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一种处理气流的设备，用以保持其冷却
- en: An agreement between the operator of the Kali Purple instance and their customer
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kali Purple 实例的操作员与其客户之间的协议
- en: An Elasticsearch process that only executes if all its demands are met
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个 Elasticsearch 进程，只有在满足所有需求的情况下才会执行
- en: A set of commands that will control the data flow based on whether predefined
    criteria are met
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一组命令，用于根据预定标准控制数据流
- en: Further reading
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the **BM25** **algorithm**: [https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables](https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解本章所涵盖的主题，可以查看**BM25** **算法**：[https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables](https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables)。
