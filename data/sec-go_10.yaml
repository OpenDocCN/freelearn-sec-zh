- en: Web Scraping
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络抓取
- en: Information gathering from the web can be useful for many situations. Websites
    can provide a wealth of information. The information can be used to help when
    performing a social engineering attack or a phishing attack. You can find names
    and emails for potential targets, or collect keywords and headers that can help
    to quickly understand the topic or business of a website. You can also potentially
    learn the location of the business, find images and documents, and analyze other
    aspects of a website using web scraping techniques.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 从网络收集信息在许多场合下都非常有用。网站可以提供大量信息，这些信息可以用来帮助进行社会工程学攻击或钓鱼攻击。你可以找到潜在目标的姓名和电子邮件，或者收集关键词和标题，帮助快速理解网站的主题或业务。你还可以通过网页抓取技术，了解业务的地理位置，找到图片和文档，分析网站的其他方面。
- en: Learning about the target allows you to create a believable pretext. Pretexting
    is a common technique attackers use to trick an unsuspecting victim into complying
    with a request that compromises the user, their account, or their machine in some
    kind of way. For example, someone researches a company and finds out that it is
    a large company with a centralized IT support department in a specific city. They
    can call or email people at the company, pretending to be a support technician,
    and ask them to perform actions or provide their password. Information from a
    company's public website can contain many details used to set up a pretexting
    situation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 了解目标有助于你创建一个可信的借口。借口（Pretexting）是攻击者用来欺骗毫无防备的受害者，诱使他们按照请求操作，从而损害用户、其账户或计算机的一种常见技巧。例如，有人研究了一家公司，发现它是一家大型公司，且在某个特定城市设有集中的
    IT 支持部门。他们可以打电话或发邮件给公司的人，假装自己是技术支持人员，要求他们执行某些操作或提供密码。从公司的公共网站上获得的信息可能包含许多可以用来设立借口的细节。
- en: Web crawling is another aspect of scraping, which involves following hyperlinks
    to other pages. Breadth-first crawling refers to finding as many different websites
    as you can and following them to find more sites. Depth-first crawling refers
    to crawling a single site to find all pages possible before moving on to the next
    site.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 网络爬虫是抓取的另一个方面，它涉及跟随超链接到其他页面。广度优先爬取指的是尽可能找到更多不同的网站并跟随它们，寻找更多的网站。深度优先爬取指的是爬取一个网站，找到所有可能的页面，然后再移动到下一个网站。
- en: In this chapter, we will cover web scraping and web crawling. We will walk you
    through examples of basic tasks such as finding links, documents, and images,
    looking for hidden files and information, and using a powerful third-party package
    named `goquery`. We will also discuss techniques for mitigating scraping of your
    own websites.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍网页抓取和网页爬取。我们将通过一些基本任务的示例，帮助你完成诸如查找链接、文档和图片，寻找隐藏文件和信息，使用一个强大的第三方包 `goquery`。我们还将讨论减少自己网站抓取的技巧。
- en: 'In this chapter, we will specifically cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将特别介绍以下主题：
- en: Web scraping fundamentals
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络抓取基础
- en: String matching
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符串匹配
- en: Regular expressions
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则表达式
- en: Extracting HTTP headers from a response
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从响应中提取 HTTP 头部
- en: Using cookies
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 cookies
- en: Extracting HTML comments from a page
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从页面中提取 HTML 注释
- en: Searching for unlisted files on a web server
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索网页服务器上未列出的文件
- en: Modifying your user agent
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改用户代理
- en: Fingerprinting web applications and servers
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对网页应用程序和服务器进行指纹识别
- en: Using the goquery package
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 goquery 包
- en: Listing all links in a page
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出页面中的所有链接
- en: Listing all document links in a page
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出页面中的所有文档链接
- en: Listing title and headings of a page
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出页面的标题和标题标签
- en: Calculating most common words used on a page
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算页面中最常用的单词
- en: Listing all external JavaScript sources of a page
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出页面中的所有外部 JavaScript 源
- en: Depth-first crawling
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度优先爬取
- en: Breadth-first crawling
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广度优先爬取
- en: Protecting against web scraping
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防止网页抓取
- en: Web scraping fundamentals
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络抓取基础
- en: Web scraping, as used in this book, is the process of extracting information
    from an HTML-structured page that is intended to be viewed by a human and not
    consumed programmatically. Some services provide an API that is efficient for
    programmatic use, but some websites only provide their information in HTML pages.
    These web scraping examples demonstrate various ways of extracting information
    from HTML. We'll look at basic string matching, then regular expressions, and
    then a powerful package named `goquery`, for web scraping.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中所说的网页抓取，是指从HTML结构化页面中提取信息的过程，这些页面是供人类查看的，而非供程序消费。一些服务提供了适合程序化使用的API，但有些网站只提供HTML页面中的信息。这些网页抓取示例展示了从HTML中提取信息的不同方法。我们将从基本的字符串匹配开始，然后是正则表达式，最后是一个强大的名为`goquery`的网页抓取包。
- en: Finding strings in HTTP responses with the strings package
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用strings包在HTTP响应中查找字符串
- en: To get started, let's look at making a basic HTTP request and searching for
    a string using the standard library. First, we will create `http.Client` and set
    any custom variables; for example, whether or not the client should follow redirects,
    what set of cookies it should use, or what transport to use.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始，我们先看一下如何使用标准库发起一个基本的HTTP请求，并查找字符串。首先，我们将创建`http.Client`并设置任何自定义变量；例如，客户端是否应该跟随重定向，应该使用哪些cookie，或者使用什么传输。
- en: The `http.Transport` type implements the network request operations to perform
    the HTTP request and get a response. By default, `http.RoundTripper` is used,
    and this executes a single HTTP request. For the majority of use cases, the default
    transport is just fine. By default, the HTTP proxy from the environment is used,
    but the proxy can also be specified in the transport. This might be useful if
    you want to use multiple proxies. This example does not use a custom `http.Transport`
    type, but I wanted to highlight how `http.Transport` is an embedded type within
    `http.Client`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`http.Transport`类型实现了执行HTTP请求并获取响应的网络请求操作。默认情况下，使用`http.RoundTripper`，它执行单个HTTP请求。对于大多数使用场景，默认的传输就足够了。默认情况下，环境中的HTTP代理会被使用，但代理也可以在传输中指定。如果你想使用多个代理，这可能会很有用。此示例未使用自定义的`http.Transport`类型，但我想强调的是，`http.Transport`是`http.Client`中的一个嵌入类型。'
- en: We are creating a custom `http.Client` type, but only to override the `Timeout`
    field. By default, there is no timeout and an application could hang forever.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在创建一个自定义的`http.Client`类型，但仅仅是为了重写`Timeout`字段。默认情况下，没有超时设置，应用程序可能会永远挂起。
- en: 'Another embedded type that can be overridden within `http.Client` is the `http.CookieJar`
    type. Two functions the `http.CookieJar` interface requires are: `SetCookies()`
    and `Cookies()`. The standard library comes with the `net/http/cookiejar` package,
    and it contains a default implementation of `CookieJar`. One use case for multiple
    cookie jars is to log in and store multiple sessions with a website. You can log
    in as many users, and store each session in a cookie jar and use each one, as
    needed. This example does not use a custom cookie jar.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可以在`http.Client`中重写的嵌入类型是`http.CookieJar`类型。`http.CookieJar`接口要求的两个函数是：`SetCookies()`和`Cookies()`。标准库中包含了`net/http/cookiejar`包，并且它包含了`CookieJar`的默认实现。多个cookie
    jar的一个使用场景是登录并存储与网站的多个会话。你可以登录多个用户，将每个会话存储在一个cookie jar中，并按需使用它们。此示例未使用自定义cookie
    jar。
- en: HTTP responses contain the body as a reader interface. We can extract the data
    from the reader using any function that accepts a reader interface. This includes
    functions such as the `io.Copy()`, `io.ReadAtLeast()`, `io.ReadlAll()`, and `bufio`
    buffered readers. In this example, `ioutil.ReadAll()` is used to quickly store
    the full contents of the HTTP response into a byte-slice variable.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP响应包含一个作为读取器接口的主体。我们可以使用任何接受读取器接口的函数从读取器中提取数据。这些函数包括`io.Copy()`、`io.ReadAtLeast()`、`io.ReadAll()`以及`bufio`缓冲读取器。在此示例中，使用`ioutil.ReadAll()`快速将HTTP响应的完整内容存储到字节切片变量中。
- en: 'The following is the code implementation of this example:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是此示例的代码实现：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using regular expressions to find email addresses in a page
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用正则表达式在页面中查找电子邮件地址
- en: A regular expression, or regex, is actually a form of language in its own right.
    Essentially, it is a special string that expresses a text search pattern. You
    may be familiar with the asterisk (`*`) when using a shell. Commands such as `ls
    *.txt` use a simple regular expression. The asterisk in this case represents *anything*;
    so any string would match as long as it ended with `.txt`. Regular expressions
    have other symbols besides the asterisk, like the period (`.`), which matches
    any single character as opposed to the asterisk, which will match a string of
    any length. There are even more powerful expressions that can be crafted with
    the handful of symbols that are available.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式（regex）实际上是一种语言形式。本质上，它是一种表示文本搜索模式的特殊字符串。你可能熟悉在使用 shell 时的星号（`*`）。例如，命令`ls
    *.txt`使用了一个简单的正则表达式。在这种情况下，星号代表*任何东西*；所以只要字符串以`.txt`结尾，都会匹配。正则表达式除了星号外，还有其他符号，比如句点（`.`），它代表匹配任何单个字符，而星号则匹配任何长度的字符串。利用这些符号，还可以构建更强大的表达式。
- en: Regular expressions have a reputation for being slow. The implementation used
    is guaranteed to run in linear time, as opposed to exponential time, based on
    the input length. This means it will run faster than many other implementations
    of regular expressions that do not provide that guarantee, such as Perl. Russ
    Cox, one of Go's authors, published a deep comparison of the two different approaches
    in 2007, which is available at [https://swtch.com/~rsc/regexp/regexp1.html](https://swtch.com/~rsc/regexp/regexp1.html).
    This is very important for our use case of searching the contents of an HTML page.
    If the regular expression ran in exponential time, based on the input length,
    it could take quite literally years to perform a search of certain expressions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式因其运行速度较慢而广为人知。所使用的实现方式保证了基于输入长度的线性时间运行，而非指数时间运行。这意味着它将比许多没有提供这种保证的正则表达式实现（如
    Perl）运行得更快。Go 的作者之一 Russ Cox 在 2007 年发表了一篇深入比较这两种方法的文章，文章链接为[https://swtch.com/~rsc/regexp/regexp1.html](https://swtch.com/~rsc/regexp/regexp1.html)。这对于我们在
    HTML 页面中搜索内容的应用场景至关重要。如果正则表达式以指数时间运行，基于输入长度，某些表达式的搜索可能字面上需要几年时间才能完成。
- en: Learn more about regular expressions in general from [https://en.wikipedia.org/wiki/Regular_expression](https://en.wikipedia.org/wiki/Regular_expression)
    and the relevant Go documentation at [https://golang.org/pkg/regexp/](https://golang.org/pkg/regexp/).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多关于正则表达式的一般信息，请访问[https://en.wikipedia.org/wiki/Regular_expression](https://en.wikipedia.org/wiki/Regular_expression)
    和相关的 Go 文档：[https://golang.org/pkg/regexp/](https://golang.org/pkg/regexp/)。
- en: This example uses a regular expression that searches for email address links
    embedded in HTML. It will search for any `mailto` links and extract the email
    address. We'll use the default HTTP client and call `http.Get()` instead of creating
    a custom client to modify the timeout.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例使用了一个正则表达式，用于搜索嵌入在 HTML 中的电子邮件地址链接。它会搜索任何`mailto`链接并提取电子邮件地址。我们将使用默认的 HTTP
    客户端，并调用`http.Get()`，而不是创建自定义客户端来修改超时设置。
- en: 'A typical email link looks like one of these:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的电子邮件链接看起来像这样：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The regular expression used is in this example is this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例中使用的正则表达式是：
- en: '`"mailto:.*?["?]`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`"mailto:.*?["?]`'
- en: 'Let''s break this down and examine each part:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一分析并检查每个部分：
- en: '`"mailto:`: This whole piece is just a string literal. The first character
    is a quotation mark (`"`) and has no special meaning in the regular expression.
    It is treated like as a regular character. This means that the regex will begin
    by searching for a quotation mark character first. After the quotation mark is
    the text `mailto` with a colon (`:`). The colon has no special meaning either.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"mailto:`：这一部分只是一个字符串字面量。第一个字符是一个引号（`" `），在正则表达式中没有特殊含义。它被当作普通字符对待。这意味着正则表达式将首先搜索一个引号字符。在引号后面是文本`mailto`和一个冒号（`:`），冒号也没有特殊含义。'
- en: '`.*?`: The period (`.`) means match any character except a newline. The asterisk
    means continue matching based on the previous symbol (the period) for zero or
    more characters. Directly after the asterisk, is a question mark (`?`). This question
    mark tells the asterisk to be non-greedy. It will match the shortest string possible.
    Without it, the asterisk will continue to match as long as possible, while still
    satisfying the full regular expression. We only want the email address itself
    and not any query parameters such as `?subject`, so we are telling it to do a
    non-greedy or short match.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.*?`：句点（`.`）表示匹配除换行符之外的任何字符。星号表示根据前一个符号（句点）继续匹配零个或多个字符。星号后紧跟着一个问号（`?`）。这个问号告诉星号进行非贪婪匹配，它会匹配最短的字符串。如果没有问号，星号会尽可能匹配更长的字符串，同时仍然满足整个正则表达式。我们只想要电子邮件地址本身，而不是任何查询参数，比如`?subject`，所以我们让它进行非贪婪或短匹配。'
- en: '`["?]`: The last piece of the regular expression is the `["?]` set. The brackets
    tell the regex to match any character encapsulated by the brackets. We only have
    two characters: the quotation mark and the question mark. The question mark here
    has no special meaning and is treated as a regular character. The two characters
    inside the brackets are the two possible characters that deliminate the end of
    the email address. By default, the regex would go with whichever one came last
    and return the longest string possible because the asterisk that preceded it would
    have been greedy. However, because we added the other question mark in the previous
    section directly after the asterisk, it will perform a non-greedy search and stop
    at the first thing that matches a character inside the brackets.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`["?]`：正则表达式的最后一部分是`["?]`集合。方括号告诉正则表达式匹配方括号内的任何字符。我们这里只有两个字符：引号和问号。这里的问号没有特殊意义，视为普通字符。方括号内的两个字符是限定电子邮件地址结尾的两个可能字符。默认情况下，正则表达式会选择最后出现的字符，并返回最长的字符串，因为之前的星号会使其贪婪匹配。然而，由于我们在前一部分的星号后面紧接着添加了另一个问号，它会执行非贪婪匹配，并在找到第一个匹配方括号内字符的地方停止。'
- en: Using this technique means that we will only find emails that are explicitly
    linked using an `<a>` tag in the HTML. It will not find emails that are just written
    as plaintext in the page. Creating a regular expression to search for an email
    string based on a pattern such as `<word>@<word>.<word>` may seem simple, but
    the nuances between different regular expression implementations and the complex
    variations that emails can have make it difficult to craft a regular expression
    that catches all valid email combinations. If you do a quick search online for
    an example, you will see how many variations there are and how complex they get.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种技术意味着我们只会找到通过HTML中的`<a>`标签显式链接的电子邮件地址。它不会找到页面中仅作为纯文本书写的电子邮件地址。创建一个基于模式如`<word>@<word>.<word>`的正则表达式来查找电子邮件字符串看似简单，但不同正则表达式实现之间的细微差别和电子邮件地址可能出现的复杂变化使得编写一个能捕获所有有效电子邮件组合的正则表达式变得困难。如果你在网上快速搜索一个例子，你会看到有多少种变化，以及它们是多么复杂。
- en: If you are creating some kind of web service it is important to verify a person's
    email account by sending them an email and having them respond or verify with
    a link in some way. I do not recommend that you ever rely solely on a regular
    expression to determine if an email is valid, and I also recommend that you be
    extremely careful about using regular expressions to perform client-side email
    validation. A user may have a weird email address that is technically valid and
    you may prevent them from signing up to your service.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在创建某种网络服务，验证用户的电子邮件账户非常重要，可以通过发送邮件让用户回复或通过某种方式验证链接来完成验证。我不建议你仅仅依靠正则表达式来判断电子邮件是否有效，我还建议你在使用正则表达式进行客户端电子邮件验证时要格外小心。用户可能有一个在技术上有效但很奇怪的电子邮件地址，而你可能会阻止他们注册你的服务。
- en: 'Here are some examples of email addresses that are actually valid according
    to *RFC 822* from 1982:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些根据1982年发布的*RFC 822*标准实际上有效的电子邮件地址示例：
- en: '`*.*@example.com`'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*.*@example.com`'
- en: '`$what^the.#!$%@example.com`'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$what^the.#!$%@example.com`'
- en: '`!#$%^&*=()@example.com`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`!#$%^&*=()@example.com`'
- en: '`"!@#$%{}^&~*()|/="@example.com`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"!@#$%{}^&~*()|/="@example.com`'
- en: '`"hello@example.com"@example.com`'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"hello@example.com"@example.com`'
- en: In 2001, *RFC 2822* replaced *RFC 822*. Out of all the preceding examples, only
    the last two containing an at (`@`) symbol are considered invalid by the newer
    *RFC 2822*. All of the other examples are still valid. Read the original RFCs
    at [https://www.ietf.org/rfc/rfc822.txt](https://www.ietf.org/rfc/rfc822.txt)
    and [https://www.ietf.org/rfc/rfc2822.txt](https://www.ietf.org/rfc/rfc2822.txt).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2001 年，*RFC 2822* 替代了 *RFC 822*。在所有前面的示例中，只有最后两个包含 `@` 符号的被新的 *RFC 2822* 视为无效。所有其他示例仍然有效。阅读原始
    RFC 文档：[https://www.ietf.org/rfc/rfc822.txt](https://www.ietf.org/rfc/rfc822.txt)
    和 [https://www.ietf.org/rfc/rfc2822.txt](https://www.ietf.org/rfc/rfc2822.txt)。
- en: 'The following is the code implementation of this example:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是该示例的代码实现：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Extracting HTTP headers from an HTTP response
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 HTTP 响应中提取 HTTP 头部
- en: 'HTTP headers contain metadata and descriptive information about the request
    and response. You can potentially learn a lot about a server by inspecting the
    HTTP headers it serves with a response. You can learn the following things about
    the server:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP 头部包含有关请求和响应的元数据和描述性信息。通过检查服务器返回的 HTTP 头部，您可以潜在地了解有关服务器的很多信息。您可以从头部中了解以下内容：
- en: Caching system
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓存系统
- en: Authentication
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 认证
- en: Operating system
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作系统
- en: Web server
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Web 服务器
- en: Response type
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应类型
- en: Framework or content management system
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 框架或内容管理系统
- en: Programming language
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编程语言
- en: Spoken language
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 口语语言
- en: Security headers
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全头部
- en: Cookies
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cookie
- en: Not every web server will return all of those headers, but it is helpful to
    learn as much as you can from the headers. Popular frameworks such as WordPress
    and Drupal will return an `X-Powered-By` header telling you whether it is WordPress
    or Drupal and what version.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 不是每个 web 服务器都会返回所有这些头部，但了解尽可能多的头部信息是有帮助的。像 WordPress 和 Drupal 这样的流行框架会返回一个 `X-Powered-By`
    头部，告诉您是 WordPress 还是 Drupal，并且会显示版本号。
- en: The session cookie can give away a lot of information too. A cookie named `PHPSESSID`
    tells you it is most likely a PHP application. Django's default session cookie
    is named `sessionid`, that of Java is `JSESSIONID`, and the session cookie of
    Ruby on Rail follows the `_APPNAME_session` pattern. You can use these clues to
    fingerprint web servers. If you only want the headers and don't need the whole
    body of a page, you can always use the HTTP `HEAD` method instead of HTTP `GET`.
    The `HEAD` method will return only headers.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 会话 Cookie 也能泄露很多信息。一个名为 `PHPSESSID` 的 Cookie 表明它很可能是一个 PHP 应用程序。Django 的默认会话
    Cookie 名为 `sessionid`，Java 的为 `JSESSIONID`，Ruby on Rail 的会话 Cookie 遵循 `_APPNAME_session`
    模式。您可以利用这些线索对 web 服务器进行指纹识别。如果您只需要头部信息，而不需要页面的整个正文，您可以使用 HTTP `HEAD` 方法代替 HTTP
    `GET`。`HEAD` 方法将只返回头部信息。
- en: 'This example makes a `HEAD` request to a URL and prints out all of its headers.
    The `http.Response` type contains a map of strings to strings named `Header`,
    which contain the key-value pair for each HTTP header:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例向一个 URL 发出一个 `HEAD` 请求，并打印出所有的头部。`http.Response` 类型包含一个名为 `Header` 的字符串到字符串的映射，里面包含每个
    HTTP 头部的键值对：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Setting cookies with an HTTP client
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 HTTP 客户端设置 Cookie
- en: Cookies are an essential component of modern web applications. Cookies are sent
    back and forth between the client and server as HTTP headers. Cookies are just
    text key-value pairs that are stored by the browser client. They are used to store
    persistent data on the client. They can be used to store any text value, but are
    commonly used to store preferences, tokens, and session information.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Cookie 是现代 web 应用程序的重要组成部分。Cookie 在客户端和服务器之间作为 HTTP 头部来回传递。Cookie 只是由浏览器客户端存储的文本键值对。它们用于在客户端存储持久化数据。Cookie
    可以存储任何文本值，但通常用于存储偏好设置、令牌和会话信息。
- en: Session cookies usually store a token that matches the token the server has.
    When a user logs in, the server creates a session with an identifying token tied
    to that user. The server then sends the token back to the user in the form of
    a cookie. When the client sends the session token in the form of a cookie, the
    server looks and finds a matching token in the session store, which may be a database,
    a file, or in memory. The session token requires sufficient entropy to ensure
    that it is unique and attackers cannot guess it.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 会话 Cookie 通常存储一个与服务器中的令牌匹配的令牌。当用户登录时，服务器会创建一个与该用户相关的标识令牌的会话。服务器随后以 Cookie 形式将令牌发送回用户。当客户端以
    Cookie 形式发送会话令牌时，服务器会查找并在会话存储中找到匹配的令牌，存储可能是数据库、文件或内存。会话令牌需要足够的熵，以确保它是唯一的，攻击者无法猜测。
- en: If a user is on a public Wi-Fi network and visits a website that does not use
    SSL, anyone nearby can see the HTTP requests in plaintext. An attacker could steal
    the session cookie and use it in their own requests. When a cookie is sidejacked
    in this fashion, the attacker can impersonate the victim. The server will treat
    them as the already logged in user. The attacker may never learn the password
    and does not need to.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户在公共 Wi-Fi 网络上，并访问一个没有使用 SSL 的网站，附近的任何人都可以看到明文的 HTTP 请求。攻击者可能会窃取会话 cookie，并在自己的请求中使用它。当
    cookie 以这种方式被侧面劫持时，攻击者可以冒充受害者。服务器会将其视为已经登录的用户。攻击者可能永远无法知道密码，而且也不需要知道。
- en: For this reason, it can be useful to log out of websites occasionally and destroy
    any active sessions. Some websites allow you to manually destroy all active sessions.
    If you run a web service, I recommend that you set a reasonable expiration time
    for sessions. Bank websites do a good job of this usually enforcing a short 10-15
    minute expiration.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，偶尔退出网站并销毁所有活动会话是有用的。有些网站允许你手动销毁所有活动会话。如果你运行一个网络服务，我建议你为会话设置合理的过期时间。银行网站通常会做得很好，强制执行短时间（10-15分钟）的过期时间。
- en: There is a `Set-Cookie` header that a server sends to the client when creating
    a new cookie. The client then sends the cookies back to the server using the `Cookie`
    header.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器在创建新 cookie 时，会发送一个`Set-Cookie`头部到客户端。客户端随后会使用`Cookie`头部将 cookie 发送回服务器。
- en: 'Here is a simple example of cookie headers sent from a server:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是服务器发送的一个简单的 cookie 头部示例：
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here is an example header from a client:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是来自客户的一个示例标题：
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: There are other attributes that a cookie can contain, such as the `Secure` and
    `HttpOnly` flags discussed in [Chapter 9](f15910a1-239e-49a5-b4d9-3881a524bfa9.xhtml),
    *Web Applications*. Other attributes include an expiration date, a domain, and
    a path. This example is only presenting the simplest application.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: cookie 还可以包含其他属性，例如在[第9章](f15910a1-239e-49a5-b4d9-3881a524bfa9.xhtml)《Web 应用程序》中讨论的`Secure`和`HttpOnly`标志。其他属性包括过期日期、域名和路径。这个示例仅展示了最简单的应用。
- en: In this example, a simple request is made with a custom session cookie. The
    session cookie is what allows you to be *logged in* when making a request to a
    website. This example should serve as a reference for how to make a request with
    a cookie and not a standalone tool. First, the URL is defined just before the
    `main` function. Then, the HTTP request is created first with the HTTP `GET` method
    specified. A nil body is provided since `GET` requests generally don't require
    a body. The new request is then updated with a new header, the cookie. In this
    example, `session_id` is the name of the session cookie, but that will vary depending
    on the web application being interacted with.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，发起了一个带有自定义会话 cookie 的简单请求。会话 cookie 使你在访问网站时能够保持*登录状态*。这个示例应该作为如何使用 cookie
    发起请求的参考，而不是一个独立的工具。首先，在`main`函数之前定义 URL。然后，创建 HTTP 请求，首先指定 HTTP `GET` 方法。由于`GET`请求通常不需要正文，所以提供一个空正文。接着，更新新请求，添加新的头部信息——cookie。在这个示例中，`session_id`
    是会话 cookie 的名称，但这会根据所交互的 web 应用而有所不同。
- en: Once the request is prepared, an HTTP client is created to actually make the
    request and process the response. Note that the HTTP request and the HTTP client
    are separate and independent entities. For example, you can reuse a request multiple
    times, use a request with different clients, and use multiple requests with a
    single client. This allows you to create multiple request objects with different
    session cookies if you need to manage multiple client sessions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦请求准备好，就创建一个 HTTP 客户端来实际发起请求并处理响应。请注意，HTTP 请求和 HTTP 客户端是独立的实体。例如，你可以多次重用一个请求，使用不同的客户端发送请求，或者使用一个客户端发起多个请求。这使得你可以在需要管理多个客户端会话时创建多个带有不同会话
    cookie 的请求对象。
- en: 'The following is the code implementation of this example:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是该示例的代码实现：
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Finding HTML comments in a web page
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在网页中查找 HTML 注释
- en: HTML comments can sometimes hold amazing pieces of information. I have personally
    seen websites with the admin username and password in HTML comments. I have also
    seen an entire menu commented out, but the links still worked and could be reached
    directly. You never know what kind of information a careless developer might leave
    behind.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: HTML 注释有时包含一些惊人的信息。我曾亲眼见过一些网站在 HTML 注释中暴露了管理员的用户名和密码。我也曾见过整个菜单被注释掉，但链接仍然有效并且可以直接访问。你永远不知道一个粗心的开发者可能会留下什么信息。
- en: If you are going to leave comments in your code, it is always ideal to leave
    them in the server-side code and not in the client-facing HTML and JavaScript.
    Comment in the PHP, Ruby, Python, or whatever backend code you have. You never
    want to give the client more information than they need in the code.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打算在代码中留下注释，最好将它们留在服务器端代码中，而不是客户端的HTML和JavaScript中。可以在PHP、Ruby、Python或任何后端代码中进行注释。你永远不希望在代码中向客户端提供超过他们需要的信息。
- en: 'The regular expression used in this program consists of a few special sequences.
    Here is the full regular expression. It essentially says, "match anything between
    the `<!--` and `-->` strings." Let''s examine it piece by piece:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序中使用的正则表达式包含一些特殊的序列。这里是完整的正则表达式。它本质上表示“匹配`<!--`和`-->`字符串之间的任何内容”。让我们逐部分分析：
- en: '`<!--(.|\n)*?-->`: The beginning and the end start with `<!--` and `-->`, which
    are the designations for opening and closing an HTML comment. Those are plain
    characters and not special characters to the regular expression.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<!--(.|\n)*?-->`：开始和结束都是`<!--`和`-->`，这分别是HTML注释的开头和结尾标记。它们是普通字符，不是正则表达式中的特殊字符。'
- en: '`(.|\n)*?`: This can be broken down into two pieces:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(.|\n)*?`：这可以分解成两部分：'
- en: '`(.|\n)`: The first part has a few special characters. The parentheses, `()`,
    enclose a set of options. The pipe, `|`, separates the options. The options themselves
    are the dot, `.`, and the newline character, `\n`. The dot means match any character,
    except a newline. Because an HTML comment can span multiple lines, we want to
    match any character, including a newline character. The whole piece, `(.|\n)`
    means match the dot or a newline character.'
  id: totrans-95
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(.|\n)`：第一部分有一些特殊字符。括号`()`包裹了一组选项。管道符号`|`分隔这些选项。选项本身是点号`.`和换行符`\n`。点号表示匹配任何字符，除了换行符。由于HTML注释可能跨越多行，我们希望匹配包括换行符在内的任何字符。整个部分`(.|\n)`表示匹配点号或换行符。'
- en: '`*?`: The asterisk means continue matching the previous character or expression
    zero or more times. Immediately preceding the asterisk is the set of parentheses,
    so it will continue trying to match `(.|\n)`. The question mark tells the asterisk
    to be non-greedy, or return the smallest match possible. Without the question
    mark, to designate it as non-greedy; it will match the largest thing possible,
    which means it will start at the beginning of the first comment in the page, and
    end at the ending of the very last comment in the page, including everything in
    between.'
  id: totrans-96
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*?`：星号表示继续匹配前一个字符或表达式零次或多次。紧接星号的是一组括号，因此它会继续尝试匹配`(.|\n)`。问号告诉星号以非贪婪模式工作，即返回最小的匹配项。如果没有问号来指定非贪婪模式，它会匹配尽可能大的内容，这意味着它会从页面上第一个注释的开始处开始匹配，直到最后一个注释的结束位置，涵盖其中的所有内容。'
- en: Try running this program against some websites and see what kind of HTML comments
    you find. You might be surprised at what kind of information you can uncover.
    For example, the MailChimp signup forms come with an HTML comment that actually
    gives you tips on bypassing the bot signup prevention. The MailChimp signup form
    uses a honeypot field that should not be filled out or it assumes the form was
    submitted by a bot. See what you can find.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试在一些网站上运行这个程序，看看你能找到什么样的HTML注释。你可能会对能揭示出的信息感到惊讶。例如，MailChimp的注册表单包含一个HTML注释，实际上给你提供了绕过机器人防止注册的技巧。MailChimp的注册表单使用了一个蜜罐字段，如果这个字段被填充，系统就会认为是机器人提交的表单。看看你能找到什么。
- en: 'This example will first fetch the URL provided, then use the regular expression
    we walked through earlier to search for HTML comments. Every match found is then
    printed out to standard output:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子首先会获取提供的URL，然后使用我们之前讲解过的正则表达式来搜索HTML注释。每找到一个匹配项，就会打印到标准输出：
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Finding unlisted files on a web server
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Web服务器上查找未列出的文件
- en: There is a popular program called DirBuster, which penetration testers use for
    finding unlisted files. DirBuster is an OWASP project that comes preinstalled
    on Kali, the popular penetration testing Linux distribution. With nothing but
    the standard library, we can create a fast, concurrent, and simple clone of DirBuster
    with just a few lines. More information about DirBuster is available at [https://www.owasp.org/index.php/Category:OWASP_DirBuster_Project](https://www.owasp.org/index.php/Category:OWASP_DirBuster_Project).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个流行的程序叫做DirBuster，渗透测试人员用它来查找未列出的文件。DirBuster是一个OWASP项目，预装在Kali上，Kali是流行的渗透测试Linux发行版。仅凭标准库，我们就能用几行代码快速、并发且简单地克隆DirBuster。有关DirBuster的更多信息，请访问[https://www.owasp.org/index.php/Category:OWASP_DirBuster_Project](https://www.owasp.org/index.php/Category:OWASP_DirBuster_Project)。
- en: This program is a simple clone of DirBuster that searches for unlisted files
    based on a word list. You will have to create your own word list. A small list
    of example filenames will be provided here to give you some ideas and to use as
    a starting list. Build your list of files based on your own experience and based
    on the source code. Some web applications have files with specific names that
    will allow you to fingerprint which framework is being used. Also look for backup
    files, configuration files, version control files, changelog files, private keys,
    application logs, and anything else that is not intended to be public. You can
    also find prebuilt word lists on the internet, including DirBuster's lists.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序是一个简单的DirBuster克隆，基于单词列表搜索未列出的文件。你需要自己创建单词列表。这里会提供一个小的示例文件名列表，以便给你一些想法并作为起始列表。根据你的经验和源代码来构建文件列表。某些Web应用程序具有特定名称的文件，能让你指纹识别使用的框架。此外，还要查找备份文件、配置文件、版本控制文件、更新日志文件、私钥、应用程序日志以及任何不应公开的文件。你也可以在互联网上找到现成的单词列表，包括DirBuster的列表。
- en: 'Here is a sample list of files that you could search for:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个你可以搜索的文件示例列表：
- en: '`.gitignore`'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.gitignore`'
- en: '`.git/HEAD`'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.git/HEAD`'
- en: '`id_rsa`'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id_rsa`'
- en: '`debug.log`'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`debug.log`'
- en: '`database.sql`'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`database.sql`'
- en: '`index-old.html`'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index-old.html`'
- en: '`backup.zip`'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backup.zip`'
- en: '`config.ini`'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config.ini`'
- en: '`settings.ini`'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`settings.ini`'
- en: '`settings.php.bak`'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`settings.php.bak`'
- en: '`CHANGELOG.txt`'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CHANGELOG.txt`'
- en: This program will search a domain with the provided word list and report any
    files that do not return a 404 NOT FOUND response. The word list should have filenames
    separated with a newline and have one filename per line. When providing the domain
    name as a parameter, the trailing slash is optional, and the program will behave
    properly with or without the trailing slash on the domain name. The protocol must
    be specified though, so that the request knows whether to use HTTP or HTTPS.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序将使用提供的单词列表搜索域名，并报告任何没有返回404 NOT FOUND响应的文件。单词列表应该以换行符分隔文件名，每行一个文件名。提供域名作为参数时，尾部的斜杠是可选的，程序无论有无尾部斜杠都会正确运行。然而，协议必须被指定，这样请求才能知道是使用HTTP还是HTTPS。
- en: The `url.Parse()` function is used to create a proper URL object. With the URL
    type, you can independently modify `Path` without modifying `Host` or `Scheme`.
    This provides an easy way to update the URL without resorting to manual string
    manipulation.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`url.Parse()`函数用于创建一个正确的URL对象。使用URL类型，你可以独立地修改`Path`，而无需修改`Host`或`Scheme`。这提供了一种便捷的方式来更新URL，而无需手动处理字符串。'
- en: 'To read the file line by line, a scanner is used. By default, scanners split
    by newlines, but they can be overridden by calling `scanner.Split()` and providing
    a custom split function. We use the default behavior since the words are expected
    to be provided on separate lines:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要逐行读取文件，使用了扫描器。默认情况下，扫描器按换行符拆分，但可以通过调用`scanner.Split()`并提供自定义拆分函数来覆盖此行为。由于预计单词将单独一行提供，因此我们使用默认行为：
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Changing the user agent of a request
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更改请求的用户代理
- en: A common technique to block scrapers and crawlers is to block certain user agents.
    Some services will blacklist certain user agents that contain keywords such as `curl`
    and `python`. You can get around most of these by simply changing your user agent
    to `firefox`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的阻止爬虫和抓取器的技术是阻止特定的用户代理。一些服务会将包含如`curl`和`python`等关键词的用户代理列入黑名单。你可以通过简单地将用户代理更改为`firefox`来绕过大多数这些限制。
- en: To set the user agent, you must first create the HTTP request object. The header
    must be set before making the actual request. This means that you can't use the
    shortcut convenience functions such as `http.Get()`. We have to create the client
    and then create a request, and then use the client to `client.Do()` the request.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置用户代理，必须首先创建HTTP请求对象。该头部必须在发出实际请求之前设置。这意味着你不能使用像`http.Get()`这样的快捷便利函数。我们必须创建客户端，然后创建请求，再使用客户端通过`client.Do()`发出请求。
- en: This example creates an HTTP request with `http.NewRequest()`, and then modifies
    the request headers to override the `User-Agent` header. You can use this to hide,
    fake, or be honest. To be a good web citizen, I recommend that you create a unique
    user agent for your crawler so that webmasters can throttle or block your bot.
    I also recommend that you include a website or email address in the user agent
    so that webmasters can request to be skipped by your scraper.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子通过`http.NewRequest()`创建一个HTTP请求，然后修改请求头来覆盖`User-Agent`头。你可以用它来隐藏、伪造或保持真实。为了成为一个合格的网络公民，我建议你为爬虫创建一个唯一的用户代理，这样网站管理员就可以限制或屏蔽你的爬虫。我还建议你在用户代理中包含一个网站或电子邮件地址，这样网站管理员可以要求跳过你的抓取工具。
- en: 'The following is the code implementation of this example:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是此示例的代码实现：
- en: '[PRE9]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Fingerprinting web application technology stacks
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网页应用程序技术栈的指纹识别
- en: Fingerprinting a web application is when you try to identify the technology
    that is being used to serve a web application. Fingerprinting can be done at several
    levels. At a lower level, HTTP headers can give clues to what operating system,
    such as Windows or Linux, and what web server, such as Apache or nginx, is running.
    The headers may also give information about the programming language or framework
    being used at the application level. At a higher level, the web application can
    be fingerprinted to identify which JavaScript libraries are being used, whether
    any analytics platforms are being included, any ad networks are being displayed,
    the caching layers in use, and other information. We will first look at the HTTP
    headers, and then cover more complex methods of fingerprinting.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 指纹识别网页应用程序是指你试图识别用于提供网页应用程序的技术。指纹识别可以在多个层面进行。在较低层面，HTTP头部可以提供关于操作系统（如Windows或Linux）和运行的网页服务器（如Apache或nginx）的线索。头部还可能提供有关应用层使用的编程语言或框架的信息。在更高层面，网页应用程序可以通过指纹识别来识别使用的JavaScript库、是否包含任何分析平台、是否显示任何广告网络、使用的缓存层以及其他信息。我们将首先查看HTTP头部，然后介绍更复杂的指纹识别方法。
- en: Fingerprinting is a critical step in an attack or penetration test because it
    helps narrow down options and determine which paths to take. Identifying what
    technologies are being used also lets you search for known vulnerabilities. If
    a web application is not kept up to date, a simple fingerprinting and vulnerability
    search may be all that is needed for finding and exploiting an already-known vulnerability.
    If nothing else, it helps you learn about the target.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 指纹识别是攻击或渗透测试中的关键步骤，因为它有助于缩小选择范围并确定要采取的路径。识别使用的技术还可以帮助你查找已知的漏洞。如果一个网页应用程序没有及时更新，简单的指纹识别和漏洞搜索可能就是找到并利用已知漏洞所需要的一切。如果没有别的，它至少能帮助你了解目标。
- en: Fingerprinting based on HTTP response headers
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于HTTP响应头的指纹识别
- en: I recommend that you inspect the HTTP headers first since they are simple key-value
    pairs, and generally, there are only a few returned with each request. It doesn't
    take very long to go through the headers manually, so you can inspect them first
    before moving on to the application. Fingerprinting at the application level is
    more complicated and we'll talk about that in a moment. Earlier in this chapter,
    there was a section about extracting HTTP headers and printing them out for inspection
    (the *Extracting HTTP headers from an HTTP response* section). You can use that
    program to dump the headers of different web pages and see what you can find.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议你首先检查HTTP头部，因为它们是简单的键值对，而且每次请求返回的通常只有几个头部。手动浏览这些头部不会花费太长时间，因此可以在继续检查应用程序之前先查看它们。应用层的指纹识别更为复杂，我们稍后会讲到这一点。在本章的早些部分，有一节关于提取HTTP头并打印出来以供检查的内容（*从HTTP响应中提取HTTP头部*）。你可以使用该程序转储不同网页的头部，并查看你能发现什么。
- en: The basic idea is simple. Look for keywords. Some headers in particular contain
    the most obvious clues, such as the `X-Powered-By`, `Server`, and `X-Generator`
    headers. The `X-Powered-By` header can contain the name of the framework or **Content
    Management System** (**CMS**) being used, such as WordPress or Drupal.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的思路很简单。寻找关键词。特别是某些头信息包含最明显的线索，例如`X-Powered-By`、`Server`和`X-Generator`头。`X-Powered-By`头可以包含正在使用的框架或**内容管理系统**（**CMS**）的名称，例如WordPress或Drupal。
- en: There are two basic steps to examining the headers. First, you need to get the
    headers. Use the example provided earlier in this chapter for extracting HTTP
    headers. The second step is to do a string search to look for the keywords. You
    can use `strings.ToUpper()` and `strings.Contains()` to search directly for keywords,
    or use regular expressions. Refer to the earlier examples in this chapter that
    explain how to use regular expressions. Once you are able to search through the
    headers, you just need to be able to generate the list of keywords to search for.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 检查头信息的基本步骤有两个。首先，你需要获取头信息。使用本章前面提供的示例来提取HTTP头。第二步是进行字符串搜索，寻找关键词。你可以使用`strings.ToUpper()`和`strings.Contains()`直接搜索关键词，或者使用正则表达式。参阅本章前面解释如何使用正则表达式的示例。一旦你能够在头信息中搜索，你只需要能够生成关键词列表来进行搜索。
- en: 'There are many keywords you can look for. What you search for will depend on
    what you are looking for. I''ll try to cover several broad categories to give
    you ideas on what to look for. The first thing you can try to identify is the
    operating system that the host is running. Here is a sample list of keywords that
    you can find in HTTP headers to indicate the operating system:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以寻找许多关键词。你搜索什么取决于你想寻找什么。为了给你提供一些思路，我会尝试涵盖几个广泛的类别。你首先可以尝试识别主机运行的是哪种操作系统。下面是一个示例列表，列出了你可以在HTTP头中找到的，用来指示操作系统的关键词：
- en: '`Linux`'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Linux`'
- en: '`Debian`'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Debian`'
- en: '`Fedora`'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Fedora`'
- en: '`Red Hat`'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Red Hat`'
- en: '`CentOS`'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CentOS`'
- en: '`Ubuntu`'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Ubuntu`'
- en: '`FreeBSD`'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FreeBSD`'
- en: '`Win32`'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Win32`'
- en: '`Win64`'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Win64`'
- en: '`Darwin`'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Darwin`'
- en: 'Here is a list of keywords that will help you determine which web server is
    being used. This is by no means an exhaustive list, but does cover several keywords
    that will yield results if you search the internet:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些可以帮助你确定使用的是哪种Web服务器的关键词列表。这绝不是一个详尽无遗的列表，但涵盖了几个关键词，如果你在互联网上搜索，它们会产生结果：
- en: '`Apache`'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Apache`'
- en: '`Nginx`'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Nginx`'
- en: '`Microsoft-IIS`'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Microsoft-IIS`'
- en: '`Tomcat`'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Tomcat`'
- en: '`WEBrick`'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WEBrick`'
- en: '`Lighttpd`'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Lighttpd`'
- en: '`IBM HTTP Server`'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IBM HTTP Server`'
- en: 'Determining which programming language is being used can make a big difference
    in your attack choices. Scripted languages such as PHP are vulnerable to different
    things than a Java server or an ASP.NET application. Here are a few sample keywords
    you can use to search in HTTP headers to identify which language is powering an
    application:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 确定正在使用的编程语言可以在你的攻击选择中产生重大影响。像PHP这样的脚本语言与Java服务器或ASP.NET应用程序在脆弱性上有所不同。以下是你可以在HTTP头中使用的一些关键词，帮助你识别应用程序使用的是哪种语言：
- en: '`Python`'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Python`'
- en: '`Ruby`'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Ruby`'
- en: '`Perl`'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Perl`'
- en: '`PHP`'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PHP`'
- en: '`ASP.NET`'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ASP.NET`'
- en: 'Session cookies are also big giveaways as to what framework or language is
    being used. For example, `PHPSESSID` indicates PHP and `JSESSIONID` indicates
    Java. Here are a few session cookies you can search for:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 会话Cookie也是确定正在使用的框架或语言的重要线索。例如，`PHPSESSID`表示PHP，`JSESSIONID`表示Java。以下是你可以搜索的几个会话Cookie：
- en: '`PHPSESSID`'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PHPSESSID`'
- en: '`JSESSIONID`'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`JSESSIONID`'
- en: '`session`'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`session`'
- en: '`sessionid`'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sessionid`'
- en: '`CFID/CFTOKEN`'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CFID/CFTOKEN`'
- en: '`ASP.NET_SessionId`'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ASP.NET_SessionId`'
- en: Fingerprinting web applications
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指纹识别Web应用程序
- en: Fingerprinting web applications in general covers a much broader scope than
    just looking at the HTTP headers. You can do basic keyword searches in the HTTP
    headers, as just discussed, and learn a lot, but there is also a wealth of information
    in the HTML source code and the contents, or simply the existence, of other files
    on the server.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，指纹识别Web应用程序的范围比仅查看HTTP头要广泛得多。你可以在HTTP头中进行基本的关键词搜索，正如之前所讨论的，学到很多信息，但在HTML源代码和内容中，以及服务器上其他文件的存在或内容中，也有丰富的信息。
- en: In the HTML source code, you can look for clues such as the structure of pages
    themselves and the names of classes and IDs of HTML elements. AngularJS applications
    have distinct HTML attributes, such as `ng-app`, that can be used as keywords
    for fingerprinting. Angular is also generally included with a `script` tag, the
    same way other frameworks such as jQuery are included. The `script` tags can also
    be inspected for other clues. Look for things such as Google Analytics, AdSense,
    Yahoo ads, Facebook, Disqus, Twitter, and other third-party JavaScript embedded.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在HTML源代码中，你可以查找一些线索，如页面结构本身以及HTML元素的类名和ID。AngularJS应用程序有独特的HTML属性，例如`ng-app`，可以作为指纹识别的关键词。Angular通常也包含在一个`script`标签中，就像其他框架如jQuery一样。`script`标签还可以检查其他线索。查找诸如Google
    Analytics、AdSense、Yahoo广告、Facebook、Disqus、Twitter和其他第三方嵌入的JavaScript等信息。
- en: Simply looking at the file extensions in URLs can tell you what language is
    being used. For example, `.php`, `.jsp`, and `.asp` indicate that PHP, Java, and
    ASP are being used, respectively.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅通过查看URL中的文件扩展名，你就能知道使用了什么语言。例如，`.php`、`.jsp`和`.asp`分别表示使用了PHP、Java和ASP。
- en: We also looked at a program that finds HTML comments in a web page. Some frameworks
    and CMSes leave an identifiable footer or hidden HTML comment. Sometimes the marker
    is in the form of a small image.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还查看了一个可以在网页中找到HTML注释的程序。一些框架和内容管理系统（CMS）会留下可识别的页脚或隐藏的HTML注释。有时，标记可能以小图像的形式存在。
- en: Directory structure can also be another giveaway. It requires familiarity with
    different frameworks first. For example, Drupal stores site information in a directory
    called `/sites/default`. If you attempt to visit that URL and you get a 403 FORBIDDEN
    response and not a 404 NOT FOUND error, you likely found a Drupal-based website.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 目录结构也可能是另一个线索。这需要首先熟悉不同的框架。例如，Drupal将站点信息存储在名为`/sites/default`的目录中。如果你尝试访问该URL并且得到的是403
    FORBIDDEN响应，而不是404 NOT FOUND错误，那么你很可能发现了一个基于Drupal的网站。
- en: Look for files such as `wp-cron.php`. In the *Finding unlisted files on a web
    server* section, we looked at finding unlisted files with the DirBuster clone.
    Find a list of unique files that you can use to fingerprint web applications and
    add them to your word list. You can figure out which files to look for by inspecting
    the code bases for different web frameworks. For example, the source code for
    WordPress and Drupal are publicly available. Use the program discussed earlier
    in this chapter for finding unlisted files to search for files. Other unlisted
    files that you can search for are related to documentation, such as `CHANGELOG.txt`,
    `readme.txt`, `readme.md`, `readme.html`, `LICENSE.txt`, `install.txt`, or `install.php`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 查找像`wp-cron.php`这样的文件。在*寻找未列出的文件*部分，我们讨论了使用DirBuster克隆工具查找未列出的文件。找到一份可以用于指纹识别Web应用程序的唯一文件列表，并将它们添加到你的词表中。你可以通过检查不同Web框架的代码库来确定要查找哪些文件。例如，WordPress和Drupal的源代码是公开可用的。使用本章早些时候讨论的程序来查找未列出的文件，进行文件搜索。你还可以查找与文档相关的其他未列出的文件，如`CHANGELOG.txt`、`readme.txt`、`readme.md`、`readme.html`、`LICENSE.txt`、`install.txt`或`install.php`。
- en: It is possible to get even more detail out of a web application by fingerprinting
    the version of an application that is running. It is much easier if you have access
    to the source code. I will use WordPress as an example since is it so ubiquitous
    and the source is available on GitHub at [https://github.com/WordPress/WordPress](https://github.com/WordPress/WordPress).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指纹识别正在运行的应用程序的版本，你可以获得更多的Web应用程序详细信息。如果你能够访问源代码，这将更容易。我将使用WordPress作为示例，因为它非常普及，而且其源代码可以在GitHub上找到，[https://github.com/WordPress/WordPress](https://github.com/WordPress/WordPress)。
- en: The goal is to find the differences between versions. WordPress is a good example
    because they all come with the `/wp-admin/` directory that contains all the administrative
    interfaces. Inside `/wp-admin/`, there are the `css` and `js` folders with style
    sheets and scripts in them, respectively. These files are publicly accessible
    when a site is hosted on a server. Use the `diff` command on these folders to
    identify which versions introduce new files, which versions remove files, and
    which versions modify existing files. With all that information combined, you
    can generally narrow down applications to a specific version or to at least a
    small range of versions.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是找出不同版本之间的差异。WordPress 是一个很好的例子，因为它们都带有 `/wp-admin/` 目录，里面包含所有的管理接口。在 `/wp-admin/`
    目录下，有 `css` 和 `js` 文件夹，分别存放样式表和脚本文件。当网站托管在服务器上时，这些文件是公开可访问的。你可以使用 `diff` 命令对这些文件夹进行比较，找出哪些版本新增了文件，哪些版本删除了文件，哪些版本修改了现有文件。通过将所有信息综合起来，你通常可以将应用程序缩小到某个特定版本，或者至少是一个小范围的版本。
- en: 'As a contrived example, let''s say version 1.0 contains only one file: `main.js`.
    Version 1.1 introduces a second file: `utility.js`. Version 1.3 removes both of
    those and replaces them with a single file: `master.js`. You can make HTTP requests
    to the web server for all three files: `main.js`, `utility.js`, and `master.js`.
    Based on which files are found with a 200 OK error and which files return a 404
    NOT FOUND error, you can determine which version is running.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 举一个简单的例子，假设版本 1.0 只包含一个文件：`main.js`。版本 1.1 引入了第二个文件：`utility.js`。版本 1.3 删除了这两个文件，并用一个文件：`master.js`
    替代了它们。你可以向 Web 服务器发起 HTTP 请求，获取这三个文件：`main.js`、`utility.js` 和 `master.js`。根据返回
    200 OK 状态的文件和返回 404 NOT FOUND 状态的文件，你可以确定当前运行的版本。
- en: If the same files are present across multiple versions, you can inspect deeper
    into the contents of the files. Either do a byte-by-byte comparison or hash the
    files and compare the checksums. Hashing and examples of hashing are covered in [Chapter
    6](f68073f0-8cc8-40b5-af0e-795ce30e5271.xhtml), *Cryptography*.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果相同的文件出现在多个版本中，你可以更深入地检查这些文件的内容。可以逐字节比较，或者对文件进行哈希并比较校验和。哈希和哈希示例在[第六章](f68073f0-8cc8-40b5-af0e-795ce30e5271.xhtml)，《加密学》中有详细讲解。
- en: Sometimes, identifying the version can be much simpler than that whole process
    just described. Sometimes there is a `CHANGELOG.txt` or `readme.html` file that
    will tell you exactly which version is running without having to do any work.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，识别版本比刚才描述的整个过程要简单得多。有时会有一个 `CHANGELOG.txt` 或 `readme.html` 文件，它可以直接告诉你当前运行的是哪个版本，而不需要进行任何工作。
- en: How to prevent fingerprinting of your applications
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何防止应用程序被指纹识别
- en: As demonstrated earlier, there are multiple ways to fingerprint applications
    at many different levels of the technology stack. The first question you should
    really ask yourself is, "Do I need to prevent fingerprinting?" In general, trying
    to prevent fingerprinting is a form of obfuscation. Obfuscation is a bit controversial,
    but I think everyone does agree that obfuscation is not security in the same way
    that encoding is not encryption. It may slow down, limit information, or confuse
    an attacker temporarily, but it does not truly prevent any vulnerability from
    being exploited. Now, I'm not saying that there is no benefit at all from obfuscation,
    but it can never be relied on by itself. Obfuscation is simply a thin layer of
    concealment.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，指纹识别应用程序有多种方法，可以在技术栈的不同层级进行。你真正应该问自己的第一个问题是，“我需要防止指纹识别吗？”一般来说，试图防止指纹识别是一种混淆技术。混淆技术有些争议，但我认为大家都同意混淆并不是安全，就像编码并不是加密一样。它可能会暂时减缓攻击者、限制信息或造成困惑，但并不能真正阻止任何漏洞的利用。现在，我不是说混淆完全没有好处，但它永远不能单独依赖。混淆只是一个薄弱的掩盖层。
- en: Obviously, you don't want to give away too much information about your application,
    such as debug output or configuration settings, but some information is going
    to be available no matter what when a service is available on the network. You
    will have to make a choice about how much time and effort you want to put into
    hiding information.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，你不希望泄露关于应用程序的过多信息，比如调试输出或配置设置，但当服务在网络上可用时，无论如何总会有一些信息是可用的。你需要决定投入多少时间和精力来隐藏这些信息。
- en: Some people go as far as outputting false information to mislead attackers.
    Personally, putting out fake headers is not on my checklist of things to do when
    hardening a server. One thing I recommend that you do is to remove any extra files
    as mentioned earlier. Files such as changelog files, default setting files, installation
    files, and documentation files should all be removed before deployment. Don't
    publicly serve the files that are not required for the application to work.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人甚至通过输出虚假信息来误导攻击者。就个人而言，在加强服务器安全时，我不会把发布虚假头信息列入我的清单中。我建议你做的一件事是删除任何多余的文件，如前面所提到的。像更改日志文件、默认设置文件、安装文件和文档文件这样的文件，在部署之前应该全部删除。不要公开提供那些应用程序运行所不需要的文件。
- en: Obfuscation is a topic that warrants its own chapter or even its own book. There
    are obfuscation competitions dedicated to awarding the most creative and bizarre
    forms of obfuscation. There are some tools that help you obfuscate JavaScript
    code, but on the flip side, there are also deobfuscation tools.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆是一个值得单独成章，甚至是写书的主题。甚至有专门的混淆比赛，旨在奖励最具创意和奇特的混淆方式。有些工具可以帮助你混淆JavaScript代码，但另一方面，也有解混淆工具。
- en: Using the goquery package for web scraping
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用goquery包进行网页抓取
- en: The `goquery` package is not part of the standard library, but is available
    on GitHub. It is intended to work similar to jQuery—a popular JavaScript framework
    for interacting with the HTML DOM. As demonstrated in the previous sections, trying
    to search with string matching and regular expressions is both tedious and complicated.
    The `goquery` package makes it much easier to work with HTML content and search
    for specific elements. The reason I suggest this package is because it is modelled
    after the very popular jQuery framework that many people are already familiar
    with.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`goquery`包不是标准库的一部分，但可以在GitHub上找到。它的设计类似于jQuery——一个流行的JavaScript框架，用于与HTML
    DOM进行交互。正如前面章节所示，使用字符串匹配和正则表达式进行搜索既繁琐又复杂。`goquery`包使得处理HTML内容和查找特定元素变得更加容易。我建议使用这个包，因为它是基于非常流行的jQuery框架的，许多人已经对此非常熟悉。'
- en: 'You can get the `goquery` package with the `go get` command:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`go get`命令获取`goquery`包：
- en: '[PRE10]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The documentation is available at [https://godoc.org/github.com/PuerkitoBio/goquery](https://godoc.org/github.com/PuerkitoBio/goquery).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 文档可以在[https://godoc.org/github.com/PuerkitoBio/goquery](https://godoc.org/github.com/PuerkitoBio/goquery)上找到。
- en: Listing all hyperlinks in a page
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 列出页面中的所有超链接
- en: 'For the introduction to the `goquery` package, we''ll look at a common and
    simple task. We will find all hyperlinks in a page and print them out. A typical
    link looks something like this:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`goquery`包的介绍，我们将探讨一个常见且简单的任务。我们将查找页面中的所有超链接并将其打印出来。一个典型的链接看起来像这样：
- en: '[PRE11]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In HTML, the `a` tag stands for **anchor** and the `href` attribute stands
    for **hyperlink reference**. It is possible to have an anchor tag with no `href`
    attribute but only a `name` attribute. These are called bookmarks, or named anchors,
    and are used to jump to a location on the same page. We will ignore these since
    they only link within the same page. The `target` attribute is just an optional
    one specifying which window or tab to open the link in. We are only interested
    in the `href` value for this example:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在HTML中，`a`标签代表**锚点**，`href`属性代表**超链接引用**。可以存在没有`href`属性但只有`name`属性的锚点标签，这些被称为书签或命名锚点，用于跳转到同一页面中的某个位置。我们将忽略这些，因为它们只在同一页面内链接。`target`属性只是一个可选的属性，用于指定在哪个窗口或标签页中打开链接。对于这个示例，我们只关心`href`值：
- en: '[PRE12]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Finding documents in a web page
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在网页中查找文档
- en: Documents are also points of interest. You might want to scrape a web page and
    look for documents. Word processor documents, spreadsheets, slideshow decks, CSV,
    text, and other files can contain useful information for a variety of purposes.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 文档也是感兴趣的内容。你可能会想抓取一个网页并查找文档。文字处理文档、电子表格、幻灯片、CSV、文本文件和其他文件可能包含有用的信息，用于各种目的。
- en: The following example will search through a URL and search for documents based
    on the file extensions in the links. A global variable is defined at the top for
    convenience with the list of all extensions that should be searched for. Customize
    the list of extensions to search for your target file types. Consider extending
    the application to take a list of file extensions in from a file instead of being
    hardcoded. What other file extensions would you look for when trying to find sensitive
    information?
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例将搜索一个 URL，并根据链接中的文件扩展名搜索文档。为了方便起见，在顶部定义了一个全局变量，列出了所有应该搜索的扩展名。自定义要搜索的扩展名列表，以便查找目标文件类型。考虑将该应用程序扩展为从文件中读取文件扩展名列表，而不是硬编码。你在寻找敏感信息时，还会查找哪些其他文件扩展名？
- en: 'The following is the code implementation of this example:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是此示例的代码实现：
- en: '[PRE13]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Listing page title and headings
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 列出页面标题和标题
- en: Headings are the primary structural elements that define the hierarchy of a
    web page, with `<h1>` being the top level and `<h6>` being the lowest or deepest
    level of the hierarchy. The title, defined in the `<title>` tag, of an HTML page
    is what gets displayed in the browser title bar, and it is not part of the rendered
    page.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 标题是定义网页层次结构的主要结构元素，其中 `<h1>` 是最高层级，`<h6>` 是最低或最深的层级。HTML 页面的 `<title>` 标签定义了页面标题，显示在浏览器的标题栏中，它不属于渲染页面的一部分。
- en: By listing the title and headings, you can quickly get an idea of what the topic
    of the page is, assuming that they properly formatted their HTML. There is only
    supposed to be one `<title>` and one `<h1>` tag, but not everyone conforms to
    the standards.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 通过列出标题和标题，你可以快速了解页面的主题，前提是他们正确地格式化了 HTML。每个页面应该只有一个 `<title>` 和一个 `<h1>` 标签，但并非每个人都遵守标准。
- en: 'This program loads a web page and then prints the title and all headings to
    standard output. Try running this program against a few URLs and see whether you
    are able to get a quick idea of the contents just by looking at the headings:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序加载一个网页，然后将标题和所有标题打印到标准输出。尝试针对一些 URL 运行此程序，看看你是否能够仅通过查看标题就快速了解页面的内容：
- en: '[PRE14]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Crawling pages on the site that store the most common words
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 爬取网站页面，收集最常见的单词
- en: This program prints out a list of all the words used on a web page along with
    the count of how many times each word appeared in the page. This will search all
    paragraph tags. If you search the whole body, it will treat all the HTML code
    as words, which clutters the data and does not really help you understand the
    content of the site. It trims the spaces, commas, periods, tabs, and newlines
    from strings. It also converts all words to lowercase in an attempt to normalize
    the data.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序打印出网页上所有单词的列表，并计算每个单词在页面中出现的次数。它会搜索所有段落标签。如果你搜索整个正文，它会将所有 HTML 代码视为单词，这会使数据杂乱无章，且并不真正帮助你理解网站的内容。它会修剪字符串中的空格、逗号、句点、制表符和换行符。它还会将所有单词转换为小写，以便标准化数据。
- en: 'For each paragraph it finds, it will split the text contents apart. Each word
    is stored in a map that maps the string to an integer count. In the end, the map
    is printed out, listing each word and how many times it was seen on the page:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个找到的段落，它会将文本内容拆分开来。每个单词都会存储在一个映射中，该映射将字符串映射到整数计数。在最后，映射会被打印出来，列出每个单词以及它在页面上出现的次数：
- en: '[PRE15]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Printing a list of external JavaScript files in a page
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 打印页面中外部 JavaScript 文件的列表
- en: Inspecting the URLs of JavaScript files that are included on a page can help
    if you are trying to fingerprint an application or determine what third-party
    libraries are being loaded. This program will list the external JavaScript files
    referenced in a web page. External JavaScript files might be hosted on the same
    domain, or might be loaded from a remote site. It inspects the `src` attribute
    of all the `script` tags.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 检查页面中包含的 JavaScript 文件的 URL 如果你想识别一个应用程序或确定加载了哪些第三方库，可能会有所帮助。该程序将列出网页中引用的外部
    JavaScript 文件。外部 JavaScript 文件可能托管在相同的域名上，也可能从远程站点加载。它检查所有 `script` 标签的 `src`
    属性。
- en: 'For example, if an HTML page had the following tag:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个 HTML 页面有以下标签：
- en: '[PRE16]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The URL of the `src` attribute is what would be printed:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`src` 属性的 URL 将被打印出来：'
- en: '[PRE17]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that URLs in the `src` attribute may be fully qualified or relative URLs.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`src` 属性中的 URL 可以是完全限定的或相对 URL。
- en: 'The following program loads a URL and then looks for all the `script` tags.
    It will print the `src` attribute for each script it finds. This will only look
    for scripts that are linked externally. To print inline scripts, refer to the
    comment at the bottom of the file regarding `script.Text()`. Try running this
    against some websites you visit frequently and see how many external and third-party
    scripts they embed:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 以下程序加载一个 URL，然后查找所有的`script`标签。它将打印出每个找到的脚本的`src`属性。该程序只会查找外部链接的脚本。如果要打印内联脚本，请参考文件底部关于`script.Text()`的注释。试着在你经常访问的网站上运行这个程序，看看它们嵌入了多少外部和第三方脚本：
- en: '[PRE18]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This example looks for external scripts referenced by the `src` attribute, but
    some scripts are written directly in the HTML between the opening and closing
    `script` tags. These types of inline script won't have a `src` attribute referencing.
    Get inline script text using the `.Text()` function on the `goquery` object. Refer
    to the bottom of this example, where `script.Text()` is mentioned.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例查找的是由`src`属性引用的外部脚本，但有些脚本直接写在 HTML 中，位于`script`标签的开头和结尾之间。这些内联脚本不会有引用的`src`属性。可以使用`goquery`对象上的`.Text()`函数获取内联脚本的文本。请参阅本示例底部关于`script.Text()`的说明。
- en: The reason this program does not print out the inline scripts and instead focuses
    only on the externally loaded scripts is because that is where a lot of vulnerabilities
    are introduced. Loading remote JavaScript is risky and should be done with trusted
    sources only. Even then, we don't get 100% assurance that the remote content provider
    will never be compromised and serve malicious code. Consider a large corporation
    such as Yahoo! who has acknowledged publicly that their systems have been compromised
    in the past. Yahoo! also has an ad network that hosts a **Content Delivery Network**
    (**CDN**) that serves JavaScript files to a large network of websites. This would
    be a prime target for attackers. Consider these risks when including remote JavaScript
    files in a sensitive customer portal.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序之所以不打印内联脚本，而是专注于外部加载的脚本，是因为外部 JavaScript 文件常常带来很多漏洞。加载远程 JavaScript 是有风险的，应该只从可信的来源加载。即便如此，我们也无法100%确保远程内容提供商永远不会被攻破并提供恶意代码。考虑像雅虎这样的巨大企业，雅虎曾公开承认他们的系统在过去曾遭受过攻击。雅虎还有一个广告网络，托管着一个**内容分发网络**（**CDN**），为大量网站提供
    JavaScript 文件。这是攻击者的主要目标之一。考虑到这些风险，在敏感客户门户中引入远程 JavaScript 文件时要格外小心。
- en: Depth-first crawling
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度优先爬取
- en: Depth-first crawling is when you prioritize links on the same domain over links
    that lead to other domains. In this program, external links are completely ignored,
    and only paths on the same domain or relative links are followed.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 深度优先爬取是指优先考虑同一域名内的链接，而不是指向其他域名的链接。在这个程序中，外部链接会被完全忽略，只跟随同一域名下的路径或相对链接。
- en: In this example, unique paths are stored in a slice and printed all together
    at the end. Any errors encountered during the crawl are ignored. Errors are encountered
    often due to malformed links, and we don't want the whole program to exit on errors
    like that.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，唯一的路径被存储在一个切片中，并在最后一起打印出来。在爬取过程中遇到的任何错误都会被忽略。由于链接格式错误，爬取过程中经常会遇到错误，我们不希望程序因这些错误而退出。
- en: Instead of trying to parse URLs manually using string functions, the `url.Parse()`
    function is utilized. It does the work of splitting apart the host from the path.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序不使用字符串函数手动解析 URL，而是使用了`url.Parse()`函数。它可以将主机和路径分开。
- en: 'When crawling, any query strings and fragments are ignored to reduce duplicates.
    Query strings are designated with the question mark in the URL, and fragments,
    also called bookmarks, are designated with the pound or hash sign. This program
    is single-threaded and does not use goroutines:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在爬取过程中，任何查询字符串和片段都被忽略，以减少重复。查询字符串通过 URL 中的问号标记，片段也称为书签，通过井号（#）标记。这个程序是单线程的，并没有使用
    goroutines：
- en: '[PRE19]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Breadth-first crawling
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 广度优先爬取
- en: Breadth-first crawling is when priority is given to finding new domains and
    spreading out as far as possible, as opposed to continuing through a single domain
    in a depth-first manner.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 广度优先爬取是指优先寻找新的域名并尽可能地扩展，而不是继续以深度优先的方式遍历单一域名。
- en: Writing a breadth-first crawler will be left as an exercise for the reader based
    on the information provided in this chapter. It is not very different from the
    depth-first crawler in the previous section, except that it should prioritize
    URLs that point to domains that have not been seen before.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 写一个广度优先的爬虫将留给读者作为本章提供的信息基础上的练习。它与前面章节的深度优先爬虫没有太大区别，只不过它应该优先考虑那些指向尚未访问过的域名的 URL。
- en: There are a couple of notes to keep in mind. If you're not careful and you don't
    set a maximum limit, you could potentially end up crawling petabytes of data!
    You might choose to ignore subdomains, or you can enter a site that has infinite
    subdomains and you will never leave.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 有几点需要注意。如果不小心且没有设置最大限制，你可能最终会爬取到数PB的数据！你可能会选择忽略子域名，或者进入一个拥有无限子域名的网站，你将永远也爬不完。
- en: How to protect against web scraping
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何防止网页抓取
- en: It is difficult, if not impossible, to completely prevent web scraping. If you
    serve the information from the web server, there will be a way to extract the
    data programmatically somehow. There are only hurdles you can put in the way.
    It amounts to obfuscation, which you could argue is not worth the effort.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 完全防止网页爬虫是困难的，甚至可以说是不可能的。如果你从网页服务器提供信息，就总会有某种方法可以以编程方式提取数据。你只能设置障碍物。归根结底，这只是一种模糊化手段，你可以说这种做法并不值得付出太多努力。
- en: JavaScript makes it more difficult, but not impossible since Selenium can drive
    real web browsers, and frameworks such as PhantomJS can be used to execute the
    JavaScript.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: JavaScript 增加了难度，但并非不可能，因为 Selenium 可以驱动真实的网页浏览器，像 PhantomJS 这样的框架也可以用来执行 JavaScript。
- en: Requiring authentication can help limit the amount of scraping done. Rate limiting
    can also provide some relief. Rate limiting can be done using tools such as iptables
    or done at the application level, based on the IP address or user session.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 要求身份验证可以帮助限制爬虫的抓取量。速率限制也能提供一些缓解。速率限制可以使用如 iptables 等工具来实现，也可以在应用层基于 IP 地址或用户会话来实现。
- en: Checking the user agent provided by the client is a shallow measure, but can
    help a bit. Discard requests that come with user agents that include keywords
    such as `curl`, `wget`, `go`, `python`, `ruby`, and `perl`. Blocking or ignoring
    these requests can prevent simple bots from scraping your site, but the client
    can fake or omit their user agent so that it is easy to bypass.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 检查客户端提供的用户代理是一种浅显的措施，但可以稍微起到一些作用。丢弃包含如`curl`、`wget`、`go`、`python`、`ruby`和`perl`等关键字的用户代理的请求。阻止或忽略这些请求可以防止简单的爬虫抓取你的网站，但客户端可以伪造或省略其用户代理，绕过这一限制。
- en: If you want to take it even further, you can make the HTML ID and class names
    dynamic so that they can't be used to find specific information. Change your HTML
    structure and naming frequently to play the *cat-and-mouse* game to make it more
    work than it is worth for the scraper. This is not a real solution, and I wouldn't
    recommend it, but it is worth mentioning, as it is annoying in the eyes of the
    scraper.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想更进一步，你可以使 HTML 的 ID 和类名动态化，以便无法用于查找特定信息。频繁更改你的 HTML 结构和命名，进行*猫鼠游戏*，让爬虫抓取变得比值得做的工作更繁琐。这不是一个真正的解决方案，我不推荐这么做，但值得一提，因为它在爬虫眼中是令人烦恼的。
- en: You can use JavaScript to check information about the client, such as screen
    size, before presenting data. If the screen size is 1 x 1 or 0 × 0, or something
    strange, you can assume that it is a bot and refuse to render content.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 JavaScript 在展示数据之前检查客户端的信息，例如屏幕大小。如果屏幕大小为 1 x 1 或 0 x 0，或者其他奇怪的尺寸，你可以假设它是爬虫，并拒绝渲染内容。
- en: Honeypot forms are another method of detecting bot behavior. Hide form fields
    with CSS or a `hidden` attribute, and check whether values have been provided
    in those fields. If data is in these fields, assume that a bot is filling out
    all the fields and ignore the request.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 蜂蜜罐表单是另一种检测爬虫行为的方法。通过 CSS 或 `hidden` 属性隐藏表单字段，检查这些字段中是否有值。如果这些字段中有数据，假设是爬虫填写了所有字段并忽略该请求。
- en: Another option is to use images to store information instead of text. For example,
    if you output only the image of a pie chart, it is much more difficult for someone
    to scrape the data than when you output the data as a JSON object and have JavaScript
    render the pie chart. The scraper can grab the JSON data directly. Text can be
    placed in images as well to prevent text from being scraped and to prevent keyword
    text searches, but **Optical Character Recognition** (**OCR**) can get around
    that with some extra effort.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是使用图像而非文本来存储信息。例如，如果你只输出一个饼状图的图像，那么相比输出数据作为JSON对象并让JavaScript渲染饼状图，别人抓取数据要困难得多。抓取程序可以直接抓取JSON数据。文本也可以放入图像中，以防止文本被抓取，并防止关键词文本搜索，但**光学字符识别**（**OCR**）可以通过一些额外的努力绕过这一点。
- en: Depending on the application, some of the preceding techniques can be useful.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 根据应用的不同，前述的一些技术可能会很有用。
- en: Summary
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Having read this chapter, you should now understand the fundamentals of web
    scraping, such as performing an HTTP `GET` request and searching for a string
    using string matching or regular expressions to find HTML comments, emails, and
    other keywords. You should also understand how to extract the HTTP headers and
    set custom headers to set cookies and custom user agent strings. Moreover, you
    should understand the basic concepts of fingerprinting and have some idea of how
    to gather information about a web application based on the source code provided.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 读完本章后，你应该已经理解了网页抓取的基本原理，例如执行HTTP `GET`请求，并使用字符串匹配或正则表达式搜索HTML评论、电子邮件和其他关键词。你还应该理解如何提取HTTP头部，并设置自定义头部以设置Cookies和自定义用户代理字符串。此外，你应该了解指纹识别的基本概念，并对如何根据提供的源代码收集Web应用程序的信息有一定的了解。
- en: Having worked through this chapter, you should also understand the basics of
    using the `goquery` package to find HTML elements in the DOM in a jQuery style.
    You should feel comfortable finding links in a web page, finding documents, listing
    title and headers, finding JavaScript files, and finding the difference between
    breadth-first and depth-first crawling.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章的学习，你应该已经理解了如何使用`goquery`包以jQuery风格查找DOM中的HTML元素。你应该能轻松找到网页中的链接，找到文档，列出标题和头部，找到JavaScript文件，并理解广度优先与深度优先爬虫的区别。
- en: A note about scraping public websites—be respectful. Don't produce unreasonable
    amounts of traffic to websites by sending huge batches or letting a crawler go
    uninhibited. Set reasonable rate limits and maximum page count limits on programs
    you write as to not overburden remote servers. If you are scraping for data, always
    check to see if an API is available instead. APIs are much more efficient and
    intended to be used programmatically.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 关于抓取公共网站的说明——请保持尊重。不要通过发送大量数据或让爬虫不受限制地运行，给网站带来不合理的流量。对你编写的程序设置合理的速率限制和最大页面数限制，以免给远程服务器带来过大负担。如果你是为了抓取数据，最好检查是否有API可以使用。API通常更高效且是为了程序化使用而设计的。
- en: Can you think of any other way to apply the tools examined in this chapter?
    Can you think of any additional features you can add to the examples provided?
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 你能想到本章中讨论的工具可以应用的其他方法吗？你能想到可以为示例添加的其他功能吗？
- en: In the next chapter, we will look at the methods of host discovery and enumeration.
    We will cover things such as TCP sockets, proxies, port scanning, banner grabbing,
    and fuzzing.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍主机发现与枚举的方法。我们将涵盖TCP套接字、代理、端口扫描、横幅抓取和模糊测试等内容。
