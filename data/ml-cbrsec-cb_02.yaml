- en: Machine Learning-Based Malware Detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于机器学习的恶意软件检测
- en: In this chapter, we begin to get serious about applying data science to cybersecurity.
    We will begin by learning how to perform static and dynamic analysis on samples.
    Building on this knowledge, we will learn how to featurize samples in order to
    construct a dataset with informative features. The highlight of the chapter is
    learning how to build a static malware detector using the featurization skills
    we have learned. Finally, you will learn how to tackle important machine learning
    challenges that occur in the domain of cybersecurity, such as class imbalance
    and **false positive rate** (**FPR**) constraints.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开始认真地将数据科学应用于网络安全。我们将从学习如何对样本进行静态和动态分析开始。在此基础上，我们将学习如何对样本进行特征化，以便构建一个具有信息量的特征的数据集。本章的亮点是学习如何使用我们学到的特征化技能构建静态恶意软件检测器。最后，您将学习如何解决网络安全领域中常见的机器学习挑战，如类别不平衡和**假阳性率**（**FPR**）限制。
- en: 'The chapter covers the following recipes:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下内容：
- en: Malware static analysis
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恶意软件静态分析
- en: Malware dynamic analysis
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恶意软件动态分析
- en: Using machine learning to detect the file type
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习检测文件类型
- en: Measuring the similarity between two strings
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量两个字符串之间的相似度
- en: Measuring the similarity between two files
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量两个文件之间的相似度
- en: Extracting N-grams
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取 N-gram
- en: Selecting the best N-grams
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择最佳 N-gram
- en: Building a static malware detector
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建静态恶意软件检测器
- en: Tackling class imbalance
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决类别不平衡问题
- en: Handling type I and type II errors
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理类型 I 和类型 II 错误
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will be using the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下工具：
- en: YARA
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YARA
- en: '`pefile`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pefile`'
- en: '`PyGitHub`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PyGitHub`'
- en: Cuckoo Sandbox
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cuckoo 沙箱
- en: '**Natural Language Toolkit** (**NLTK**)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言工具包**（**NLTK**）'
- en: '`imbalanced-learn`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`imbalanced-learn`'
- en: The code and datasets can be found at [https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter02](https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter02).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 代码和数据集可以在 [https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter02](https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter02)
    找到。
- en: Malware static analysis
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 恶意软件静态分析
- en: In static analysis, we examine a sample without executing it. The amount of
    information that can be obtained this way is large, ranging from something as
    simple as the name of the file to the more complex, such as specialized YARA signatures.
    We will be covering a selection of the large variety of features you could obtain
    by statically analyzing a sample. Despite its power and convenience, static analysis
    is no silver bullet, mainly because software can be obfuscated. For this reason,
    we will be employing dynamic analysis and other techniques in later chapters.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在静态分析中，我们在不执行样本的情况下进行检查。通过这种方式可以获得大量信息，从文件名称到更复杂的信息，如专用的 YARA 签名。我们将介绍通过静态分析样本可以获取的各种特征。尽管静态分析强大且方便，但它并不是万能的，主要是因为软件可能被混淆。因此，我们将在后续章节中使用动态分析和其他技术。
- en: Computing the hash of a sample
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算样本的哈希值
- en: Without delving into the intricacies of hashing, a hash is essentially a short
    and unique string signature. For example, we may hash the sequence of bytes of
    a file to obtain an essentially unique code for that file. This allows us to quickly
    compare two files to see whether they are identical.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不深入探讨哈希的复杂性，哈希本质上是一个简短且唯一的字符串签名。例如，我们可以对文件的字节序列进行哈希处理，从而得到该文件的唯一代码。这使我们能够快速比较两个文件，查看它们是否相同。
- en: There exist many hash procedures out there, so we will focus on the most important
    ones, namely, SHA256 and MD5\. Note that MD5 is known to exhibit vulnerabilities
    due to hash collisions—instances where two different objects have the same hash
    and, therefore, should be used with caution. In this recipe, we take an executable
    file and compute its MD5 and SHA256 hashes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 市面上有许多哈希算法，因此我们将重点介绍最重要的几种，即 SHA256 和 MD5。需要注意的是，MD5 因哈希碰撞（即两个不同的对象具有相同的哈希值）而存在已知的漏洞，因此使用时需要小心。在本教程中，我们将使用一个可执行文件，并计算它的
    MD5 和 SHA256 哈希值。
- en: Getting ready
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Preparation for this recipe consists of downloading a test file, which is the
    Python executable from [https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe](https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程的准备工作包括下载一个测试文件，即来自 [https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe](https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe)
    的 Python 可执行文件。
- en: How to do it...
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In the following steps, we will see how to obtain the hash of a file:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下步骤中，我们将展示如何获取文件的哈希值：
- en: 'Begin by importing the libraries and selecting the desired file you wish to
    hash:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入库并选择您希望计算哈希的文件：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Instantiate the MD5 and SHA256 objects, and specify the size of the chunks
    we will be reading:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化MD5和SHA256对象，并指定我们将读取的块的大小：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We then read in the file in chunks of 64 KB and incrementally construct our
    hashes:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将文件以64 KB为块进行读取，并增量构建哈希值：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, print out the resulting hashes:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，输出计算结果的哈希值：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This results in the following output:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How it works...
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何工作…
- en: 'This section will explain the steps that have been provided in the previous
    section:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将解释前面章节中提供的步骤：
- en: In step 1, we import `hashlib`, a standard Python library for hash computation.
    We also specify the file we will be hashing—in this case, the file is `python-3.7.2-amd64.exe`.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在步骤1中，我们导入了`hashlib`，一个用于哈希计算的标准Python库。我们还指定了我们将要计算哈希的文件——在这个例子中，文件是`python-3.7.2-amd64.exe`。
- en: In step 2, we instantiate an `md5` object and an `sha256` object and specify
    the size of the chunks we will be reading.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在步骤2中，我们实例化一个`md5`对象和一个`sha256`对象，并指定我们将读取的块的大小。
- en: In step 3, we utilize the `.update(data)` method. This method allows us to compute
    the hash incrementally because it computes the hash of the concatenation. In other
    words, `hash.update(a)` followed by `hash.update(b)` is equivalent to `hash.update(a+b)`.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在步骤3中，我们使用`.update(data)`方法。这个方法允许我们增量计算哈希，因为它计算的是连接字符串的哈希。换句话说，`hash.update(a)`后跟`hash.update(b)`等同于`hash.update(a+b)`。
- en: In step 4, we print out the hashes in hexadecimal digits.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在步骤4中，我们以十六进制数字的形式输出哈希值。
- en: 'We can also verify that our computation is consistent with the hash calculations
    given by other sources, such as VirusTotal and the official Python website. The
    MD5 hash is displayed on the Python web page ([https://www.python.org/downloads/release/python-372/](https://www.python.org/downloads/release/python-372/)):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以验证我们的计算结果是否与其他来源提供的哈希计算一致，比如VirusTotal和官方Python网站。MD5哈希值显示在Python网页上（[https://www.python.org/downloads/release/python-372/](https://www.python.org/downloads/release/python-372/)）：
- en: '![](assets/b284d160-12e6-4289-8b11-4c1c3f4bea6b.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/b284d160-12e6-4289-8b11-4c1c3f4bea6b.png)'
- en: 'The SHA256 hash is computed by uploading the file to VirusTotal ([https://www.virustotal.com/gui/home](https://www.virustotal.com/gui/home)):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过将文件上传到VirusTotal（[https://www.virustotal.com/gui/home](https://www.virustotal.com/gui/home)）来计算SHA256哈希值：
- en: '![](assets/12683aeb-2465-444a-8c15-582da8136d2d.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/12683aeb-2465-444a-8c15-582da8136d2d.png)'
- en: YARA
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YARA
- en: 'YARA is a computer language that allows a security expert to conveniently specify
    a rule that will then be used to classify all samples matching the rule. A minimal
    rule consists of a name and a condition, for example, the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: YARA是一种计算机语言，允许安全专家方便地指定规则，然后用该规则对所有符合条件的样本进行分类。一个最小的规则包含一个名称和一个条件，例如，以下内容：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This rule will not match any file. Conversely, the following rule will match
    every sample:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个规则不会匹配任何文件。相反，下面的规则将匹配每个样本：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'A more useful example will match any file over 100 KB:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更有用的例子是匹配任何大于100 KB的文件：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Another example is checking whether a particular file is a PDF. To do so, we
    check if the magic numbers of the file correspond to the PDF. Magic numbers are
    a sequence of several bytes that occurs at the beginning of a file and indicates
    the type of file it is. In the case of a PDF, the sequence is `25 50 44 46`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是检查某个特定文件是否为PDF文件。为此，我们需要检查文件的魔术数字是否与PDF文件的魔术数字匹配。魔术数字是文件开头的一段字节序列，表示文件类型。在PDF文件的情况下，该序列为`25
    50 44 46`：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, let's see how to run our rules against files.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何将规则应用于文件。
- en: Getting ready
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Preparation for this recipe consists of installing YARA on your device. Instructions
    can be found at [https://yara.readthedocs.io/en/stable/](https://yara.readthedocs.io/en/stable/).
    For Windows, you need to download an executable file for YARA.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程的准备工作包括在设备上安装YARA。安装说明可以在[https://yara.readthedocs.io/en/stable/](https://yara.readthedocs.io/en/stable/)找到。对于Windows，您需要下载YARA的可执行文件。
- en: How to do it…
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'In the following steps, we show you how to create YARA rules and test them
    against a file:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下步骤中，我们将向您展示如何创建YARA规则并对文件进行测试：
- en: 'Copy your rules, as seen here, into a text file and name it `rules.yara`:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您的规则（如下面所示）复制到文本文件中并命名为`rules.yara`：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, select a file you would like to check your rules against. Call it `target_file`.
    In a terminal, execute `Yara rules.yara target_file` as follows:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，选择一个你希望用规则进行检查的文件。称之为`target_file`。在终端中执行`Yara rules.yara target_file`，如下所示：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The result should be as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应该如下所示：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: How it works…
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: As you can observe, in *Step 1*, we copied several YARA rules. The first rule
    checks the magic numbers of a file to see if they match those of a PDF. The other
    two rules are trivial rules—one that matches every file, and one that matches
    no file. Then, in *Step 2*, we used the YARA program to run the rules against
    the target file. We saw from a printout that the file matched some rules but not
    others, as expected from an effective YARA ruleset.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在*步骤 1*中，我们复制了几条YARA规则。第一条规则检查文件的魔术数字，看看它们是否与PDF文件的魔术数字匹配。其他两条规则是简单的规则——一条匹配所有文件，另一条不匹配任何文件。然后，在*步骤
    2*中，我们使用YARA程序将这些规则应用到目标文件上。通过打印输出，我们看到文件匹配了一些规则，但没有匹配其他规则，这符合有效的YARA规则集的预期。
- en: Examining the PE header
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查PE头
- en: '**Portable executable** (**PE**) files are a common Windows file type. PE files
    include the `.exe`, `.dll`, and `.sys` files. All PE files are distinguished by
    having a PE header, which is a header section of the code that instructs Windows
    on how to parse the subsequent code. The fields from the PE header are often used
    as features in the detection of malware. To easily extract the multitude of values
    of the PE header, we will utilize the `pefile` Python module. In this recipe,
    we will parse the PE header of a file, and then print out notable portions of
    it.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**便携式可执行文件** (**PE**) 是一种常见的Windows文件类型。PE文件包括`.exe`、`.dll`和`.sys`文件。所有PE文件都有一个PE头，这是代码的一个头部部分，指示Windows如何解析随后的代码。PE头中的字段通常作为特征被用于恶意软件的检测。为了方便提取PE头的众多值，我们将使用`pefile`
    Python模块。在本食谱中，我们将解析一个文件的PE头，然后打印出其中一些重要部分。'
- en: Getting ready
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'Preparation for this recipe consists of installing the `pefile` package in
    `pip`. In a terminal of your Python environment, run the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱的准备工作包括在`pip`中安装`pefile`包。在你的Python环境的终端中运行以下命令：
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In addition, download the test file Python executable from [https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe](https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，从[https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe](https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe)下载测试文件Python可执行文件。
- en: How to do it...
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In the following steps, we will parse the PE header of a file, and then print
    out notable portions of it:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将解析一个文件的PE头，并打印出其中一些重要部分：
- en: 'Import the PE file and use it to parse the PE header of your desired file:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入PE文件并使用它来解析你希望检查的文件的PE头：
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'List the imports of the PE file:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出PE文件的导入项：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'A small portion of the output is shown here:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这里显示了一小部分输出：
- en: '![](assets/ab7af973-2df6-4626-9f0c-071d90b86067.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ab7af973-2df6-4626-9f0c-071d90b86067.png)'
- en: 'List the sections of the PE file:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出PE文件的各个部分：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output of the previous code is as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 之前代码的输出结果如下：
- en: '![](assets/494806e9-3dab-4482-890a-e74329520e6f.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/494806e9-3dab-4482-890a-e74329520e6f.png)'
- en: 'Print a full dump of the parsed information:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印解析信息的完整转储：
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'A small portion of the output is displayed here:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里显示了一小部分输出：
- en: '![](assets/b237281e-fa9c-4bcc-ae48-672e34331f3f.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/b237281e-fa9c-4bcc-ae48-672e34331f3f.png)'
- en: How it works...
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We began in *step 1* by importing the `pefile` library and specifying which
    file we will be analyzing. In this case, the file was `python-3.7.2-amd64.exe`,
    though it is just as easy to analyze any other PE file. We then continued on to
    examine the DLLs being imported by the file, in order to understand which methods
    the file may be using in *Step 2*. DLLs answer this question because a DLL is
    a library of code that other applications may call upon. For example, `USER32.dll`
    is a library that contains Windows USER, a component of the Microsoft Windows
    operating system that provides core functionality for building user interfaces.
    The component allows other applications to leverage the functionality for window
    management, message passing, input processing, and standard controls. Logically
    then, if we see that a file is importing a method such as `GetCursorPos`, then
    it is likely to be looking to determine the position of the cursor. Continuing
    in *step 3*, we printed out the sections of the PE file. These provide a logical
    and physical separation to the different parts of a program, and therefore offer
    the analyst valuable information about the program. Finally, we printed out all
    of the parsed PE header information from the file in preparation for later utilizing
    it for feature engineering (*Step 4*).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从*步骤 1*开始，导入了`pefile`库，并指定了我们将要分析的文件。在这个例子中，文件是`python-3.7.2-amd64.exe`，但同样也可以轻松分析任何其他PE文件。接着，我们继续检查文件所导入的DLL，以了解文件可能使用的方法，在*步骤
    2*中进行分析。DLL可以回答这个问题，因为DLL是代码库，其他应用程序可能会调用它。例如，`USER32.dll`是一个包含Windows USER的库，它是Microsoft
    Windows操作系统的一部分，提供用于构建用户界面的核心功能。该组件允许其他应用程序利用窗口管理、消息传递、输入处理和标准控件等功能。因此，从逻辑上讲，如果我们看到一个文件导入了像`GetCursorPos`这样的函数，那么它很可能是在查看光标的位置。在*步骤
    3*中，我们打印了PE文件的各个部分。它们为程序的不同部分提供了逻辑和物理的分隔，因此为分析人员提供了有关程序的宝贵信息。最后，我们打印了文件的所有解析后的PE头部信息，为后续用于特征工程做好准备（*步骤
    4*）。
- en: Featurizing the PE header
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取PE头部特征
- en: In this section, we will extract features from the PE header to be used in building
    a `malware/benign` samples classifier. We will continue utilizing the `pefile`
    Python module.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从PE头部提取特征，用于构建`恶意/良性`样本分类器。我们将继续使用`pefile` Python模块。
- en: Getting ready
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'Preparation for this recipe consists of installing the `pefile` package in
    `pip`. The command is as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 本配方的准备工作包括在`pip`中安装`pefile`包。命令如下：
- en: '[PRE17]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In addition, benign and malicious files have been provided for you in the `PE
    Samples Dataset` folder in the root of the repository. Extract all archives named
    `Benign PE Samples*.7z` to a folder named `Benign PE Samples`. Extract all archives
    named `Malicious PE Samples*.7z` to a folder named `Malicious PE Samples`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，良性和恶意文件已经提供给你，位于根目录下的`PE Samples Dataset`文件夹中。将所有名为`Benign PE Samples*.7z`的压缩包解压到名为`Benign
    PE Samples`的文件夹中。将所有名为`Malicious PE Samples*.7z`的压缩包解压到名为`Malicious PE Samples`的文件夹中。
- en: How to do it...
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In the following steps, we will collect notable portions of the PE header:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将收集PE头部的显著部分：
- en: 'Import `pefile` and modules for enumerating our samples:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pefile`和用于列举样本的模块：
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We define a function to collect the names of the sections of a file and preprocess
    them for readability and normalization:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了一个函数来收集文件的节名称，并对其进行预处理以提高可读性和标准化：
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We define a convenience function to preprocess and standardize our imports:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了一个便捷函数来预处理和标准化我们的导入信息：
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We then define a function to collect the imports from a file using `pefile`:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义一个函数，使用`pefile`收集文件中的导入信息：
- en: '[PRE21]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we prepare to iterate through all of our files and create lists to
    store our features:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们准备迭代所有文件，并创建列表来存储我们的特征：
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In addition to collecting the preceding features, we also collect the number
    of sections of a file:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了收集前述特征外，我们还会收集文件的节数：
- en: '[PRE23]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In case a file''s PE header cannot be parsed, we define a try-catch clause:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果无法解析文件的PE头部，我们定义了一个try-catch语句：
- en: '[PRE24]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: How it works...
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: As you can see, in *Step 1*, we imported the `pefile` module to enumerate the
    samples. Once that is done, we define the convenience function, as you can see
    in *Step 2*. The reason being that it often imports using varying cases (upper/lower).
    This causes the same import to appear as distinct imports.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在*步骤 1*中，我们导入了`pefile`模块来列举样本。完成之后，我们定义了一个便捷函数，正如你在*步骤 2*中看到的那样。这样做的原因是它经常使用不同的大小写（大写/小写）导入。这会导致相同的导入看起来像是不同的导入。
- en: After preprocessing the imports, we then define another function to collect
    all the imports of a file into a list. We will also define a function to collect
    the names of the sections of a file in order to standardize these names such as `.text`,
    `.rsrc`, and `.reloc` while containing distinct parts of the file (*Step 3*).
    The files are then enumerated in our folders and empty lists will be created to
    hold the features we will be extracting. The predefined functions will then collect
    the imports (*Step 4*), section names, and the number of sections of each file
    (*Steps 5* and *6*). Lastly, a try-catch clause will be defined in case a file's
    PE header cannot be parsed (*Step 7*). This can happen for many reasons. One reason
    being that the file is not actually a PE file. Another reason is that its PE header
    is intentionally or unintentionally malformed.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在预处理导入项后，我们定义另一个函数，将文件的所有导入项收集到一个列表中。我们还将定义一个函数，用来收集文件的各个部分的名称，以便规范化这些名称，如`.text`、`.rsrc`和`.reloc`，同时包含文件的不同部分（*步骤
    3*）。然后，文件将在我们的文件夹中进行枚举，空列表将被创建，用于存放我们将要提取的特征。预定义的函数将收集导入项（*步骤 4*）、部分名称以及每个文件的部分数量（*步骤
    5* 和 *6*）。最后，将定义一个try-catch语句块，以防某个文件的PE头无法解析（*步骤 7*）。这种情况可能由于多种原因发生。一个原因是文件本身并不是PE文件。另一个原因是其PE头部故意或无意地被篡改。
- en: Malware dynamic analysis
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 恶意软件动态分析
- en: Unlike static analysis, dynamic analysis is a malware analysis technique in
    which the expert executes the sample, and then studies the sample's behavior as
    it is being run. The main advantage of dynamic analysis over static is that it
    allows you to bypass obfuscation by simply observing how a sample behaves, rather
    than trying to decipher the sample's contents and behavior. Since malware is intrinsically
    unsafe, researchers resort to executing samples in a **virtual machine** (**VM**).
    This is called **sandboxing**.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 与静态分析不同，动态分析是一种恶意软件分析技术，其中专家执行样本，然后在样本运行时研究其行为。动态分析相对于静态分析的主要优势是，它允许通过观察样本的行为来绕过混淆，而不是试图解读样本的内容和行为。由于恶意软件本质上是不安全的，研究人员会选择在**虚拟机**（**VM**）中执行样本。这被称为**沙箱化**。
- en: Getting ready
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'One of the most prominent tools for automating the analysis of samples in a
    VM is Cuckoo Sandbox. The initial installation of Cuckoo Sandbox is straightforward;
    simply run the following command:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在虚拟机中自动化样本分析的最突出工具之一是Cuckoo沙箱。Cuckoo沙箱的初始安装非常简单；只需运行以下命令：
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: You must make sure that you also have a VM that your machine can control. Configuring
    the sandbox can be a challenge, but instructions are available at [https://cuckoo.sh/docs/](https://cuckoo.sh/docs/).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须确保还拥有可以由您的机器控制的虚拟机。配置沙箱可能具有挑战性，但可以通过[https://cuckoo.sh/docs/](https://cuckoo.sh/docs/)的说明来完成。
- en: We show now how to utilize Cuckoo Sandbox to obtain a dynamic analysis of a
    sample.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们展示如何利用Cuckoo沙箱来获取样本的动态分析。
- en: How to do it...
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Once your Cuckoo Sandbox is set up, and has a web interface running, follow
    these steps to gather runtime information about a sample:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的Cuckoo沙箱设置完成，并且运行了Web界面，按照以下步骤收集样本的运行时信息：
- en: 'Open up your web interface (the default location is `127.0.0.1:8000`), click
    **SUBMIT A FILE FOR ANALYSIS**, and select the sample you wish to analyze:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开您的Web界面（默认位置是`127.0.0.1:8000`），点击**提交文件进行分析**，并选择您希望分析的样本：
- en: '![](assets/291e48cd-c544-4e1e-a6a6-db1011cdb26d.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/291e48cd-c544-4e1e-a6a6-db1011cdb26d.png)'
- en: 'The following screen will appear automatically. In it, select the type of analysis
    you wish to perform on your sample:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下屏幕将自动出现。在其中，选择您希望对样本执行的分析类型：
- en: '![](assets/e3968f76-8e95-4f15-b910-28254321e6da.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/e3968f76-8e95-4f15-b910-28254321e6da.png)'
- en: 'Click **Analyze** to analyze the sample in your sandbox. The result should
    look as follows:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**分析**以在沙箱中分析样本。结果应如下所示：
- en: '![](assets/8f250a32-58bd-4c91-82bf-2c007cfaee3d.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/8f250a32-58bd-4c91-82bf-2c007cfaee3d.png)'
- en: 'Next, open up the report for the sample you have analyzed:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，打开您分析的样本的报告：
- en: '![](assets/9bb0c653-8e33-4dee-9e86-434341350844.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/9bb0c653-8e33-4dee-9e86-434341350844.png)'
- en: 'Select the **Behavioral Analysis** tab:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**行为分析**标签：
- en: '![](assets/01b72b50-2410-48d1-8f35-fb5aeb472c81.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/01b72b50-2410-48d1-8f35-fb5aeb472c81.png)'
- en: The displayed sequence of API calls, registry key changes, and other events
    can all be used as input to a classifier.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 显示的API调用顺序、注册表键更改和其他事件均可用作分类器的输入。
- en: How it works...
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: At a conceptual level, obtaining dynamic analysis results consists of running
    samples in environments that allow the analyst to collect runtime information.
    Cuckoo Sandbox is a flexible framework with prebuilt modules to do just that.
    We began our recipe for using Cuckoo Sandbox by opening up the web portal (*Step
    1*). A **command-line interface** (**CLI**) exists as well. We proceeded to submit
    a sample and select the type of analysis we wished to perform (*Steps 2* and *3*).
    These steps, too, can be performed through the Cuckoo CLI. We proceeded to examine
    the analysis report (*Step 4*). You can see at this stage how the many modules
    of Cuckoo Sandbox reflect in the final analysis output. For instance, if a module
    for capturing traffic is installed and used, then the report will contain the
    data captured in the network tab. We proceeded to focus our view of the analysis
    to behavioral analysis (*Step 5*), and in particular to observe the sequence of
    API calls. API calls are basically operations performed by the OS. This sequence
    makes up a fantastic feature set that we will utilize to detect malware in future
    recipes. Finally, note that in a production environment, it may make sense to
    create a custom-made sandbox with custom modules for data collection, as well
    as equip it with anti-VM detection software to facilitate successful analysis.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，获取动态分析结果包括在允许分析员收集运行时信息的环境中运行样本。Cuckoo Sandbox是一个灵活的框架，带有预构建模块来完成这一任务。我们从打开Cuckoo
    Sandbox的Web门户开始了我们的操作流程（*步骤1*）。**命令行界面**（**CLI**）也可用。我们继续提交一个样本并选择希望执行的分析类型（*步骤2*和*步骤3*）。这些步骤也可以通过Cuckoo
    CLI来执行。接着，我们查看了分析报告（*步骤4*）。此时你可以看到Cuckoo Sandbox的许多模块如何反映在最终的分析结果中。例如，如果安装并使用了一个捕获流量的模块，那么报告中会包含网络标签中捕获的数据。我们继续将视角集中在行为分析（*步骤5*）上，特别是观察API调用的顺序。API调用基本上是操作系统执行的操作。这个顺序构成了一个极好的特征集，我们将利用它在未来的操作中检测恶意软件。最后，值得注意的是，在生产环境中，可能有必要创建一个定制的沙箱，配备自定义的数据收集模块，并搭载反虚拟机检测软件，以促进成功的分析。
- en: Using machine learning to detect the file type
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用机器学习检测文件类型
- en: One of the techniques hackers use to sneak their malicious files into security
    systems is to obfuscate their file types. For example, a (malicious) PowerShell
    script is expected to have an extension, `.ps1`. A system administrator can aim
    to combat the execution of all PowerShell scripts on a system by preventing the
    execution of all files with the `.ps1` extension. However, the mischievous hacker
    can remove or change the extension, rendering the file's identity a mystery. Only
    by examining the contents of the file can it then be distinguished from an ordinary
    text file. For practical reasons, it is not possible for humans to examine all
    text files on a system. Consequently, it is expedient to resort to automated methods.
    In this chapter, we will demonstrate how you can use machine learning to detect
    the file type of an unknown file. Our first step is to curate a dataset.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 黑客常用的技巧之一就是通过混淆文件类型来悄悄将恶意文件渗透到安全系统中。例如，一个（恶意的）PowerShell脚本通常应该有`.ps1`的扩展名。系统管理员可以通过阻止所有带有`.ps1`扩展名的文件执行，来防止PowerShell脚本的执行。然而，狡猾的黑客可以删除或更改扩展名，使得文件的身份变得模糊。只有通过检查文件的内容，才能将其与普通文本文件区分开来。由于实际原因，人类不可能检查系统中的所有文本文件。因此，采用自动化方法就显得尤为重要。在本章中，我们将展示如何利用机器学习来检测未知文件的文件类型。我们的第一步是策划一个数据集。
- en: Scraping GitHub for files of a specific type
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从GitHub抓取特定类型的文件
- en: To curate a dataset, we will scrape GitHub for the specific file types we are
    interested in.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了策划数据集，我们将从GitHub上抓取我们感兴趣的特定文件类型。
- en: Getting ready
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Preparation for this recipe consists of installing the `PyGitHub` package in
    `pip` by running the following command:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 准备工作包括通过运行以下命令，在`pip`中安装`PyGitHub`包：
- en: '[PRE26]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In addition, you will need GitHub account credentials.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还需要GitHub账户凭证。
- en: How to do it...
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'In the following steps, we curate a dataset and then use it to create a classifier
    to determine the file type. For demonstration purposes, we show how to obtain
    a collection of PowerShell scripts, Python scripts, and JavaScript files by scraping
    GitHub. A collection of samples obtained in this way can be found in the accompanying
    repository as `PowerShellSamples.7z`, `PythonSamples.7z`, and `JavascriptSamples.7z`.
    First, we will write the code for the JavaScript scraper:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将整理一个数据集，并利用它创建一个分类器来确定文件类型。为了演示，我们将展示如何通过抓取GitHub获取PowerShell脚本、Python脚本和JavaScript文件的集合。通过这种方式获得的示例集合可以在随附的仓库中找到，文件名为`PowerShellSamples.7z`、`PythonSamples.7z`和`JavascriptSamples.7z`。首先，我们将编写用于抓取JavaScript文件的代码：
- en: 'Begin by importing the `PyGitHub` library in order to be able to call the GitHub
    API. We also import the `base64` module for decoding the `base64` encoded files:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先导入`PyGitHub`库，以便能够调用GitHub API。我们还导入了`base64`模块，以便解码`base64`编码的文件：
- en: '[PRE27]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We must supply our credentials, and then specify a query—in this case, for
    JavaScript—to select our repositories:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们必须提供我们的凭证，然后指定一个查询——在这种情况下，查询JavaScript——来选择我们的仓库：
- en: '[PRE28]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We loop over the repositories matching our criteria:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们遍历符合条件的仓库：
- en: '[PRE29]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We create a directory for each repository matching our search criteria, and
    then read in its contents:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为每个匹配我们搜索标准的仓库创建一个目录，然后读取其内容：
- en: '[PRE30]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We add all directories of the repository to a queue in order to list all of
    the files contained within the directories:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将所有仓库的目录添加到队列中，以便列出这些目录中包含的所有文件：
- en: '[PRE31]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'If we find a non-directory file, we check whether its extension is `.js`:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们发现一个非目录文件，我们检查它的扩展名是否为`.js`：
- en: '[PRE32]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'If the extension is `.js`, we write out a copy of the file:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果扩展名是`.js`，我们将文件的副本写出：
- en: '[PRE33]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Once finished, it is convenient to move all the JavaScript files into one folder.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦完成，方便的方法是将所有JavaScript文件移到一个文件夹中。
- en: 'To obtain PowerShell samples, run the same code, changing the following:'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要获取PowerShell示例，请运行相同的代码，并修改以下内容：
- en: '[PRE34]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'To the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来：
- en: '[PRE35]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Similarly, for Python files, we do the following:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，对于Python文件，我们进行如下操作：
- en: '[PRE36]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: How it works...
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理……
- en: We start by importing the `PyGitHub` library in *Step 1* in order to be able
    to conveniently call the GitHub APIs. These will allow us to scrape and explore
    the universe of repositories. We also import the `base64` module for decoding
    the `base64` encoded files that we will be downloading from GitHub. Note that
    there is a rate limit on the number of API calls a generic user can make to GitHub.
    For this reason, you will find that if you attempt to download too many files
    in a short duration, your script will not get all of the files. Our next step
    is to supply our credentials to GitHub (*step 2*), and specify that we are looking
    for repositories with JavaScript, using the `query='language:javascript'` command.
    We enumerate such repositories matching our criteria of being associated with
    JavaScript, and if they do, we search through these for files ending with `.js` and
    create local copies (steps 3 to 6). Since these files are encoded in `base64`,
    we make sure to decode them to plaintext in step 7\. Finally, we show you how
    to adjust the script in order to scrape other file types, such as Python and PowerShell
    (Step 8).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从导入`PyGitHub`库开始，进行*步骤1*，这样我们就能方便地调用GitHub API。这些API将允许我们抓取并探索仓库的世界。我们还导入了`base64`模块，以解码我们从GitHub下载的`base64`编码的文件。请注意，GitHub对普通用户的API调用次数有限制。因此，如果你尝试在短时间内下载太多文件，你的脚本可能无法获取所有文件。我们的下一步是向GitHub提供我们的凭证（*步骤2*），并指定我们正在寻找具有JavaScript语言的仓库，使用`query='language:javascript'`命令。我们列举出所有符合我们搜索条件的JavaScript仓库，如果符合条件，我们继续在这些仓库中查找以`.js`结尾的文件，并创建本地副本（步骤3到6）。由于这些文件是`base64`编码的，我们确保在步骤7中将它们解码为纯文本。最后，我们展示如何调整脚本，以便抓取其他类型的文件，比如Python和PowerShell文件（步骤8）。
- en: Classifying files by type
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按文件类型分类
- en: Now that we have a dataset, we would like to train a classifier. Since the files
    in question are scripts, we approach the problem as an NLP problem.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个数据集，我们希望训练一个分类器。由于相关文件是脚本文件，我们将问题作为一个自然语言处理（NLP）问题来处理。
- en: Getting ready
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Preparation for this recipe consists of installing the `scikit-learn` package
    in `pip`. The instructions are as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这个步骤的准备工作包括在`pip`中安装`scikit-learn`包。安装说明如下：
- en: '[PRE37]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In addition, we have supplied you with samples of each file type in the `JavascriptSamples.7z`,
    `PythonSamples.7z`, and `PowerShellSamples.7z` archives, in case you would like
    to supplement your own dataset. Extract these into separate folders for the following
    recipe.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还为您提供了`JavascriptSamples.7z`、`PythonSamples.7z`和`PowerShellSamples.7z`压缩包中每种文件类型的样本，以防您希望补充自己的数据集。将其解压到不同的文件夹中，以便按照以下步骤操作。
- en: How to do it...
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'The code for the following can be found on [https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/blob/master/Chapter02/Classifying%20Files%20by%20Type/File%20Type%20Classifier.ipynb](https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/blob/master/Chapter02/Classifying%20Files%20by%20Type/File%20Type%20Classifier.ipynb).
    We build a classifier using this data to predict files as JavaScript, Python,
    or PowerShell:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码可在[https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/blob/master/Chapter02/Classifying%20Files%20by%20Type/File%20Type%20Classifier.ipynb](https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/blob/master/Chapter02/Classifying%20Files%20by%20Type/File%20Type%20Classifier.ipynb)找到。我们利用这些数据构建分类器，以预测文件为JavaScript、Python或PowerShell：
- en: 'Begin by importing the necessary libraries and specifying the paths of the
    samples we will be using to train and test:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先导入必要的库并指定我们将用于训练和测试的样本的路径：
- en: '[PRE38]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Next, we read in all of the file types. We also create an array of labels with
    -1, 0, and 1 representing the JavaScript, Python, and PowerShell scripts, respectively:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们读取所有文件类型。同时，我们创建一个标签数组，分别代表JavaScript、Python和PowerShell脚本，分别为-1、0和1：
- en: '[PRE39]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We go on to create a train-test split and a pipeline that will perform basic
    NLP on the files, followed by a random forest classifier:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们继续创建训练-测试分割和管道，该管道将对文件执行基本的自然语言处理，然后使用随机森林分类器：
- en: '[PRE40]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We fit the pipeline to the training data, and then use it to predict on the
    testing data. Finally, we print out the accuracy and the confusion matrix:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将管道拟合到训练数据上，然后在测试数据上进行预测。最后，我们打印出准确度和混淆矩阵：
- en: '[PRE41]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This results in the following output:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '![](assets/cb75e173-58ff-41db-a345-5959d5436b5d.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/cb75e173-58ff-41db-a345-5959d5436b5d.png)'
- en: How it works...
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: Leveraging the dataset we built up in the *Scraping GitHub for files of a specific
    type* recipe, we place files in different directories, based on their file type,
    and then specify the paths in preparation for building our classifier (step 1).
    The code for this recipe assumes that the `"JavascriptSamples"` directory and
    others contain the samples, and have no subdirectories. We read in all files into
    a corpus, and record their labels (step 2). We train-test split the data and prepare
    a pipeline that will perform basic NLP on the files, followed by a random forest
    classifier (step 3). The choice of classifier here is meant for illustrative purposes,
    rather than to imply a best choice of classifier for this type of data. Finally,
    we perform the basic, but important, steps in the process of creating a machine
    learning classifier, consisting of fitting the pipeline to the training data and
    then assessing its performance on the testing set by measuring its accuracy and
    confusion matrix (step 4).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 利用我们在*从GitHub抓取特定类型文件*配方中建立的数据集，我们将文件放置在不同的目录中，根据其文件类型，并指定路径以准备构建我们的分类器（步骤1）。本配方的代码假设`"JavascriptSamples"`目录和其他目录包含样本，并且没有子目录。我们将所有文件读入一个语料库，并记录它们的标签（步骤2）。我们对数据进行训练-测试分割，并准备一个管道，该管道将对文件执行基本的自然语言处理，然后使用随机森林分类器（步骤3）。这里选择的分类器是为了说明目的，而不是为了暗示对于这类数据的最佳分类器选择。最后，我们执行创建机器学习分类器过程中的基本但重要的步骤，包括将管道拟合到训练数据上，然后通过测量其在测试集上的准确性和混淆矩阵来评估其性能（步骤4）。
- en: Measuring the similarity between two strings
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量两个字符串之间的相似度
- en: To check whether two files are identical, we utilize standard cryptographic
    hash functions, such as SHA256 and MD5\. However, at times, we would like to also
    know to what extent two files are similar. For that purpose, we utilize similarity
    hashing algorithms. The one we will be demonstrating here is `ssdeep`.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查两个文件是否相同，我们使用标准的加密哈希函数，例如SHA256和MD5。然而，有时我们也想知道两个文件在多大程度上相似。为此，我们使用相似性哈希算法。在这里我们将演示的是`ssdeep`。
- en: First, let's see how to use `ssdeep` to compare two strings. This can be useful
    to detect tampering in a text or script and also plagiarism.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看如何使用`ssdeep`比较两个字符串。这对于检测文本或脚本中的篡改以及抄袭非常有用。
- en: Getting ready
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Preparation for this recipe consists of installing the `ssdeep` package in `pip`.
    The installation is a little tricky and does not always work on Windows. Instructions
    can be found at [https://python-ssdeep.readthedocs.io/en/latest/installation.html.](https://python-ssdeep.readthedocs.io/en/latest/installation.html)
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱的准备工作包括在`pip`中安装`ssdeep`包。安装过程稍微复杂，并且在Windows上并不总是能成功。安装说明可以在[https://python-ssdeep.readthedocs.io/en/latest/installation.html](https://python-ssdeep.readthedocs.io/en/latest/installation.html)找到。
- en: 'If you only have a Windows machine and installing `ssdeep` does not work, then
    one possible solution is to run `ssdeep` on an Ubuntu VM, and then install it
    in `pip`, using the following command:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只有一台Windows机器，并且安装`ssdeep`没有成功，那么一种可能的解决方法是通过运行`ssdeep`在Ubuntu虚拟机上，然后在`pip`中安装它，使用以下命令：
- en: '[PRE42]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: How to do it...
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Begin by importing the `ssdeep` library and creating three strings:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先导入`ssdeep`库并创建三个字符串：
- en: '[PRE43]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Hash the strings:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对字符串进行哈希：
- en: '[PRE44]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: As a reference,
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 作为参考，
- en: hash1 is `u'3:f4oo8MRwRJFGW1gC6uWv6MQ2MFSl+JuBF8BSnJi:f4kPvtHMCMubyFtQ'`,
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: hash1是`u'3:f4oo8MRwRJFGW1gC6uWv6MQ2MFSl+JuBF8BSnJi:f4kPvtHMCMubyFtQ'`，
- en: hash2 is `u'3:f4oo8MRwRJFGW1gC6uWv6MQ2MFSl+JuBF8BS+EFECJi:f4kPvtHMCMubyFIsJQ'`,
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: hash2是`u'3:f4oo8MRwRJFGW1gC6uWv6MQ2MFSl+JuBF8BS+EFECJi:f4kPvtHMCMubyFIsJQ'`，
- en: hash3 is `u'3:f4oo8MRwRJFGW1gC6uWv6MQ2MFSl+JuBF8BS6:f4kPvtHMCMubyF0'`, and
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: hash3是`u'3:f4oo8MRwRJFGW1gC6uWv6MQ2MFSl+JuBF8BS6:f4kPvtHMCMubyF0`，并且
- en: hash4 is `u'3:60QKZ+4CDTfDaRFKYLVL:ywKDC2mVL'`.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: hash4是`u'3:60QKZ+4CDTfDaRFKYLVL:ywKDC2mVL'`。
- en: 'Next, we see what kind of similarity scores the strings have:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们查看这些字符串的相似度分数：
- en: '[PRE45]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The numerical results are as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 数值结果如下：
- en: '[PRE46]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: How it works...
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The basic idea behind `ssdeep` is to combine a number of traditional hashes
    whose boundaries are determined by the context of the input. This collection of
    hashes can then be used to identify modified versions of known files even when
    they have been modified by insertion, modification, or deletion.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '`ssdeep`的基本思路是结合多个传统哈希，这些哈希的边界由输入的上下文决定。这些哈希集合可以用来识别已知文件的修改版本，即使它们已经通过插入、修改或删除进行了更改。'
- en: For our recipe, we began by creating a set of four test strings meant as a toy
    example to illustrate how changes in a string will affect its similarity measures
    (step 1). The first, `str1`, is simply the first sentence of Lorem Ipsum. The
    second string, `str2`, differs in the capitalization of `m` in magna. The third
    string, `str3`, is missing the word magna altogether. Finally, the fourth string
    is an entirely different string. Our next step, step 2, is to hash the strings
    using the similarity hashing `ssdeep` library. Observe that similar strings have
    visibly similar similarity hashes. This should be contrasted with traditional
    hashes, in which even a small alteration produces a completely different hash.
    Next, we derive the similarity score between the various strings using `ssdeep`
    (step 3). In particular, observe that the `ssdeep` similarity score between two
    strings is an integer ranging between 0 and 100, with 100 being identical and
    0 being dissimilar. Two identical strings will have a similarity score of 100\.
    Changing the case of one letter in our string lowered the similarity score significantly
    to 39 because the strings are relatively short. Removing a word lowered it to
    37\. And two completely different strings had a similarity of 0.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们首先创建了四个测试字符串，作为一个玩具示例，来说明字符串的变化如何影响其相似度度量（步骤1）。第一个字符串`str1`仅仅是Lorem
    Ipsum的第一句。第二个字符串`str2`在“magna”中的`m`字母大小写上有所不同。第三个字符串`str3`完全缺少了“magna”这个词。最后，第四个字符串是完全不同的字符串。我们的下一步，步骤2，是使用相似度哈希库`ssdeep`对这些字符串进行哈希处理。请注意，相似的字符串具有明显相似的相似度哈希。这与传统哈希形成鲜明对比，在传统哈希中，即使是一个小小的修改也会产生完全不同的哈希。接下来，我们通过`ssdeep`（步骤3）得出这些字符串之间的相似度分数。特别注意，`ssdeep`的相似度分数是一个介于0和100之间的整数，100表示完全相同，0表示完全不同。两个完全相同的字符串的相似度分数是100。改变一个字母的大小写会显著降低相似度分数至39，因为字符串相对较短。删除一个单词将相似度分数降低到37。两个完全不同的字符串的相似度为0。
- en: Although other, in some cases better, fuzzy hashes are available, `ssdeep` is
    still a primary choice because of its speed and being a de facto standard.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然还有其他一些在某些情况下更好的模糊哈希可供选择，但由于`ssdeep`的速度和它作为事实标准的地位，它仍然是首选。
- en: Measuring the similarity between two files
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量两个文件之间的相似度
- en: Now, we are going to see how to apply `ssdeep` to measure the similarity between
    two binary files. The applications of this concept are many, but one in particular
    is using the similarity measure as a distance in clustering.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将了解如何应用`ssdeep`来衡量两个二进制文件之间的相似性。这个概念有很多应用，尤其是在聚类中将相似性度量作为距离。
- en: Getting ready
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Preparation for this recipe consists of installing the `ssdeep` package in `pip`.
    The installation is a little tricky and does not always work on Windows. Instructions
    can be found at [https://python-ssdeep.readthedocs.io/en/latest/installation.html](https://python-ssdeep.readthedocs.io/en/latest/installation.html).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的准备工作包括通过`pip`安装`ssdeep`包。安装过程有点复杂，且在Windows上并不总是有效。可以参考[https://python-ssdeep.readthedocs.io/en/latest/installation.html](https://python-ssdeep.readthedocs.io/en/latest/installation.html)上的说明。
- en: 'If you only have a Windows machine and it does not work, then one possible
    solution is to run `ssdeep` on an Ubuntu VM by installing `pip` with this command:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只有一台Windows机器且无法运行，那么一个可能的解决方案是在Ubuntu虚拟机上通过安装`pip`并使用以下命令运行`ssdeep`：
- en: '[PRE47]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In addition, download a test file such as the Python executable from [https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe](https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请从[https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe](https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe)下载测试文件，如Python可执行文件。
- en: How to do it...
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In the following recipe, we tamper with a binary file. We then compare it to
    the original to see that `ssdeep` determines that the two files are highly similar
    but not identical:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们篡改一个二进制文件。然后我们将其与原文件进行比较，发现`ssdeep`认为这两个文件高度相似但并不完全相同：
- en: 'First, we download the latest version of Python, `python-3.7.2-amd64.exe`.
    I am going to create a copy, rename it `python-3.7.2-amd64-fake.exe`, and add
    a null byte at the end:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们下载Python的最新版本，`python-3.7.2-amd64.exe`。我将创建一个副本，将其重命名为`python-3.7.2-amd64-fake.exe`，并在末尾添加一个空字节：
- en: '[PRE48]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Using `hexdump`, I can verify that the operation was successful by looking
    at the file before and after:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`hexdump`，我可以通过查看操作前后的文件来验证操作是否成功：
- en: '[PRE49]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This results in the following output:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE50]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The same can be verified with a second file using the following command:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用以下命令通过第二个文件进行验证：
- en: '[PRE51]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'This results in the following output:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE52]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now, I will hash the two files using `ssdeep` and compare the result:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我将使用`ssdeep`对两个文件进行哈希，并比较结果：
- en: '[PRE53]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The output to the preceding code is `99`.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出是`99`。
- en: How it works...
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This scenario simulates tampering with a file and then utilizing similarity
    hashing to detect the existence of tampering, as well as measuring the size of
    the delta. We begin with a vanilla Python executable and then tamper with it by
    adding a null byte at the end (step 1). In real life, a hacker may take a legitimate
    program and insert malicious code into the sample. We double-checked that the
    tempering was successful and examined its nature using a `hexdump` in step 2\.
    We then ran a similarity computation using similarity hashing on the original
    and tempered file, to observe that a minor alteration took place (step 3). Utilizing
    only standard hashing, we would have no idea how the two files are related, other
    than to conclude that they are not the same file. Knowing how to compare files
    allows us to cluster malware and benign files in machine learning algorithms,
    as well as group them into families.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这个场景模拟了篡改文件后，利用相似性哈希检测篡改的存在，并衡量差异的大小。我们从一个标准的Python可执行文件开始，然后通过在末尾添加一个空字节进行篡改（步骤1）。在现实中，黑客可能会在合法程序中插入恶意代码。我们通过`hexdump`双重检查篡改是否成功，并检查篡改的性质（步骤2）。然后，我们对原始文件和篡改后的文件进行相似性计算，观察到发生了微小的变化（步骤3）。仅使用标准哈希，我们将无法知道这两个文件之间的关系，除了得出它们不是同一个文件的结论。知道如何比较文件使我们能够在机器学习算法中对恶意软件和良性文件进行聚类，并将它们分组为家族。
- en: Extracting N-grams
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取N-gram
- en: In standard quantitative analysis of text, N-grams are sequences of N tokens
    (for example, words or characters). For instance, given the text *The quick brown
    fox jumped over the lazy dog,* if our tokens are words, then the 1-grams are *the*,
    *quick*, *brown*, *fox*, *jumped*, *over*, *the*, *lazy*, and *dog*. The 2-grams
    are *the quick*, *quick brown*, *brown fox*, and so on. The 3-grams are *the quick
    brown*, *quick brown fox*, *brown fox jumped*, and so on. Just like the local
    statistics of the text allowed us to build a Markov chain to perform statistical
    predictions and text generation from a corpus, N-grams allow us to model the local
    statistical properties of our corpus. Our ultimate goal is to utilize the counts
    of N-grams to help us predict whether a sample is malicious or benign. In this
    recipe, we demonstrate how to extract N-gram counts from a sample.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本的标准定量分析中，N-gram是由N个标记（例如，单词或字符）组成的序列。例如，给定文本*The quick brown fox jumped over
    the lazy dog*，如果我们的标记是单词，则1-gram是*the*，*quick*，*brown*，*fox*，*jumped*，*over*，*the*，*lazy*和*dog*。2-gram是*the
    quick*，*quick brown*，*brown fox*，依此类推。3-gram是*the quick brown*，*quick brown fox*，*brown
    fox jumped*，依此类推。就像文本的局部统计信息使我们能够构建马尔可夫链以进行统计预测和从语料库生成文本一样，N-gram使我们能够建模语料库的局部统计特性。我们的最终目标是利用N-gram的计数帮助我们预测样本是恶意的还是良性的。在本食谱中，我们演示了如何从样本中提取N-gram计数。
- en: Getting ready
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Preparation for this recipe consists of installing the `nltk` package in `pip`.
    The instructions are as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱的准备工作包括在`pip`中安装`nltk`包，安装说明如下：
- en: '[PRE54]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: In addition, download a test file, such as the Python executable from [https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe](https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，下载一个测试文件，例如来自[https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe](https://www.python.org/ftp/python/3.7.2/python-3.7.2-amd64.exe)的Python可执行文件。
- en: How to do it...
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'In the following steps, we will enumerate all the 4-grams of a sample file
    and select the 50 most frequent ones:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将枚举一个示例文件的所有4-gram，并选择其中50个最频繁的：
- en: 'We begin by importing the `collections` library to facilitate counting and
    the `ngrams` library from `nltk` to ease extraction of N-grams:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入`collections`库以方便计数，并从`nltk`库导入`ngrams`库以简化N-gram的提取：
- en: '[PRE55]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We specify which file we would like to analyze:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们指定要分析的文件：
- en: '[PRE56]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We define a convenience function to read in a file''s bytes:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了一个便捷函数来读取文件的字节：
- en: '[PRE57]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We write a convenience function to take a byte sequence and obtain N-grams:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们编写一个便捷函数来处理字节序列并获取N-gram：
- en: '[PRE58]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We write a function to take a file and obtain its count of N-grams:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们编写一个函数来处理文件并获取其N-gram的计数：
- en: '[PRE59]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We specify that our desired value is N=4 and obtain the counts of all 4-grams
    in the file:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们指定所需的值为N=4，并获取文件中所有4-gram的计数：
- en: '[PRE60]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We list the 10 most common 4-grams of our file:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们列出了文件中最常见的10个4-gram：
- en: '[PRE61]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The result is as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE62]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: How it works...
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In the literature and industry, it has been determined that the most frequent
    N-grams are also the most informative ones for a malware classification algorithm.
    For this reason, in this recipe, we will write functions to extract them for a
    file. We start by importing some helpful libraries for our extraction of N-grams
    (step 1). In particular, we import the collections library and the `ngrams` library
    from `nltk`. The collections library allows us to convert a list of N-grams to
    a frequency count of the N-grams, while the `ngrams` library allows us to take
    an ordered list of bytes and obtain a list of N-grams. We specify the file we
    would like to analyze and write a function that will read all of the bytes of
    a given file (steps 2 and 3). We define a few more convenience functions before
    we begin the extraction. In particular, we write a function to take a file's sequence
    of bytes and output a list of its N-grams (step 4), and a function to take a file
    and output the counts of its N-grams (step 5). We are now ready to pass in a file
    and extracts its N-grams. We do so to extract the counts of 4-grams of our file
    (step 6) and then display the 10 most common of them, along with their counts
    (step 7). We see that some of the N-gram sequences, such as (0,0,0,0) and (255,255,255,255)
    may not be very informative. For this reason, we will utilize feature selection
    methods to cut out the less informative N-grams in our next recipe.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献和工业界中已经确定，最常见的N-gram也是恶意软件分类算法中最具信息量的。因此，在本教程中，我们将编写函数来提取文件的N-gram。我们首先导入一些有助于提取N-gram的库（第1步）。特别地，我们导入`collections`库和`nltk`中的`ngrams`库。`collections`库允许我们将N-gram列表转换为N-gram的频次计数，而`ngrams`库则允许我们获取有序字节列表并得到N-gram列表。我们指定要分析的文件，并编写一个函数来读取给定文件的所有字节（第2步和第3步）。在开始提取之前，我们定义几个便利函数。特别地，我们编写一个函数来获取文件的字节序列并输出其N-gram列表（第4步），并编写一个函数来获取文件并输出其N-gram计数（第5步）。现在我们准备传入文件并提取其N-gram。我们这样做以提取文件的4-gram计数（第6步），然后展示其中最常见的10个及其计数（第7步）。我们看到一些N-gram序列，如(0,0,0,0)和(255,255,255,255)，可能不太有信息量。因此，我们将在下一个教程中利用特征选择方法去除这些不太有信息量的N-gram。
- en: Selecting the best N-grams
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择最佳N-gram
- en: The number of different N-grams grows exponentially in N. Even for a fixed tiny
    N, such as N=3, there are *256x256x256=16,777,216* possible N-grams. This means
    that the number of N-grams features is impracticably large. Consequently, we must
    select a smaller subset of N-grams that will be of most value to our classifiers.
    In this section, we show three different methods for selecting the topmost informative
    N-grams.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 不同N-gram的数量随着N的增大呈指数增长。即使对于一个固定的小N，如N=3，也有*256x256x256=16,777,216*种可能的N-gram。这意味着N-gram特征的数量巨大，几乎不可能实际使用。因此，我们必须选择一个较小的N-gram子集，这些子集将对我们的分类器最有价值。在这一部分中，我们展示了三种选择最具信息量N-gram的不同方法。
- en: Getting ready
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Preparation for this recipe consists of installing the `scikit-learn` and `nltk`
    packages in `pip`. The instructions are as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程的准备工作包括在`pip`中安装`scikit-learn`和`nltk`包。安装说明如下：
- en: '[PRE63]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: In addition, benign and malicious files have been provided for you in the `PE
    Samples Dataset` folder in the root of the repository. Extract all archives named
    `Benign PE Samples*.7z` to a folder named `Benign PE Samples`. Extract all archives
    named `Malicious PE Samples*.7z` to a folder named `Malicious PE Samples`.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，善意和恶意文件已在仓库根目录下的`PE Samples Dataset`文件夹中提供。将所有名为`Benign PE Samples*.7z`的压缩包解压到名为`Benign
    PE Samples`的文件夹中。将所有名为`Malicious PE Samples*.7z`的压缩包解压到名为`Malicious PE Samples`的文件夹中。
- en: How to do it...
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In the following steps, we show three different methods for selecting the most
    informative N-grams. The recipe assumes that `binaryFileToNgramCounts(file, N)`
    and all other helper functions from the previous recipe have been included:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们展示了三种选择最具信息量N-gram的方法。本教程假设已包括前一个教程中的`binaryFileToNgramCounts(file,
    N)`和所有其他辅助函数：
- en: 'Begin by specifying the folders containing our samples, specifying our `N`,
    and importing modules to enumerate files:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先指定包含我们样本的文件夹，指定我们的`N`，并导入模块以枚举文件：
- en: '[PRE64]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Next, we count all the N-grams from all the files:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们从所有文件中统计所有的N-gram：
- en: '[PRE65]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We collect the `K1=1000` most frequent N-grams into a list:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`K1=1000`个最常见的N-gram收集到一个列表中：
- en: '[PRE66]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'A helper method, `featurize_sample`, will be used to take a sample and output
    the number of appearances of the most common N-grams in its byte sequence:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个辅助方法`featurize_sample`将用于获取一个样本并输出其字节序列中最常见N-gram的出现次数：
- en: '[PRE67]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'We iterate through our directories, and use the preceding `featurize_sample`
    function to featurize our samples. We also create a set of labels:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们遍历目录，并使用前面的 `featurize_sample` 函数来特征化我们的样本。同时，我们创建一组标签：
- en: '[PRE68]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We import the libraries we will be using for feature selection and specify
    how many features we would like to narrow down to:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入将用于特征选择的库，并指定希望缩小到多少个特征：
- en: '[PRE69]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We perform three types of feature selections for our N-grams:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对 N-gram 进行三种类型的特征选择：
- en: '**Frequency**—selects the most frequent N-grams:'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**频率**—选择最常见的 N-gram：'
- en: '[PRE70]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '**Mutual** **information**—selects the N-grams ranked highest by the mutual
    information algorithm:'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**互信息**—通过互信息算法选择排名最高的 N-gram：'
- en: '[PRE71]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '**Chi-squared**—selects the N-grams ranked highest by the chi squared algorithm:'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卡方**—通过卡方算法选择排名最高的 N-gram：'
- en: '[PRE72]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: How it works…
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: Unlike the previous recipe, in which we analyzed a single file's N-grams, in
    this recipe, we look at a large collection of files to understand which N-grams
    are the most informative features. We start by specifying the folders containing
    our samples, our value of N, and import some modules to enumerate files (step
    1). We proceed to count *all* N-grams from *all* files in our dataset (step 2).
    This allows us to find the *globally* most frequent N-grams. Of these, we filter
    down to the `K1=1000` most frequent ones (step 3). Next, we introduce a helper
    method, `featurizeSample`, to be used to take a sample and output the number of
    appearances of the K1 most common N-grams in its byte sequence (step 4). We then
    iterate through our directories of files, and use the previous `featurizeSample`
    function to featurize our samples, as well as record their labels, as malicious
    or benign (step 5). The importance of the labels is that the assessment of whether
    an N-gram is informative depends on being able to discriminate between the malicious
    and benign classes based on it.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的方案不同，在那里我们分析了单个文件的 N-gram，而在这个方案中，我们会查看大量文件，以了解哪些 N-gram 是最具信息量的特征。我们首先指定包含样本的文件夹，N
    的值，并导入一些模块来列举文件（步骤 1）。接下来，我们统计数据集中的*所有*文件的所有 N-gram（步骤 2）。这使我们能够找到*全局*最常见的 N-gram。在这些
    N-gram 中，我们筛选出 `K1=1000` 个最常见的（步骤 3）。然后，我们引入一个辅助方法 `featurizeSample`，用于提取样本并输出其字节序列中
    K1 个最常见 N-gram 的出现次数（步骤 4）。接下来，我们遍历文件目录，并使用之前的 `featurizeSample` 函数来特征化样本，同时记录它们的标签，标记为恶意或良性（步骤
    5）。标签的重要性在于，评估某个 N-gram 是否具有信息量，取决于能否基于其区分恶意和良性类别。
- en: We import the `SelectKBest` library to select the best features via a score
    function, and the two score functions, mutual information and chi-squared (step
    6). Finally, we apply the three different feature selection schemes to select
    the best N-grams and apply this knowledge to transform our features (step 7).
    In the first method, we simply select the K2 most frequent N-grams. Note that
    the selection of this method is often recommended in the literature, and is easier
    because of not requiring labels or extensive computation. In the second method,
    we use mutual information to narrow down the K2 features, while in the third,
    we use chi-squared to do so.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入 `SelectKBest` 库，通过评分函数选择最佳特征，以及两种评分函数：互信息和卡方（步骤 6）。最后，我们应用三种不同的特征选择方案来选择最佳的
    N-gram，并将这些知识应用于转换我们的特征（步骤 7）。在第一种方法中，我们简单地选择 K2 个最常见的 N-gram。请注意，这种选择方法在文献中经常推荐，因为它不需要标签或复杂的计算，较为简单。在第二种方法中，我们使用互信息来缩小
    K2 个特征，而在第三种方法中，我们使用卡方来进行选择。
- en: Building a static malware detector
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建静态恶意软件检测器
- en: In this section, we will see how to put together the recipes we discussed in
    prior sections to build a malware detector. Our malware detector will take in
    both features extracted from the PE header as well as features derived from N-grams.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到如何将之前讨论的方案组合起来，构建一个恶意软件检测器。我们的恶意软件检测器将同时采用从 PE 头部提取的特征以及从 N-gram 派生的特征。
- en: Getting ready
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Preparation for this recipe consists of installing the `scikit-learn`, `nltk`,
    and `pefile` packages in `pip`. The instructions are as follows:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 本方案的准备工作包括在 `pip` 中安装 `scikit-learn`、`nltk` 和 `pefile` 包。安装说明如下：
- en: '[PRE73]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: In addition, benign and malicious files have been provided for you in the `"PE
    Samples Dataset"` folder in the root of the repository. Extract all archives named
    `"Benign PE Samples*.7z"` to a folder named `"Benign PE Samples".` Extract all
    archives named `"Malicious PE Samples*.7z"` to a folder named `"Malicious PE Samples"`.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，在存储库根目录的`"PE Samples Dataset"`文件夹中已为您提供了良性和恶意文件。请将名为`"Benign PE Samples*.7z"`的所有存档解压到名为`"Benign
    PE Samples"`的文件夹中。将名为`"Malicious PE Samples*.7z"`的所有存档解压到名为`"Malicious PE Samples"`的文件夹中。
- en: How to do it...
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'In the following steps, we will demonstrate a complete workflow in which we
    begin with raw samples, featurize them, vectorize their results, put them together,
    and finally train and test a classifier:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将展示一个完整的工作流程，我们将从原始样本开始，对其进行特征提取，将结果向量化，将它们组合在一起，最后训练和测试分类器：
- en: 'Begin by enumerating our samples and assigning their labels:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，列举我们的样本并分配它们的标签：
- en: '[PRE74]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'We perform a stratified train-test split:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们执行分层的训练测试分离：
- en: '[PRE75]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'We introduce convenience functions from prior sections in order to obtain features:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们引入之前章节中的便捷函数，以获取特征：
- en: '[PRE76]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'We select the 100 most frequent 2-grams as our features:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们选择前100个最常见的二元组作为我们的特征：
- en: '[PRE77]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'We extract the N-gram counts, section names, imports, and number of sections
    of each sample in our training test, and skip over samples whose PE header cannot
    be parsed:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们提取每个样本中的N-gram计数、节名称、导入以及节的数量，跳过无法解析PE头部的样本：
- en: '[PRE78]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'We use a hashing vectorizer followed by `tfidf` to convert the imports and
    section names, both of which are text features, into a numerical form:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用哈希向量化器，然后使用`tfidf`将导入和节名称（均为文本特征）转换为数值形式：
- en: '[PRE79]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'We combine the vectorized features into a single array:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将向量化的特征合并为单个数组：
- en: '[PRE80]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'We train a Random Forest classifier on the training set and print out its score:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在训练集上训练了一个随机森林分类器，并打印出其得分：
- en: '[PRE81]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'We collect the features of the testing set, just as we did for the training
    set:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们收集测试集的特征，就像我们对训练集所做的一样：
- en: '[PRE82]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'We apply the previously trained transformers to vectorize the text features
    and then test our classifier on the resulting test set:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将先前训练过的转换器应用于向量化文本特征，然后在生成的测试集上测试我们的分类器：
- en: '[PRE83]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'The score of our classifier is as follows:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分类器得分如下：
- en: '[PRE84]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: How it works…
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: There are several notable new ideas in this section. We start by enumerating
    our samples and assigning them their respective labels (step 1). Because our dataset
    is imbalanced, it makes sense to use a stratified train-test split (step 2). In
    a stratified train-test split, a train-test split is created in which the proportion
    of each class is the same in the training set, testing set, and original set.
    This ensures that there is no possibility that our training set, for example,
    will consist of only one class due to a chance event. Next, we load the functions
    we will be using to featurize our samples. We employ our feature extraction techniques,
    as in previous recipes, to compute the best N-gram features (step 4) and then
    iterate through all of the files to extract all of the features (step 5). We then
    take the PE header features we obtained previously, such as section names and
    imports, and vectorize them using a basic NLP approach (step 6).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中有几个值得注意的新想法。我们首先列举我们的样本并为它们分配其相应的标签（步骤1）。由于我们的数据集不平衡，使用分层的训练测试分离是合理的（步骤2）。在分层的训练测试分离中，创建一个训练集和测试集，其中每个类的比例与原始集中的比例相同。这确保了训练集不会由于偶然事件而只包含一个类。接下来，我们加载将用于对样本进行特征提取的函数。我们像以前的方法一样使用我们的特征提取技术来计算最佳的N-gram特征（步骤4），然后遍历所有文件以提取所有特征（步骤5）。然后，我们使用基本的自然语言处理方法对先前获取的PE头部特征，如节名称和导入，进行向量化（步骤6）。
- en: Having obtained all these different features, we are now ready to combine them,
    which we do using the `scipy` hstack, to merge the different features into one
    large sparse `scipy` array (step 7). We continue on to train a Random Forest classifier
    with default parameters (step 8) and then repeat the extraction process for our
    testing set (step 9). In step 10, we finally test out our trained classifier,
    obtaining a promising starting score. Overall, this recipe provides the foundations
    for a malware classifier that can be expanded into a high-powered solution.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 获得了所有这些不同的特征后，我们现在可以将它们合并在一起，这一步通过使用`scipy`的hstack来完成，将不同的特征合并为一个大的稀疏`scipy`数组（步骤7）。接下来，我们继续训练一个使用默认参数的随机森林分类器（步骤8），然后对我们的测试集重复提取过程（步骤9）。在步骤10中，我们最终测试我们的训练好的分类器，并获得一个有前景的起始分数。总的来说，这个配方为一个恶意软件分类器提供了基础，可以扩展成一个强大的解决方案。
- en: Tackling class imbalance
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决类别不平衡问题
- en: Often in applying machine learning to cybersecurity, we are faced with highly
    imbalanced datasets. For instance, it may be much easier to access a large collection
    of benign samples than it is to collect malicious samples. Conversely, you may
    be working at an enterprise that, for legal reasons, is prohibited from saving
    benign samples. In either case, your dataset will be highly skewed toward one
    class. As a consequence, naive machine learning aimed at maximizing accuracy will
    result in a classifier that predicts almost all samples as coming from the overrepresented
    class. There are several techniques that can be used to tackle the challenge of
    class imbalance.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在将机器学习应用于网络安全时，我们经常面对严重不平衡的数据集。例如，获取大量正常样本可能比收集恶意样本容易得多。反过来，你可能在一个因法律原因而禁止保存正常样本的企业工作。在这两种情况下，你的数据集都会严重偏向于某一类。因此，旨在最大化准确度的简单机器学习方法将导致一个几乎将所有样本预测为来自过度代表类的分类器。有几种技术可以用来解决类别不平衡的问题。
- en: Getting ready
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Preparation for this recipe consists of installing the `scikit-learn` and `imbalanced-learn`
    pip packages. The instructions are as follows:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 本配方的准备工作包括安装`scikit-learn`和`imbalanced-learn`的pip包。安装说明如下：
- en: '[PRE85]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: How to do it...
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In the following steps, we will demonstrate several methods for dealing with
    imbalanced data:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将演示几种处理不平衡数据的方法：
- en: 'Begin by loading the training and testing data, importing a decision tree,
    as well as some libraries we will be using to score performance:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先加载训练和测试数据，导入决策树，以及我们将用来评估性能的一些库：
- en: '[PRE86]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Train and test a simple Decision Tree classifier:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练和测试一个简单的决策树分类器：
- en: '[PRE87]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'This results in the following output:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE88]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Next, we test several techniques to improve performance.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们测试几种提高性能的技术。
- en: '**Weighting:** We set the class weights of our classifier to `"balanced"` and
    train and test this new classifier:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加权：**我们将分类器的类别权重设置为`"balanced"`，并训练和测试这个新分类器：'
- en: '[PRE89]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'This results in the following output:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE90]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '**Upsampling the minor class: **We extract all test samples from class 0 and
    class 1:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**对小类别进行上采样：**我们从类别0和类别1中提取所有测试样本：'
- en: '[PRE91]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'We upsample the elements of class 1 with replacements until the number of samples
    of class 1 and class 0 are equal:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对类别1的元素进行有放回的上采样，直到类别1和类别0的样本数量相等：
- en: '[PRE92]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'We combine the newly upsampled samples into a single training set:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将新上采样的样本合并成一个训练集：
- en: '[PRE93]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'We train and test a Random Forest classifier on our upsampled training set:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在上采样的训练集上训练并测试一个随机森林分类器：
- en: '[PRE94]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'This results in the following output:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE95]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '**Downsampling the major class: **We perform similar steps to the preceding
    upsampling, except this time we down-sample the major class until it is of the
    same size as the minor class:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**对大类别进行下采样：**我们执行与上采样相似的步骤，只不过这次我们对大类别进行下采样，直到它与小类别的样本数量相等：'
- en: '[PRE96]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'We create a new training set from the downsampled data:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从下采样数据中创建一个新的训练集：
- en: '[PRE97]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'We train a Random Forest classifier on this dataset:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在这个数据集上训练一个随机森林分类器：
- en: '[PRE98]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'This results in the following output:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE99]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '**Classifier including inner balancing samplers: **We utilize the imbalanced-learn
    package classifiers that resample subsets of data before the training estimators:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**包含内置平衡采样器的分类器：**我们使用`imbalanced-learn`包中的分类器，这些分类器在训练估计器之前会对数据子集进行重采样：'
- en: '[PRE100]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'This results in the following output:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE101]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: How it works…
  id: totrans-375
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现…
- en: We start by loading in a predefined dataset (step 1) using the  `scipy.sparse.load_npz`
    loading function to load previously saved sparse matrices. Our next step is to
    train a basic Decision Tree model on our data (step 2). To measure performance,
    we utilize the balanced accuracy score, a measure that is often used in classification
    problems with imbalanced datasets. By definition, balanced accuracy is the average
    of recall obtained on each class. The best value is 1, whereas the worst value
    is 0.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载一个预定义的数据集（第1步），使用`scipy.sparse.load_npz`加载函数来加载之前保存的稀疏矩阵。下一步是对我们的数据训练一个基本的决策树模型（第2步）。为了评估性能，我们使用平衡准确度分数，这是一种常用于处理不平衡数据集分类问题的衡量标准。根据定义，平衡准确度是对每个类别召回率的平均值。最好的值是1，而最差的值是0。
- en: In the following steps, we employ different techniques to tackle the class imbalance.
    Our first approach is to utilize class weights to adjust our Decision Tree to
    an imbalanced dataset (step 3). The balanced mode uses the values of *y* to automatically
    adjust weights inversely proportional to the class frequencies in the input data
    as *n_samples / (n_classes * np.bincount(y))*. In steps 4 to 7, we utilize upsampling
    to tackle class imbalance. This is the process of randomly duplicating observations
    from the minority class in order to reinforce the minority class's signal.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们采用不同的技术来解决类别不平衡问题。我们的第一个方法是利用类别权重来调整决策树，以适应不平衡的数据集（第3步）。平衡模式使用* y
    *的值自动调整权重，该权重与输入数据中类别的频率成反比，公式为*n_samples / (n_classes * np.bincount(y))*。在第4到第7步中，我们使用上采样来处理类别不平衡。这是通过随机复制少数类的观察值，以加强少数类的信号。
- en: There are several methods for doing so, but the most common way is to simply
    resample with replacements as we have done. The two main concerns with upsampling
    are that it increases the size of the dataset and that it can lead to overfitting
    due to training on the same sample numerous times. In steps 8 to 10, we down-sample
    our major class. This simply means that we don't use all of the samples we have,
    but just enough so that we balance our classes.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以做到这一点，但最常见的做法是像我们之前那样进行有放回的重采样。上采样的两个主要问题是，它会增加数据集的大小，并且由于多次在相同样本上训练，它可能导致过拟合。在第8到第10步中，我们进行了主要类别的下采样。这意味着我们不会使用所有样本，而是使用足够的样本来平衡类别。
- en: The main issue with this technique is that we are forced to use a smaller training
    set. Our final approach, and the most sophisticated one, is to utilize a classifier
    that includes inner balancing samplers, namely the `BalancedBaggingClassifier`
    from `imbalanced-learn` (step 11). Overall, we see that every single one of our
    methods for tackling class imbalance increased the balanced accuracy score.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技术的主要问题在于我们被迫使用一个较小的训练集。我们最终的方案，也是最复杂的方案，是使用一个包括内部平衡采样器的分类器，即来自`imbalanced-learn`的`BalancedBaggingClassifier`（第11步）。总体来看，我们发现每一种应对类别不平衡的方法都提高了平衡准确度分数。
- en: Handling type I and type II errors
  id: totrans-380
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理类型I和类型II错误
- en: In many situations in machine learning, one type of error may be more important
    than another. For example, in a multilayered defense system, it may make sense
    to require a layer to have a low false alarm (low false positive) rate, at the
    cost of some detection rate. In this section, we provide a recipe for ensuring
    that the FPR does not exceed a desired limit by using thresholding.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多机器学习的情境中，一种错误可能比另一种更重要。例如，在一个多层防御系统中，要求某一层具有低假警报（低假阳性）率可能是合理的，虽然这会牺牲一定的检测率。在这一部分中，我们提供了一种确保假阳性率（FPR）不超过预定限制的方法，具体通过使用阈值化来实现。
- en: Getting ready
  id: totrans-382
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Preparation for this recipe consists of installing `scikit-learn` and `xgboost`
    in `pip`. The instructions are as follows:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 准备这一方法需要在`pip`中安装`scikit-learn`和`xgboost`。安装说明如下：
- en: '[PRE102]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: How to do it...
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点……
- en: 'In the following steps, we will load a dataset, train a classifier, and then
    tune a threshold to satisfy a false positive rate constraint:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将加载一个数据集，训练一个分类器，然后调整阈值以满足假阳性率的约束：
- en: 'We load a dataset and specify that the desired FPR is at or below 1%:'
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载一个数据集并指定所需的假阳性率（FPR）为1%或更低：
- en: '[PRE103]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'We write methods to calculate  `FPR` and `TPR`:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们编写方法来计算`FPR`和`TPR`：
- en: '[PRE104]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'We write a method to convert a vector of probabilities into a Boolean vector
    using thresholding:'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们编写一个方法，通过阈值化将概率向量转换为布尔向量：
- en: '[PRE105]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'We train an XGBoost model and calculate a probability prediction on the training
    data:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们训练一个XGBoost模型并计算训练数据的概率预测：
- en: '[PRE106]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Let''s examine our prediction probability vectors:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查一下我们的预测概率向量：
- en: '[PRE107]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'This results in the following output:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE108]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'We loop over 1,000 different threshold values, calculate the FPR for each,
    and when we satisfy our `FPR<=desiredFPR`, we select that threshold:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们遍历1000个不同的阈值，计算每个阈值的FPR，当满足`FPR<=desiredFPR`时，我们选择那个阈值：
- en: '[PRE109]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'This results in the following output:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE110]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: How it works…
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理……
- en: 'We begin this recipe by loading in a previously featurized dataset and specifying
    a desired FPR constraint of 1% (step 1). The value to be used in practice depends
    highly on the situation and type of file being considered. There are a few considerations
    to follow: if the file is extremely common, but rarely malicious, such as a PDF,
    the desired FPR will have to be set very low, for example, 0.01%.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过加载一个先前已特征化的数据集并指定一个1%的期望FPR约束来开始这个过程（步骤1）。实际使用的值高度依赖于具体情况和所考虑的文件类型。这里有几点需要考虑：如果文件是极为常见，但很少是恶意的，例如PDF文件，那么期望的FPR必须设置得非常低，例如0.01%。
- en: If the system is supported by additional systems that will double-check its
    verdict without human effort, then a high FPR might not be detrimental. Finally,
    a customer may have a preference, which will suggest a recommended value. We define
    a pair of convenience functions for FPR and TPR in step 2—these functions are
    very handy and reusable. Another convenience function we define is a function
    that will take our threshold value and use it to threshold a numerical vector
    (step 3).
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系统得到了额外系统的支持，这些系统可以在没有人工干预的情况下再次验证其判定结果，那么较高的FPR可能不会有害。最后，客户可能会有一个偏好，这将建议一个推荐值。我们在步骤2中定义了一对便捷函数，用于计算FPR和TPR——这些函数非常实用且可重复使用。我们定义的另一个便捷函数是一个函数，它将接受我们的阈值，并用它对一个数值向量进行阈值处理（步骤3）。
- en: In step 4, we train a model on the training data, and determine prediction probabilities
    on the training set as well. You can see what these look like in step 5\. When
    a large dataset is available, using a validation set for determining the proper
    threshold will reduce the likelihood of overfitting. Finally, we compute the threshold
    to be used in future classification in order to ensure that the FPR constraint
    will be satisfied (step 6).
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤4中，我们在训练数据上训练模型，并在训练集上确定预测概率。你可以在步骤5中看到这些预测结果。当有大量数据集时，使用验证集来确定适当的阈值将减少过拟合的可能性。最后，我们计算将来分类时要使用的阈值，以确保满足FPR约束（步骤6）。
