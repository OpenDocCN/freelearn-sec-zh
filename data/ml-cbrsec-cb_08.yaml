- en: Secure and Private AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安全与私密的人工智能
- en: Machine learning can help us diagnose and fight cancer, decide which school
    is the best for our children and make the smartest real estate investment. But
    you can only answer these questions with access to private and personal data,
    which requires a novel approach to machine learning. This approach is called *Secure
    and Private AI* and, in recent years, has seen great strides, as you will see
    in the following recipes.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可以帮助我们诊断和对抗癌症，决定哪个学校最适合我们的孩子，甚至做出最聪明的房地产投资。但你只有在能访问私人和个人数据的情况下才能回答这些问题，这就需要一种全新的机器学习方法。这种方法叫做*安全与私密的人工智能*，近年来已经取得了显著进展，正如你在以下食谱中所看到的那样。
- en: 'This chapter contains the following recipes:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含以下食谱：
- en: Federated learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 联邦学习
- en: Encrypted computation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加密计算
- en: Private deep learning prediction
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 私有深度学习预测
- en: Testing the adversarial robustness of neural networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试神经网络的对抗鲁棒性
- en: Differential privacy using TensorFlow Privacy
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Privacy 的差分隐私
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following are the technical prerequisites for this chapter:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的技术前提如下：
- en: TensorFlow Federated
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Federated
- en: Foolbox
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foolbox
- en: PyTorch
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch
- en: Torchvision
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Torchvision
- en: TensorFlow Privacy
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Privacy
- en: Installation instructions, code, and datasets may be found at [https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter08](https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter08).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 安装说明、代码和数据集可以在[https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter08](https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter08)找到。
- en: Federated learning
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联邦学习
- en: In this recipe, we will train a federated learning model using the TensorFlow
    federated framework.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将使用 TensorFlow federated 框架训练一个联邦学习模型。
- en: To understand why federated learning is valuable, consider the *next word prediction*
    model on your mobile phone when you write an SMS message. For privacy reasons,
    you wouldn't want the data, that is, your text messages, to be sent to a central
    server to be used for training the next word predictor. But it's still nice to
    have an accurate next word prediction algorithm. What to do? This is where federated
    learning comes in, which is a machine learning technique developed to tackle such
    privacy concerns.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解联邦学习的价值，考虑你在手机上写短信时使用的*下一个词预测*模型。出于隐私原因，你不希望你的数据，也就是你的短信，发送到中央服务器上用于训练下一个词预测器。但你依然希望有一个准确的下一个词预测算法。该怎么办？这时，联邦学习就派上用场了，这是一种为解决此类隐私问题而开发的机器学习技术。
- en: The core idea in federated learning is that a training dataset remains in the
    hands of its producers, preserving privacy and ownership, while still being used
    to train a centralized model. This feature is especially attractive in cybersecurity,
    where, for example, collecting benign and malicious samples from many different
    sources is crucial to creating a strong model, but difficult on account of privacy
    concerns (by way of an example, a benign sample can be a personal or confidential
    document).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习的核心思想是训练数据集由数据的生产者保管，既能保持隐私和所有权，又能用于训练一个集中式模型。这一特点在网络安全领域尤其有吸引力，例如，从多个不同来源收集良性和恶性样本对于创建强大的模型至关重要，但由于隐私问题，这一过程相当困难（例如，良性样本可以是个人或机密文件）。
- en: In passing, we should mention the fact that federated learning has been gaining
    more and more traction due to the increasing importance of data privacy (for example,
    the enactment of GDPR). Large actors, such as Apple and Google, have started investing
    heavily in this technology.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，联邦学习因数据隐私重要性日益增加而越来越受到关注（例如，GDPR的实施）。大型公司，如苹果和谷歌，已经开始大量投资这一技术。
- en: Getting ready
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Preparation for this recipe involves installing the `tensorflow_federated`,
    `tensorflow_datasets`, and `tensorflow` packages in `pip`. The command is as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备本食谱，需通过 `pip` 安装 `tensorflow_federated`、`tensorflow_datasets` 和 `tensorflow`
    包。命令如下：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will install specific versions of these packages to prevent any breaks in
    the code.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将安装这些包的特定版本，以防止代码出现中断。
- en: How to do it…
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作…
- en: In the following steps, you will create two virtual dataset environments—one
    belonging to Alice, and one belonging to Bob—and use federated averaging to preserve
    data confidentiality.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，你将创建两个虚拟数据集环境——一个属于爱丽丝，另一个属于鲍勃——并使用联邦平均算法来保持数据的保密性。
- en: 'Import TensorFlow and enable eager execution:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 TensorFlow 并启用急切执行：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Prepare a dataset by importing Fashion MNIST and splitting it into two separate
    environments, Alice and Bob:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备一个数据集，通过导入 Fashion MNIST 并将其分成 Alice 和 Bob 两个独立环境：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, define a `helper` function to cast the data type from integer to float:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，定义一个`helper`函数将数据类型从整数转换为浮点数：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, define a `helper` function to flatten the data to be fed into a neural
    network:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，定义一个`helper`函数，将数据展平以便输入到神经网络中：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, define a `helper` function to pre-process the data:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，定义一个`helper`函数来预处理数据：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Pre-process the data:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理数据：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, define a `loss` function for our neural network:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，定义一个用于神经网络的`loss`函数：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Define a function to instantiate a simple Keras neural network:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来实例化一个简单的 Keras 神经网络：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, create a dummy batch of samples and define a function to return a federated
    learning model from the Keras model:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，创建一个虚拟样本批次并定义一个函数，从 Keras 模型返回一个联邦学习模型：
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Declare an iterative process of federated averaging, and run one stage of the
    computation:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 声明一个联邦平均的迭代过程，并运行一个计算阶段：
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, display the metrics of the computation by running the following command:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，通过运行以下命令显示计算的指标：
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: How it works...
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We begin by importing TensorFlow and enabling eager execution (*Step 1*). Ordinarily,
    in TensorFlow, operations are not performed immediately. Rather, a computation
    graph is built, and, at the very end, all operations are run together. In eager
    execution, computations are executed as soon as possible. Next, in *Step 2*, we
    import the Fashion MNIST dataset. This dataset has become a *de facto* replacement
    for MNIST, offering several improvements over it (such as added challenges). We
    then subdivide the dataset 50:50 between Alice and Bob. We then define a function
    to cast the pixel values of Fashion MNIST from integers to floats to be used in
    the training of our neural network (*Step 3*) and another function to flatten
    the images into a single vector (*Step 4*). This enables us to feed the data into
    a fully connected neural network. In *Steps 5* and *6*, we employ the previously
    defined convenience functions to pre-process Alice and Bob's datasets.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入 TensorFlow 并启用急切执行模式（*步骤 1*）。通常，在 TensorFlow 中，操作不会立即执行，而是构建一个计算图，并在最后一起执行所有操作。在急切执行模式下，计算会尽可能快地执行。接下来，在*步骤
    2*中，我们导入 Fashion MNIST 数据集。这个数据集已经成为 MNIST 的*事实*替代品，相比 MNIST 提供了几项改进（如增加了挑战）。然后，我们将数据集按
    50:50 的比例分配给 Alice 和 Bob。接下来，我们定义一个函数，将 Fashion MNIST 的像素值从整数转换为浮点数，以便用于训练神经网络（*步骤
    3*），并定义另一个函数将图像展平为单个向量（*步骤 4*）。这样，我们就可以将数据输入到全连接神经网络中。在*步骤 5*和*步骤 6*中，我们使用之前定义的便捷函数对
    Alice 和 Bob 的数据集进行预处理。
- en: Next, we define a loss function that makes sense for our 10-class classification
    task (*Step 7*), and then define our Keras neural network in preparation for training
    (*Step 8*). In *Step 9*, we create a dummy batch of samples and define a function
    to return a federated learning model from the Keras model. The dummy batch of
    samples specifies the shape of input for the model to expect. In *Step 10*, we
    run one stage of the federated averaging process. Details regarding the algorithm
    can be found in the paper entitled *Communication-Efficient Learning of Deep Networks
    from Decentralized Data*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个适合我们 10 类分类任务的损失函数（*步骤 7*），然后定义我们的 Keras 神经网络以准备训练（*步骤 8*）。在*步骤 9*中，我们创建一个虚拟样本批次并定义一个函数，从
    Keras 模型返回一个联邦学习模型。虚拟样本批次指定了模型预期的输入形状。在*步骤 10*中，我们运行一个联邦平均过程的阶段。关于该算法的详细信息，请参阅题为《*从分散数据中高效学习深度网络的通信*》的论文。
- en: At a basic level, the algorithm combines local **stochastic gradient descent**
    (**SGD**) on the data of each client, and then uses a server that performs model
    averaging. The result is conserved confidentiality for the clients (in our case,
    Alice and Bob). Finally, in *Step 11*, we observe our performance, seeing that
    the algorithm indeed does train and improve accuracy, as intended.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 从基本层面来看，算法结合了每个客户端数据的局部**随机梯度下降**（**SGD**），然后使用一个执行模型平均的服务器。结果是为客户端（在我们的例子中是
    Alice 和 Bob）保留了保密性。最后，在*步骤 11*中，我们观察性能，发现算法确实能够训练并提高准确率，如预期那样。
- en: Encrypted computation
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加密计算
- en: In this recipe, we're going to walk through the basics of encrypted computation.
    In particular, we're going to focus on one popular approach, called Secure Multi-Party
    Computation. You'll learn how to build a simple encrypted calculator that can
    perform addition on encrypted numbers. The ideas in this recipe will come in handy
    in the *Private deep learning prediction* recipe.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将介绍加密计算的基础知识。特别地，我们将重点讲解一种流行的方法，称为安全多方计算。你将学习如何构建一个简单的加密计算器，可以对加密数字进行加法操作。这个配方中的理念将在*私有深度学习预测*配方中派上用场。
- en: Getting ready
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: The following recipe has no installation requirements other than Python.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下配方除了Python外没有其他安装要求。
- en: How to do it…
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Import the random library and select a large prime number, `P`:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入随机库并选择一个大素数`P`：
- en: '[PRE13]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Define an encryption function for three parties:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个用于三方的加密函数：
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Encrypt a numerical variable:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对一个数值变量进行加密：
- en: '[PRE15]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Define a function to decrypt, given the three shares:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来解密，给定三个分享份额：
- en: '[PRE16]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Decrypt the encrypted variable `x`:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解密加密变量`x`：
- en: '[PRE17]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Define a function to add two encrypted numbers:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来对两个加密数字进行加法操作：
- en: '[PRE19]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Add two encrypted variables and decrypt their sum:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对两个加密变量进行加法并解密它们的和：
- en: '[PRE20]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: How it works…
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理…
- en: We begin *Step 1* by importing the random library in order to generate random
    integers in *Step 2*. We also define a large prime number, P, as we will be wanting
    a random distribution modulo, P. In *Step 2*, we define how a function encrypts
    an integer by splitting it between three parties. The value of x here is randomly
    additively split between the three parties. All operations take place in the field
    of integer modulo P. Next, in *Step 3*, we demonstrate the result of encrypting
    an integer using our approach. Proceeding to *Steps 4* and *5*, we define a function
    to reverse encryption, that is decrypt, and then show that the operation is reversible.
    In *Step 6*, we define a function to add two encrypted numbers(!). Note that encrypted
    addition is simply addition of the individual components, modulo P. In the *Encrypted
    deep learning prediction* recipe, the `.share(client, server,...)` command from
    PySyft is used. This command is basically the same encryption procedure we have
    used in this recipe, so keep in mind that these encryption schemes use the techniques
    we are discussing here. Finally, in *Step 7*, we demonstrate that we can perform
    computations on encrypted entities.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从导入随机库开始，*第 1 步*，以便在*第 2 步*中生成随机整数。我们还定义了一个大素数P，因为我们需要对其进行随机分布模P操作。在*第 2 步*中，我们定义了一个函数，通过将整数分割给三个参与方来加密该整数。这里的x值在三个参与方之间随机地加法拆分。所有操作都在模P的整数域内进行。接下来，在*第
    3 步*中，我们展示了使用我们的方法加密整数的结果。在*第 4 步*和*第 5 步*中，我们定义了一个函数来反向加密，即解密，然后证明该操作是可逆的。在*第
    6 步*中，我们定义了一个函数来对两个加密数字进行加法操作（！）。请注意，加密加法只是对单个组件进行加法操作，并对结果进行模P处理。在*加密深度学习预测*配方中，PySyft中的`.share(client,
    server,...)`命令被使用。这个命令实际上和我们在这个配方中使用的加密程序是相同的，所以请记住，这些加密方案使用的是我们在这里讨论的技术。最后，在*第
    7 步*中，我们展示了如何在加密实体上进行计算。
- en: Private deep learning prediction
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 私有深度学习预测
- en: In many situations, company A might have a trained model that it wishes to offer
    as a service. At the same time, company A may be reluctant to share this model,
    so as to avoid having its intellectual property stolen. The simple solution to
    this problem is to have customers send their data to company A, and then receive
    from it predictions. However, this becomes a problem when the customer wishes
    to preserve the privacy of their data. To resolve this tricky situation, the company,
    as well as its customers, can utilize encrypted computation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，A公司可能拥有一个训练好的模型，并希望将其作为服务提供。然而，A公司可能不愿分享这个模型，以避免其知识产权被盗用。这个问题的简单解决方法是让客户将数据发送到A公司，然后从公司获得预测结果。然而，当客户希望保持数据隐私时，这就成了一个问题。为了解决这个棘手的情况，公司和客户可以利用加密计算。
- en: In this recipe, you will learn how to share an encrypted pre-trained deep learning
    model with a client and allow the client to predict using the encrypted model
    on their own private data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，你将学习如何与客户共享一个加密的预训练深度学习模型，并允许客户使用加密模型对他们自己的私人数据进行预测。
- en: Getting ready
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Preparation for this recipe involves installing PyTorch, Torchvision, and PySyft
    in `pip`. The command is as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方的准备工作包括在`pip`中安装PyTorch、Torchvision和PySyft。命令如下：
- en: '[PRE21]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In addition, a pre-trained model named `server_trained_model.pt` has been included
    to be used in this recipe.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还包括一个名为 `server_trained_model.pt` 的预训练模型，将在此食谱中使用。
- en: How to do it…
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作……
- en: The following steps utilize PySyft to simulate a client-server interaction in
    which the server has a pre-trained deep learning model to be kept as a black box,
    and the client wishes to use the model to predict on data that is kept private.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤利用 PySyft 模拟客户端-服务器交互，其中服务器拥有一个预训练的深度学习模型，模型被保留为一个黑盒，而客户端希望使用该模型对保密数据进行预测。
- en: 'Import `torch` and access its datasets:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `torch` 并访问其数据集：
- en: '[PRE22]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Import PySyft and hook it onto `torch`:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 PySyft 并将其挂载到 `torch` 上：
- en: '[PRE23]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define a simple neural network:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个简单的神经网络：
- en: '[PRE24]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Instantiate the model and load its pre-trained weights, which are trained on
    MNIST:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化模型并加载其在 MNIST 上训练的预训练权重：
- en: '[PRE25]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Encrypt the network between `client` and `server`:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加密 `client` 和 `server` 之间的网络：
- en: '[PRE26]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Define a loader for MNIST data:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个 MNIST 数据的加载器：
- en: '[PRE27]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Define a private loader utilizing the loader for MNIST data:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个私有加载器，利用 MNIST 数据的加载器：
- en: '[PRE28]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Define a function to evaluate the private test set:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来评估私有测试集：
- en: '[PRE29]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Iterate over the private data, predict using the model, decrypt the results,
    and then print them out:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历私密数据，使用模型进行预测，解密结果，然后打印出来：
- en: '[PRE30]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Run the testing procedure:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行测试过程：
- en: '[PRE31]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The result is as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE32]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: How it works…
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: We begin by importing `torch` and its datasets, as well as some associated libraries
    (*Step 1*). We then import `pysyft` and hook it into `torch` (*Step 2*). We also
    create virtual environments for the client and server to simulate a real separation
    of data. In this step, the `crypto_provider` serves as a trusted third party to
    be used for encryption and decryption purposes. In *Step 3*, we define a simple
    neural network and, in *Step 4*, we load-in its pretrained weights. Note that,
    in *Step 5*, and, more generally, whenever the `.share(...)` command is used,
    you should think of the shared object as being encrypted, and that it is only
    possible to decrypt it with the assistance of all parties involved. In particular,
    in *Step 9*, the test function performs encrypted evaluation; the weights of the
    model, the data inputs, the prediction, and the target used for scoring are all
    encrypted. However, for the purpose of verifying that the model is working properly,
    we decrypt and display its accuracy. In *Step 5*, we encrypt the network so that
    only when the server and client are cooperating can they decrypt the network.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入 `torch` 及其数据集，和一些相关库（*步骤 1*）。然后我们导入 `pysyft` 并将其挂载到 `torch` 上（*步骤 2*）。我们还为客户端和服务器创建虚拟环境，以模拟数据的真实隔离。在此步骤中，`crypto_provider`
    作为一个可信的第三方，用于加密和解密。接着在 *步骤 3* 中，我们定义一个简单的神经网络，并在 *步骤 4* 中加载它的预训练权重。请注意，在 *步骤 5*
    中，以及一般来说，每当使用 `.share(...)` 命令时，你应该将共享的对象视为已加密，并且只有在所有相关方的协助下才能解密。特别地，在 *步骤 9*
    中，测试函数执行加密评估；模型的权重、数据输入、预测和用于评分的目标都是加密的。然而，为了验证模型是否正常工作，我们解密并显示其准确性。在 *步骤 5* 中，我们加密网络，确保只有在服务器和客户端协作时，才能解密网络。
- en: In the next two steps, we define regular and private loaders for MNIST data.
    The regular loader simply loads MNIST data, while the private loader encrypts
    the outputs of the regular loader. In *Steps 8* and *9*, we define a `helper`
    function to evaluate the private test set. In this function, we iterate over the
    private data, predict using the model, decrypt the results, and then print them
    out. Finally, we apply the function defined in *Steps 8* and *9* to establish
    that the model is performing well, while preserving privacy.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个步骤中，我们为 MNIST 数据定义常规和私有加载器。常规加载器仅加载 MNIST 数据，而私有加载器加密常规加载器的输出。在 *步骤 8*
    和 *步骤 9* 中，我们定义了一个 `helper` 函数来评估私有测试集。在该函数中，我们遍历私有数据，使用模型进行预测，解密结果，然后打印出来。最后，我们应用
    *步骤 8* 和 *步骤 9* 中定义的函数，以确保模型在保护隐私的同时能够良好地执行。
- en: Testing the adversarial robustness of neural networks
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试神经网络的对抗鲁棒性
- en: The study of adversarial attacks on neural networks has revealed a surprising
    sensitivity to adversarial perturbations. Even the most accurate of neural networks,
    when left undefended, has been shown to be vulnerable to single pixel attacks
    and the peppering of invisible-to-the-human-eye noise. Fortunately, recent advances
    in the field have offered solutions on how to harden neural networks to adversarial
    attacks of all sorts. One such solution is a neural network design called **Analysis
    by Synthesis** (**ABS**). The main idea behind the model is that it is a Bayesian
    model. Rather than directly predicting the label given the input, the model also
    learns class-conditional, sample distributions using **variational autoencoders**
    (**VAEs**). More information can be found in [https://arxiv.org/abs/1805.09190](https://arxiv.org/abs/1805.09190).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对神经网络进行对抗性攻击的研究揭示了它们对对抗性扰动的惊人敏感性。即使是最准确的神经网络，在没有防护的情况下，也已被证明容易受到单像素攻击和对人眼不可见的噪声干扰的影响。幸运的是，近期该领域的进展已提供了解决方案，帮助神经网络抵御各种对抗性攻击。其中一种解决方案是名为**综合分析**（**ABS**）的神经网络设计。该模型背后的主要思想是它是一个贝叶斯模型。模型不仅直接预测给定输入的标签，还利用**变分自编码器**（**VAEs**）学习类条件样本分布。更多信息请访问[https://arxiv.org/abs/1805.09190](https://arxiv.org/abs/1805.09190)。
- en: In this recipe, you will load a pre-trained ABS network for MNIST and learn
    how to test a neural network for adversarial robustness.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，你将加载一个预训练的ABS网络用于MNIST，并学习如何测试神经网络的对抗性鲁棒性。
- en: Getting ready
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正在准备中
- en: 'The following recipe has been tested in Python 3.6\. Preparation for this recipe
    involves installing the Pytorch, Torchvision, SciPy, Foolbox, and Matplotlib packages
    in `pip`. The command is as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下教程已在Python 3.6中测试过。准备这个教程需要在`pip`中安装Pytorch、Torchvision、SciPy、Foolbox和Matplotlib包。命令如下：
- en: '[PRE33]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: How to do it...
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In the following steps, we will load a pre-trained ABS model and a traditional
    CNN model for MNIST. We will attack both models using Foolbox to see how well
    they can defend against adversarial attacks:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将加载一个预训练的ABS模型和一个传统的CNN模型用于MNIST。我们将使用Foolbox攻击这两个模型，以查看它们在防御对抗性攻击方面的表现：
- en: 'Begin by importing a pre-trained ABS model:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先导入一个预训练的ABS模型：
- en: '[PRE34]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Define a `convenience` function to predict a batch of MNIST images using a
    model:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个`convenience`函数，使用模型预测一批MNIST图像：
- en: '[PRE35]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Predict on a batch:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对一批数据进行预测：
- en: '[PRE36]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The result is as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE37]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Wrap the model using Foolbox to enable adversarial testing:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Foolbox包装模型，以启用对抗性测试：
- en: '[PRE38]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Import the library of attacks from Foolbox and select an MNIST image:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Foolbox导入攻击库并选择一个MNIST图像：
- en: '[PRE39]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Select the attack type, in this case, a boundary attack:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择攻击类型，在本例中为边界攻击：
- en: '[PRE40]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Display the original image and its label using Matplotlib:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Matplotlib显示原始图像及其标签：
- en: '[PRE41]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The image produced is as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图像如下：
- en: '![](assets/0cb4d4d5-9a8f-43b7-ad67-f60877d32b84.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/0cb4d4d5-9a8f-43b7-ad67-f60877d32b84.png)'
- en: 'Search for an adversarial instance using Foolbox:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Foolbox查找对抗性实例：
- en: '[PRE42]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Show the discovered adversarial example:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示发现的对抗性样本：
- en: '[PRE43]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The adversarial image produced is as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的对抗性图像如下：
- en: '![](assets/f3113288-d9d3-4d8b-81d2-0c0179c5685f.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/f3113288-d9d3-4d8b-81d2-0c0179c5685f.png)'
- en: 'Instantiate a traditional CNN model trained on MNIST:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个在MNIST上训练的传统CNN模型：
- en: '[PRE44]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The model architecture is as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构如下：
- en: '[PRE45]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Perform a sanity check to make sure that the model is performing as expected:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行合理性检查，确保模型按预期运行：
- en: '[PRE46]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The printout is as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE47]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Wrap the traditional model using Foolbox:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Foolbox包装传统模型：
- en: '[PRE48]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Attack the traditional CNN model:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 攻击传统CNN模型：
- en: '[PRE49]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Display the adversarial example discovered:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示发现的对抗性样本：
- en: '[PRE50]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The adversarial image produced is as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的对抗性图像如下：
- en: '![](assets/f9fbf766-8c65-4f3d-98fd-5e2f36e9fa55.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/f9fbf766-8c65-4f3d-98fd-5e2f36e9fa55.png)'
- en: How it works...
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We begin by importing a pre-trained ABS model (*Step 1*). In *Steps 2* and *3*,
    we defined a `convenience` function to predict a batch of MNIST images and to
    verify that the model is working properly. Next, we wrapped the model using Foolbox
    in preparation for testing its adversarial robustness (*Step 4*). Note that Foolbox
    facilitates the attacking of either TensorFlow or PyTorch models using the same
    API once wrapped. Nice! In *Step 5*, we select an MNIST image to use as the medium
    for our attack. To clarify, this image gets tweaked and mutated until the result
    fools the model. In *Step 6*, we select the attack type we want to implement.
    We select a boundary attack, which is a decision-based attack that starts from
    a large adversarial perturbation and then gradually reduces the perturbation while
    remaining adversarial. The attack requires little hyperparameter tuning, hence,
    no substitute models and no gradient computations. For more information about
    decision-based attacks, refer to [https://arxiv.org/abs/1712.04248](https://arxiv.org/abs/1712.04248).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入一个预训练的ABS模型（*步骤 1*）。在*步骤 2*和*步骤 3*中，我们定义了一个`convenience`函数，用于预测一批MNIST图像并验证模型是否正常工作。接下来，我们使用Foolbox包装模型，为测试其对抗鲁棒性做准备（*步骤
    4*）。请注意，Foolbox可以通过相同的API攻击TensorFlow或PyTorch模型，一旦被包装。太棒了！在*步骤 5*中，我们选择了一张MNIST图像作为攻击的媒介。为了明确，这张图像会被调整和变形，直到结果能欺骗模型。在*步骤
    6*中，我们选择了要实施的攻击类型。我们选择了边界攻击，这是一种基于决策的攻击，从一个大的对抗扰动开始，然后逐渐减小扰动，同时保持对抗性。这种攻击需要很少的超参数调整，因此无需替代模型或梯度计算。有关基于决策的攻击的更多信息，请参考[https://arxiv.org/abs/1712.04248](https://arxiv.org/abs/1712.04248)。
- en: In addition, note that the metric used here is **mean squared error** (**MSE**),
    which determines how the adversarial example is assessed as close to, or far from,
    the original image. The criterion used is misclassification, meaning that the
    search terminates once the target model misclassifies the image. Alternative criteria
    may include a confidence level or a specific type of misclassification. In *Steps
    7-9*, we display the original image, as well as the adversarial example generated
    from it. In the next two steps, we instantiate a standard CNN and verify that
    it is working properly. In *Steps 12-14*, we repeat the attack from the previous
    steps on the standard CNN. Looking at the result, we see that the experiment is
    a strong visual indicator that the ABS model is more robust to adversarial perturbations
    than a vanilla CNN.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，这里使用的度量标准是**均方误差**（**MSE**），它用于衡量对抗样本与原始图像之间的接近程度。使用的标准是误分类，意味着一旦目标模型误分类了图像，搜索就会停止。替代标准可能包括置信度或特定类型的误分类。在*步骤
    7-9*中，我们展示了原始图像以及从其生成的对抗样本。在接下来的两步中，我们实例化一个标准的CNN并验证它是否正常工作。在*步骤 12-14*中，我们在标准CNN上重复之前步骤中的攻击。通过查看结果，我们可以看出，该实验强烈表明，ABS模型在对抗扰动方面比普通的CNN更具鲁棒性。
- en: Differential privacy using TensorFlow Privacy
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow Privacy的差分隐私
- en: TensorFlow Privacy ([https://github.com/tensorflow/privacy](https://github.com/tensorflow/privacy)) is
    a relatively new addition to the TensorFlow family. This Python library includes
    implementations of TensorFlow optimizers for training machine learning models
    with *differential privacy*. A model that has been trained to be differentially
    private does not non-trivially change as a result of the removal of any single
    training instance from its dataset. (Approximate) differential privacy is quantified
    using *epsilon* and *delta*, which give a measure of how sensitive the model is
    to a change in a single training example. Using the Privacy library is as simple
    as wrapping the familiar optimizers (for example, RMSprop, Adam, and SGD) to convert
    them to a differentially private version. This library also provides convenient
    tools for measuring the privacy guarantees, epsilon, and delta.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Privacy ([https://github.com/tensorflow/privacy](https://github.com/tensorflow/privacy))
    是TensorFlow家族中的一个相对较新的成员。这个Python库包括用于训练具有*差分隐私*的机器学习模型的TensorFlow优化器的实现。一个经过差分隐私训练的模型，在从其数据集中删除任何单一训练实例时，不会发生非平凡的变化。差分隐私（近似值）使用*epsilon*和*delta*来量化，衡量模型对单个训练示例变化的敏感性。使用Privacy库的方法很简单，只需包装常见的优化器（例如RMSprop、Adam和SGD）即可将它们转换为差分隐私版本。这个库还提供了方便的工具来衡量隐私保证、epsilon和delta。
- en: In this recipe, we show you how to implement and train a differentially private
    deep neural network for MNIST using Keras and TensorFlow Privacy.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将展示如何使用Keras和TensorFlow Privacy实现并训练一个用于MNIST的差分隐私深度神经网络。
- en: Getting ready
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Preparation for this recipe involves installing Keras and TensorFlow. The command
    is as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程的准备工作包括安装 Keras 和 TensorFlow。相关命令如下：
- en: '[PRE51]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Installation instructions for TensorFlow Privacy can be found at [https://github.com/tensorflow/privacy](https://github.com/tensorflow/privacy).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Privacy 的安装说明可以在 [https://github.com/tensorflow/privacy](https://github.com/tensorflow/privacy)
    找到。
- en: How to do it...
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Begin by defining a few convenience functions for pre-processing the MNIST
    dataset:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始时定义几个便捷函数来预处理 MNIST 数据集：
- en: '[PRE52]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Write a convenience function to load MNIST:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个便捷函数来加载 MNIST 数据集：
- en: '[PRE53]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Load the MNIST dataset:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 MNIST 数据集：
- en: '[PRE54]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The training set is 60 k in size, and the testing set 10 k.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集大小为 60k，测试集大小为 10k。
- en: '4\. Import a differentially private optimizer and define a few parameters that
    control the learning rate and extent of differential privacy:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 导入差分隐私优化器并定义几个控制学习率和差分隐私程度的参数：
- en: '[PRE55]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'In order to measure privacy, define a function to compute epsilon:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了衡量隐私性，定义一个函数来计算 epsilon：
- en: '[PRE56]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Define a standard Keras CNN for MNIST:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为 MNIST 定义一个标准的 Keras CNN：
- en: '[PRE57]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Compiling `model`:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译 `model`：
- en: '[PRE58]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Fit and test `model`:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合并测试 `model`：
- en: '[PRE59]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Compute the value of `epsilon`, the measure of privacy:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 `epsilon` 值，即隐私性的度量：
- en: '[PRE60]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: How it works...
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We begin *Steps 1-3* by preparing and loading the MNIST dataset. Next, in *Step
    4*, we import `DPGradientDescentGaussianOptimizer`, an optimizer that allows the
    model to become differentially private. A number of parameters are used at this
    stage, and these stand to be clarified. The `l2_norm_clip` parameter refers to
    the maximum norm of each gradient computed on an individual training datapoint
    from a minibatch. This parameter bounds the sensitivity of the optimizer to individual
    training points, thereby moving the model toward differential privacy. The `noise_multiplier`
    parameter controls the amount of random noise added to gradients. Generally, the
    more noise, the greater the privacy. Having finished this step, in *Step 5*, we
    define a function that computes the epsilon of the epsilon-delta definition of
    differential privacy. We instantiate a standard Keras neural network (*Step 6*),
    compile it (*Step 7*), and then train it on MNIST using the differentially private
    optimizer (*Step 8*). Finally, in *Step 9*, we compute the value of epsilon, which
    measures the extent to which the model is differentially private. Typical values
    for this recipe are an epsilon value of around 1.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 *步骤 1-3* 开始，准备并加载 MNIST 数据集。接下来，在 *步骤 4* 中，我们导入 `DPGradientDescentGaussianOptimizer`，这是一种优化器，使模型具备差分隐私。此阶段使用了多个参数，并且这些参数有待进一步解释。`l2_norm_clip`
    参数表示在每个小批量数据点的训练过程中，每个梯度的最大范数。该参数限制了优化器对单个训练点的敏感度，从而使模型朝着差分隐私的方向发展。`noise_multiplier`
    参数控制向梯度中添加的随机噪声量。通常，噪声越多，隐私性越强。完成此步骤后，在 *步骤 5* 中，我们定义一个计算差分隐私的 epsilon-delta 定义的
    epsilon 值的函数。我们在 *步骤 6* 中实例化一个标准的 Keras 神经网络，*步骤 7* 中编译它，然后使用差分隐私优化器在 MNIST 上训练它
    (*步骤 8*)。最后，在 *步骤 9* 中，我们计算 epsilon 的值，该值衡量模型的差分隐私程度。本教程的典型 epsilon 值大约为 1。
