- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Data Exposure and Sensitive Information Leakage
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据曝光和敏感信息泄漏
- en: This chapter starts the fourth part of our book, which is about advanced API
    techniques. We will better understand the inherent problems of data exposure and
    sensitive information leakage that unpatched or badly configured API endpoints
    can suffer. We will tackle the nuances of how this can happen and ways of taking
    this in our favor as API pentesters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始了我们书的第四部分，内容关于高级 API 技术。我们将更好地理解未修补或配置不当的 API 端点可能面临的数据曝光和敏感信息泄漏问题。我们将探讨这些问题发生的细微差别，并学习如何作为
    API 渗透测试员将其转化为对我们有利的局面。
- en: Either by digesting some data masses or by taking a ride on previous pentesting
    findings, we will learn how data or sensitive information can be detected among
    other garbage or less valuable assets. This can save you time not only when conducting
    a pentest but also when planning to hit the final target of a coordinated attack.
    Some testers establish the scope of their work on exfiltrating some data from
    the endpoint, whereas others work to get it down (by abusing their network, for
    example). You will learn the techniques and understand how such problems can be
    avoided when configuring or building an API.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是通过消化一些数据块，还是通过利用之前渗透测试的发现，我们将学习如何在其他垃圾或不太有价值的资产中检测到数据或敏感信息。这不仅可以节省你进行渗透测试时的时间，还能在计划进行协调攻击的最终目标时帮助你。某些测试人员的工作范围是将一些数据从端点中提取出去，而另一些则通过滥用网络等手段来进行数据下传。你将学习这些技术，并了解如何在配置或构建
    API 时避免此类问题。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要话题：
- en: Identifying sensitive data exposure
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别敏感数据曝光
- en: Testing for information leakage
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试信息泄漏
- en: Preventing data leakage
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防止数据泄漏
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: As we did in previous chapters, we’ll leverage the same environment as the one
    pointed out in previous chapters, such as an Ubuntu distro. Some other new relevant
    utilities will be mentioned in the corresponding sections.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在之前的章节中所做的那样，我们将利用与之前章节中提到的相同的环境，比如 Ubuntu 发行版。其他一些新的相关工具将在相应的章节中提到。
- en: We will be especially occupied with handling vast amounts of data in this chapter.
    Hence, we will count on some data mining and curation tools that will do the hard
    work for us when analyzing huge-sized logs or other types of big data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将特别专注于处理大量数据。因此，我们将依赖一些数据挖掘和整理工具，这些工具将在分析大规模日志或其他类型的大数据时为我们完成繁重的工作。
- en: Identifying sensitive data exposure
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别敏感数据曝光
- en: 'Identifying sensitive data exposure in APIs is a critical step in securing
    them. Regardless of their size, data breaches can cause severe and often irreparable
    damage to companies’ reputations. Hence, fully comprehending potential vulnerabilities
    on the API endpoints you own is paramount. The first step is defining what constitutes
    sensitive data. This goes beyond just **Personally Identifiable Information**
    (**PII**) such as names and addresses. Here’s a breakdown of different types of
    sensitive data and how APIs might expose them:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 识别 API 中的敏感数据曝光是确保它们安全的关键步骤。无论大小，数据泄露都可能对公司声誉造成严重且常常无法修复的损害。因此，全面了解你拥有的 API
    端点可能存在的漏洞至关重要。第一步是定义什么构成敏感数据。这不仅仅是**个人身份信息**（**PII**）如姓名和地址。以下是不同类型敏感数据的分类，以及
    API 如何暴露这些数据：
- en: '**PII**: This corresponds to all kinds of data or information that can be used
    to identify a person or individual. This includes government ID numbers (such
    as social security numbers in the USA or Europe, or CPF in Brazil), passport information
    (such as passport numbers, as well as issue and expiry dates), and even health
    data. APIs that return user profiles without proper access control might expose
    PII.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个人身份信息（PII）**：这对应于所有可以用来识别某人或个人的各种数据或信息。这包括政府身份证号码（例如美国或欧洲的社会保障号码，或巴西的 CPF），护照信息（例如护照号码，以及签发和到期日期），甚至健康数据。没有适当访问控制的
    API 返回用户资料可能会泄露个人身份信息。'
- en: '**Financial data**: Credit card details, bank account numbers, and financial
    transaction history are highly sensitive. If an API endpoint needs to process
    any type of payment, even when simply redirecting data to and receiving data from
    a payment system, it must have strict security controls in place.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**财务数据**：信用卡详细信息、银行账户号码和财务交易历史都是高度敏感的。如果一个 API 端点需要处理任何类型的支付，即使只是将数据重定向到支付系统并从中接收数据，它也必须具备严格的安全控制。'
- en: '**Authentication** (**AuthN**) **credentials**: Usernames, passwords, and access
    tokens are fundamental for securing APIs. When such data leaks, access to the
    whole system behind an API endpoint can be compromised.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**身份验证**（**AuthN**）**凭据：** 用户名、密码和访问令牌是保护 API 安全的基础。当此类数据泄露时，可能会危及 API 端点背后整个系统的访问权限。'
- en: '**Proprietary information:** Trade secrets, intellectual property documents,
    and internal configurations can all be considered sensitive data. APIs that interact
    with internal systems or databases could potentially leak such information if
    they are not properly secured.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专有信息：** 商业机密、知识产权文件以及内部配置都可以视为敏感数据。如果与内部系统或数据库交互的 API 没有得到妥善保护，它们可能会泄露此类信息。'
- en: It’s not always straightforward to detect when sensitive data is available to
    be extracted from an output. This may require some sophistication on the toolbelt
    we use to parse file dumps such as logs. We will now dive into a mass of logs
    and combine a few tools and patterns to discover which sensitive data or information
    is available. Depending on the log volume you have at hand, you may need to delegate
    this to an external system with more computing power to process it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 检测何时可以从输出中提取敏感数据并不总是那么直观。这可能需要我们在解析文件转储（如日志）时使用一些复杂的工具。现在我们将深入分析大量日志，结合几种工具和模式，发现哪些敏感数据或信息是可用的。根据手头的日志量，你可能需要将此任务委派给外部系统，以利用更强的计算能力来处理它。
- en: As true API endpoints with true sensitive data won’t be used during this exercise,
    we need a way to generate some log files to be analyzed. There’s a good open source
    project written in Golang called `go` command directly as a binary (including
    using `.tar.gz` packages) or run it as a Docker container.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在本次操作中不会使用真正的 API 端点和真正的敏感数据，我们需要一种方法来生成一些日志文件进行分析。有一个很好的开源项目是用 Golang 编写的，叫做
    `go` 命令，可以直接作为二进制文件使用（包括使用 `.tar.gz` 包）或者作为 Docker 容器运行。
- en: 'These lines will not contain any type of sensitive data we are looking for.
    Therefore, let’s boost the utility with some random data that we can further search
    in queries. The code that follows does that. The loop creates log entries and
    stores them in the file pointed to by the `LOG_FILE` variable. Observe that sensitive
    data is only inserted when the iterator variable (`i`) is divisible by 100\. When
    `i` is not divisible by 100, `flog` generates a completely random line. Hence,
    we’ll have 1,000 lines with sensitive data and 9,000 lines with no sensitive data.
    This will make the output file a big mass of data with less interesting content.
    The echo command is in a single line:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些行不会包含我们正在寻找的任何类型的敏感数据。因此，让我们通过一些随机数据来增强其功能，方便后续查询。接下来的代码就是这么做的。循环创建日志条目，并将其存储在由
    `LOG_FILE` 变量指向的文件中。请注意，只有当迭代器变量 (`i`) 能被 100 整除时，敏感数据才会被插入。当 `i` 不能被 100 整除时，`flog`
    会生成一行完全随机的内容。因此，我们将有 1,000 行包含敏感数据，9,000 行不包含敏感数据。这将使输出文件变成一个包含大量数据但不太有趣的内容。echo
    命令在一行内执行：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We are making use of BASH’s `$RANDOM` internal variable, which generates pseudorandom
    numbers when read. Observe that we need to have `openssl` available on the system
    to generate the random strings that correspond to fake tokens. Simply delete the
    `Auth_Token` part if you don’t want it to be included. The preceding code creates
    a file of around 1 GB in size.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用 BASH 的 `$RANDOM` 内部变量，它在读取时生成伪随机数。请注意，我们需要在系统上安装 `openssl` 来生成与虚假令牌对应的随机字符串。如果你不想包含
    `Auth_Token` 部分，只需删除即可。上面的代码创建了一个约 1 GB 大小的文件。
- en: So, how can we digest this data mass and only extract the interesting parts?
    There are some ways to do it. Considering that we are using a Linux system, even
    the `grep` command could fulfill this task, accompanied by a few regular expressions
    to facilitate the search. This is not the solution with the best performance though.
    We need something else.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何消化这些数据并只提取有趣的部分呢？有一些方法可以做到这一点。考虑到我们使用的是 Linux 系统，甚至 `grep` 命令也可以完成这个任务，并通过一些正则表达式来方便搜索。不过，这并不是性能最好的解决方案。我们需要其他方法。
- en: Elasticsearch and more
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Elasticsearch 及更多内容
- en: 'When handling big masses of data, we require the right tool. OK, 1 GB is not
    that big nowadays, but imagining that you will have access to terabytes of log
    files, how would search them all with `grep` in a feasible timeframe? We will
    exercise one possible solution: the **Elasticsearch, Logstash, and Kibana** (**ELK**)
    stack. They are three separate products that can be combined to provide one of
    the best-in-class experiences for data analysis and visualization activities.
    Also, they can run as Docker containers.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大量数据时，我们需要正确的工具。好吧，1 GB现在不算大，但假设你将需要访问数TB的日志文件，如何在可行的时间内用`grep`搜索它们呢？我们将练习一种可能的解决方案：**Elasticsearch、Logstash和Kibana**（**ELK**）堆栈。它们是三个独立的产品，可以结合在一起提供一流的数据分析和可视化体验。而且，它们可以作为Docker容器运行。
- en: One drawback, however, is the substantial requirement for resources (computing,
    memory, and storage). I could not run them on the lab VM (with 8 GB of RAM). Elasticsearch
    alone required more memory than was available. As a matter of fact, on the version
    I tried while writing this chapter (8.13.2), it was specifically complaining about
    the maximum map count check, which is controlled by a Linux kernel parameter.
    Even after increasing it to the number recommended by the documentation (reference
    the *Further reading* section for this), Elasticsearch didn’t work. I’ve also
    done a few tests with another system running on top of macOS, but both the container
    versions and the standalone versions presented different problems that made it
    difficult to set them up.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一个缺点，就是对资源（计算、内存和存储）的巨大需求。我无法在实验室虚拟机（8 GB RAM）上运行它们。仅仅是Elasticsearch就需要比可用内存更多的资源。事实上，在我写这章时尝试的版本（8.13.2）中特别抱怨最大映射数检查，这个检查由Linux内核参数控制。即使将其增加到文档推荐的数值（详细信息请参阅*进一步阅读*部分），Elasticsearch仍然无法工作。我还在另一台运行macOS的系统上进行了一些测试，但无论是容器版本还是独立版本，都存在不同的问题，使得配置变得困难。
- en: 'I finally decided to run this stack on Elastic’s cloud platform. They sell
    it as a **Software as a Service** (**SaaS**) with a 14-day trial period. You can
    use all the product’s features and ingest external sources. There’s a sequence
    of steps to set this platform up:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我最终决定在Elastic的云平台上运行这个堆栈。他们将其作为**软件即服务**（**SaaS**）销售，并提供14天的试用期。你可以使用产品的所有功能并导入外部数据源。设置此平台的步骤如下：
- en: You need to sign up for the platform or subscribe via AWS, Google, or Microsoft’s
    cloud marketplace. Access [https://cloud.elastic.co/](https://cloud.elastic.co/)
    and click on **Sign up**. You may receive a verification email with a link to
    click. Do it and log in.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要注册该平台或通过AWS、Google或Microsoft的云市场进行订阅。访问[https://cloud.elastic.co/](https://cloud.elastic.co/)并点击**注册**。你可能会收到一封带有链接的验证邮件，点击该链接并登录。
- en: The wizard will prompt you to answer a few questions about yourself, such as
    your full name, company name, and purpose for using the platform.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向导会提示你回答一些关于你自己的问题，例如你的全名、公司名称以及使用该平台的目的。
- en: Then, the wizard will suggest creating a deployment. By clicking on **Edit settings**,
    you can choose the public cloud provider, region, hardware profile, and Elastic
    version. The application automatically selects a combination that’s appropriate
    to your location. Type a name for this deployment and click **Create deployment**.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，向导会建议创建一个部署。通过点击**编辑设置**，你可以选择公共云提供商、区域、硬件配置和Elastic版本。应用程序会自动选择适合你所在位置的组合。为这个部署输入一个名称，然后点击**创建部署**。
- en: 'The deployment takes just a couple of minutes to be created and you are then
    redirected to the landing page of the platform. A small note is important here:
    you won’t receive your deployment’s credentials as expected. Because of that,
    you will need to follow an additional step that we’ll explain later:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建部署只需要几分钟，然后你会被重定向到平台的登录页面。这里有一个小提示很重要：你不会像预期的那样收到部署凭据。因此，你需要遵循一个额外的步骤，我们稍后会解释：
- en: '![Figure 8.1 – Elastic Cloud Platform’s landing page](img/B19657_figure_08.1.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – Elastic Cloud Platform的登录页面](img/B19657_figure_08.1.jpg)'
- en: Figure 8.1 – Elastic Cloud Platform’s landing page
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – Elastic Cloud Platform的登录页面
- en: The next step is to configure an input. You need to tell it how Elasticsearch
    and Kibana will receive data that you’ll further analyze. For that part, we will
    make use of Filebeat, which is both an external utility and a built-in integration.
    You can even stream logs directly to the platform. That’s very useful when you
    want to continuously send data to be analyzed. In our case, it will happen only
    once.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是配置输入。你需要告诉它如何接收 Elasticsearch 和 Kibana 将要进一步分析的数据。为此，我们将使用 Filebeat，它既是一个外部工具，也是一种内建集成。你甚至可以直接将日志流式传输到平台。当你希望持续发送数据进行分析时，这非常有用。在我们的情况下，这只会发生一次。
- en: There are specific installation instructions depending on the operating system
    you are using. Ubuntu, by default, does not have the repository that the application
    can be downloaded from. For your convenience, I put a link in the *Further reading*
    section with the steps you need to follow.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据你所使用的操作系统，有不同的安装说明。Ubuntu 默认没有该应用程序的可下载仓库。为了方便起见，我在 *进一步阅读* 部分提供了需要遵循的步骤链接。
- en: 'You won’t start Filebeat’s service right away. First, you’ll have to configure
    it to send data to Elastic’s cloud. At least on Ubuntu, the `filebeat.yml` configuration
    file is located at `/etc/filebeat`. You must only worry about two sections: **Filebeat
    inputs** and **Elastic Cloud**. Make a backup copy of this file and edit it with
    your preferred editor. Locate the **Filebeat** **inputs** section.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你不会立即启动 Filebeat 服务。首先，你需要配置它，将数据发送到 Elastic 的云端。至少在 Ubuntu 系统上，`filebeat.yml`
    配置文件位于 `/etc/filebeat`。你只需关注两个部分：**Filebeat 输入**和 **Elastic Cloud**。请先备份该文件，并用你喜欢的编辑器打开它。找到
    **Filebeat 输入** 部分。
- en: 'You’ll see something like this (the comments were omitted for brevity):'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会看到类似这样的内容（为了简洁，省略了注释）：
- en: '[PRE1]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You’ll have to do the following:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要执行以下操作：
- en: Replace `filestream` with `log`. This is to instruct Filebeat that this is not
    a file being constantly changed, but rather a static one.
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `filestream` 替换为 `log`。这是为了告诉 Filebeat 这个不是一个持续变化的文件，而是一个静态文件。
- en: Replace `my-filestream-id` with something more relevant, such as `sensitive-data-log`.
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `my-filestream-id` 替换为更相关的名称，比如 `sensitive-data-log`。
- en: Replace `false` with `true` to effectively activate the input.
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `false` 替换为 `true`，以有效启用输入。
- en: Replace `/var/log/*.log` with the full path of the file you generated on the
    code we used before (the one with the `flog` utility).
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `/var/log/*.log` 替换为你之前生成的文件的完整路径（即使用 `flog` 工具生成的那个文件）。
- en: 'Locate the **Elastic Cloud** section. You’ll see something like this:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到 **Elastic Cloud** 部分。你会看到类似这样的内容：
- en: '[PRE2]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'At this point, you’ll have to get back to the web console and locate both parameters.
    The Cloud ID can be found using this sequence:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时，你需要回到网页控制台并找到这两个参数。可以通过以下顺序查找 Cloud ID：
- en: From the landing page in *Figure 8**.1*, click on the three horizontal lines
    located on the left to open the lateral menu and choose **Manage** **this deployment**.
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *图 8.1* 中的登陆页面，点击左侧的三个水平线打开侧边菜单并选择 **管理此部署**。
- en: There’s a clipboard button you can click to facilitate copying this data. Click
    to copy and save it in a temporary place.
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有一个剪贴板按钮，你可以点击它来方便地复制此数据。点击复制并将其保存在临时位置。
- en: '![Figure 8.2 – Where to find the Cloud ID on Elastic’s console](img/B19657_figure_08.2.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2 – 在 Elastic 控制台中查找 Cloud ID 的位置](img/B19657_figure_08.2.jpg)'
- en: Figure 8.2 – Where to find the Cloud ID on Elastic’s console
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 在 Elastic 控制台中查找 Cloud ID 的位置
- en: 'The `Cloud Auth` parameter demands a few more steps:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Cloud Auth` 参数需要几个额外步骤：'
- en: 'On this screen, click on the **Actions** button and select **Reset password**.
    This will redirect you to the **Security** settings page, where you can make a
    few adjustments:'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此屏幕上，点击 **操作** 按钮并选择 **重置密码**。这将把你重定向到 **安全性** 设置页面，在那里你可以做一些调整：
- en: '![Figure 8.3 – Resetting the deployment’s password](img/B19657_figure_08.3.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3 – 重置部署的密码](img/B19657_figure_08.3.jpg)'
- en: Figure 8.3 – Resetting the deployment’s password
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 重置部署的密码
- en: Click on the **Reset password** button. The website will ask you for confirmation.
    Simply click on **Reset**.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **重置密码** 按钮。网站会要求你确认。只需点击 **重置**。
- en: Your new password will be defined. You are good to either copy it (using a similar
    clipboard button) or download a CSV file with the credentials. See *Figure 8**.4*
    for reference.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你的新密码将被定义。你可以复制它（使用类似的剪贴板按钮）或下载包含凭证的 CSV 文件。参考 *图 8.4*。
- en: '![Figure 8.4 – The new Elastic password is defined, with the option to click
    to copy it or download the CSV file](img/B19657_figure_08.4.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4 – 新的 Elastic 密码已定义，并提供点击复制或下载 CSV 文件的选项](img/B19657_figure_08.4.jpg)'
- en: Figure 8.4 – The new Elastic password is defined, with the option to click to
    copy it or download the CSV file
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – 新的 Elastic 密码已定义，并提供点击复制或下载 CSV 文件的选项
- en: Now, go back to the `filebeat.yml` file.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，返回到`filebeat.yml`文件。
- en: Uncomment the `cloud.id` and `cloud.auth` lines. Next, insert a blank space
    right after each of the colons in both lines.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取消注释`cloud.id`和`cloud.auth`这两行。接下来，在这两行中每个冒号后插入一个空格。
- en: Paste the data that you previously copied on the corresponding lines. For the
    `cloud.auth` line, observe that the expected format is `username:password`. The
    username portion is usually `elastic`.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你之前复制的数据粘贴到相应的行中。对于`cloud.auth`这一行，注意预期格式是`username:password`。用户名部分通常是`elastic`。
- en: 'Save and close the file. There are a few commands you can use to verify whether
    the config file is in good shape and whether Filebeat can contact the cloud deployment:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存并关闭文件。你可以使用一些命令来验证配置文件是否良好，以及 Filebeat 是否能与云部署进行连接：
- en: '[PRE3]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Note that you may need to run the commands as a superuser. This will depend
    on your operating system’s defaults. Now, you are good to either start the Filebeat
    service or run it interactively. I personally prefer the second way since you
    can watch its log output:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，你可能需要以超级用户身份运行这些命令。这取决于你的操作系统默认设置。现在，你可以启动 Filebeat 服务或以交互模式运行它。我个人更喜欢第二种方式，因为你可以查看它的日志输出：
- en: '[PRE4]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'At this stage, you can return to the console to check what it is receiving.
    Assuming that everything is working, to see the lines of `dummy.log` populated,
    click on the three horizontal lines on the lateral menu again and go to **Observability**
    | **Logs**. If nothing shows up, just click **Refresh**. By default, this view
    shows the last 15 minutes of activity. If you were doing something else while
    the data was already being sent, you may not see anything at all. If that happens,
    simply change the view control to show older data, such as **Last** **1 year**:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你可以返回控制台查看它正在接收的内容。假设一切正常，要查看`dummy.log`中的数据行，请再次点击侧边菜单中的三条横线，进入**Observability**
    | **Logs**。如果没有显示任何内容，只需点击**刷新**。默认情况下，这个视图会显示过去 15 分钟的活动。如果你在数据已发送时做了其他操作，可能什么也看不见。如果发生这种情况，只需将视图控制更改为显示更早的数据，如**过去
    1 年**：
- en: '![Figure 8.5 – Changing the view control to display log data](img/B19657_figure_08.5.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.5 – 更改视图控制以显示日志数据](img/B19657_figure_08.5.jpg)'
- en: Figure 8.5 – Changing the view control to display log data
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – 更改视图控制以显示日志数据
- en: The change to the view control takes effect immediately. The following screenshot
    shows the type of viewing you will have when browsing the log data sent by Filebeat.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对视图控制的更改立即生效。以下截图显示了你在浏览由 Filebeat 发送的日志数据时将看到的视图类型。
- en: '![Figure 8.6 – The log lines available to be queried on the Elastic Cloud platform](img/B19657_figure_08.6.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.6 – 可在 Elastic Cloud 平台上查询的日志行](img/B19657_figure_08.6.jpg)'
- en: Figure 8.6 – The log lines available to be queried on the Elastic Cloud platform
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 – 可在 Elastic Cloud 平台上查询的日志行
- en: 'Observe that the lines start with an IP address. We can use this as an index.
    To be able to search patterns on this data, we can choose one of the options that
    follow:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这些行是以 IP 地址开头的。我们可以将其用作索引。为了能够在这些数据上搜索模式，我们可以选择以下其中一种选项：
- en: Simply type the data you are looking for into this search bar. For example,
    if you type `Credit_card`, or `Auth_Token`, all lines with these patterns will
    be displayed after you press *Enter*.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只需在此搜索栏中输入你要查找的数据。例如，如果你输入`Credit_card`或`Auth_Token`，在你按下*Enter*后，所有包含这些模式的行将会显示出来。
- en: Create a data view. Some literature will use the term **Index pattern** to refer
    to this, but it was renamed some time ago to data view.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个数据视图。某些文献中会使用**索引模式**这一术语来指代这个功能，但它在一段时间前已经被更名为数据视图。
- en: 'This is a Kibana feature. To create data views, it will be easier if you type
    `Data View` in the topmost search bar. This will cause a suggestion to show up
    along with the corresponding link. Click on it. You’ll be redirected to a blank
    page with the **Create data view** button. After clicking on it, all sources will
    be displayed. Some of them were created by the deployment and there will be a
    Filebeat one:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Kibana 的一个功能。要创建数据视图，如果你在最上面的搜索栏中输入 `Data View`，会更容易。这样会出现一个建议，并附带相应的链接。点击它，你将被重定向到一个空白页面，页面上有**创建数据视图**按钮。点击它后，所有源将被显示。其中一些是由部署创建的，还会有一个
    Filebeat 的源：
- en: '![Figure 8.7 – Creating a data view in Kibana](img/B19657_figure_08.7.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.7 – 在 Kibana 中创建数据视图](img/B19657_figure_08.7.jpg)'
- en: Figure 8.7 – Creating a data view in Kibana
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7 – 在 Kibana 中创建数据视图
- en: In the `filebeat-*`. This will cause the right part of the screen to update
    and show only the Filebeat source under the `@timestamp`). Click on **Save data
    view** **to Kibana**.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `filebeat-*` 中。这将导致屏幕右侧更新并仅显示 `@timestamp` 下的 Filebeat 来源）。点击**保存数据视图** **到
    Kibana**。
- en: 'When you do this, the previous blank page will be updated with the recently
    created data view. Now, the final step is to discover patterns. Go back to the
    topmost search bar and type `Discover`. You will now be redirected to the `message`
    keyword on KQL. We could build a query like this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当你这么做时，之前的空白页面将更新为最近创建的数据视图。现在，最后一步是发现模式。回到最上面的搜索栏，输入 `Discover`。你将被重定向到 KQL
    的 `message` 关键字。我们可以构建这样的查询：
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You will end up with the filtered window displayed:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最终将显示过滤后的窗口：
- en: '![Figure 8.8 – Using KQL to look for sensitive data patterns](img/B19657_figure_08.8.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.8 – 使用 KQL 查找敏感数据模式](img/B19657_figure_08.8.jpg)'
- en: Figure 8.8 – Using KQL to look for sensitive data patterns
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8 – 使用 KQL 查找敏感数据模式
- en: This is far from being an introductory training to the ELK stack. I put other
    links in the *Further reading* section, where you can look at regular expressions
    on the platform as well as take a free training course on it. That’s cool, but
    what if you don’t want to use a browser or even leverage some cloud offering to
    do your sensitive search? We’ll cover that next.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这远不是 ELK 堆栈的入门培训。我在*进一步阅读*部分添加了其他链接，你可以在平台上查看正则表达式，并且可以参加免费的培训课程。这个很好，但如果你不想使用浏览器，甚至不想利用一些云服务来进行敏感数据搜索怎么办？我们接下来会介绍这个。
- en: ripgrep
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ripgrep
- en: If you’re looking for a tool with a smaller footprint than ELK for searching
    through logs for sensitive data, `rg` is a line-oriented search tool that combines
    the usability of The Silver Searcher (link in the *Further reading* section) with
    the raw speed of grep. `rg` works very efficiently by default, ignoring binary
    files, respecting your `.gitignore` files to skip hidden and ignored files, and
    using memory efficiently.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在寻找一个比 ELK 更小巧的工具来搜索日志中的敏感数据，`rg` 是一个面向行的搜索工具，它结合了 The Silver Searcher（链接在*进一步阅读*部分）的可用性和
    grep 的原始速度。`rg` 默认工作非常高效，忽略二进制文件，尊重你的 `.gitignore` 文件以跳过隐藏和忽略的文件，并且有效地使用内存。
- en: '`rg` has at least three advantages when compared to the ELK stack:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 与 ELK 堆栈相比，`rg` 至少有三个优势：
- en: It is extremely fast and performs well even on large files.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它非常快速，即使在大文件上也能表现良好。
- en: It is a sole executable file, easy to install and use without complex configurations.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是一个独立的可执行文件，安装和使用都非常简单，不需要复杂的配置。
- en: Does not require running services or daemons and has minimal memory and CPU
    usage compared to ELK.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不需要运行服务或守护进程，与 ELK 相比，内存和 CPU 使用非常少。
- en: 'Installing it on Ubuntu is as easy as installing any application available
    via `apt` or `apt-get`. There are also versions available for macOS and Windows.
    Let’s see how it behaves with our 1 GB dummy file when looking for credit card
    numbers:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Ubuntu 上安装它和安装任何通过 `apt` 或 `apt-get` 提供的应用程序一样简单。它也有适用于 macOS 和 Windows 的版本。让我们看看它在查找信用卡号时如何处理我们的
    1 GB 虚拟文件：
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In around 2.5 seconds, `rg` could find 26 lines with credit card numbers in
    a file of almost 1 GB in size! That happened while it was running on an Ubuntu
    VM with 4 vCPUs and 8 GB of RAM. By the way, Filebeat was still up, and my browser
    instances were also disputing CPU and memory with it. Let’s check how it goes
    with AuthN tokens:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 大约 2.5 秒内，`rg` 就能在接近 1 GB 大小的文件中找到 26 行包含信用卡号的数据！这发生在它运行于拥有 4 个虚拟 CPU 和 8 GB
    内存的 Ubuntu 虚拟机上。顺便提一下，Filebeat 仍在运行，我的浏览器实例也在和它争夺 CPU 和内存。接下来我们来看看它在 AuthN token
    上的表现：
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'That was even more insane. Since the regular expression was simpler, it could
    find 100 lines with the pattern in less than 0.5 seconds! As is the case with
    the regular `grep`, `rg` is case-sensitive. The same switch (`-i`) can be used
    to turn this off. You can also combine regular expressions to look for multiple
    patterns at once:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 那更疯狂了。由于正则表达式更简单，它可以在不到 0.5 秒的时间内找到 100 行匹配的模式！和常规的`grep`一样，`rg`是区分大小写的。可以使用相同的开关（`-i`）来关闭此功能。你还可以组合正则表达式，一次查找多个模式：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Everything finished in less than 2 seconds. That’s a win! You can optionally
    integrate `rg` into automation scripts and redirect its output to log files. Carefully
    look at its man page to discover more about this fabulous tool. Next, we are going
    to learn how we can make some tests to detect information leakage.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一切在不到 2 秒钟内完成。这是一次胜利！你可以选择将`rg`集成到自动化脚本中，并将其输出重定向到日志文件中。仔细查看它的手册页，了解更多关于这个神奇工具的信息。接下来，我们将学习如何进行测试，以检测信息泄露。
- en: Testing for information leakage
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信息泄露测试
- en: Cool! So, you had access to a data mass, obtained via data exfiltration, social
    engineering, or any other pentesting technique, and you just learned how to extract
    data from such mass with a few rather nice tools. However, how can you possibly
    test an API endpoint to verify whether it is vulnerable to leaking something you’re
    looking for? That’s what we’re going to see here. It is not redundant to say that
    we are not testing real public API endpoints because we obviously do not have
    access for doing so. Consider the teachings here to be for educational and professional
    purposes only.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 太酷了！所以，你获得了一个数据集，通过数据外泄、社会工程学或其他任何渗透测试技术获得的，现在你刚学会了如何使用几个相当不错的工具从这个数据集中提取数据。那么，如何测试一个
    API 端点，以验证它是否容易泄露你正在寻找的数据呢？这就是我们将在这里看到的内容。需要强调的是，我们并不是在测试真实的公共 API 端点，因为显然我们没有权限这么做。请将这里的教学视为仅用于教育和专业目的。
- en: 'We will use our controlled lab environment to put some API routes to run and
    play with them a bit to understand to which extent they can disclose data that
    is supposed to be protected. The first thing you need to have is the data itself,
    of course. You can either pick a file with dummy data you may already have or
    run the script that follows. This will create 1,000 lines of random data, again
    making use of the `$RANDOM` BASH variable. It will contain user IDs, email addresses,
    credit card numbers, and AuthN tokens:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用受控的实验室环境，运行一些 API 路由并稍作实验，以了解它们可能泄露哪些原本应该受到保护的数据。首先，你需要的数据当然是数据本身。你可以选择一个你可能已经拥有的包含虚拟数据的文件，或者运行以下脚本。这将生成
    1,000 行随机数据，使用 `$RANDOM` BASH 变量。它将包含用户 ID、电子邮件地址、信用卡号和身份验证令牌：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The created file will be a CSV and will look like the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 创建的文件将是 CSV 格式，看起来如下：
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s now implement the API with five routes:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来实现包含五个路由的 API：
- en: '`/users`: An endpoint that exposes sensitive user information without AuthN.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/users`：一个未进行身份验证的端点，暴露敏感的用户信息。'
- en: '`/login`: An endpoint that is vulnerable to SQL injection.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/login`：一个容易受到 SQL 注入攻击的端点。'
- en: '`/profile/<user_id>`: An endpoint with inadequate access control.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/profile/<user_id>`：一个访问控制不足的端点。'
- en: '`/get_sensitive_data`: An endpoint that is vulnerable to data leakage.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/get_sensitive_data`：一个容易发生数据泄露的端点。'
- en: '`/cause_error`: An endpoint that triggers verbose error messages with stack
    traces.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/cause_error`：一个触发详细错误消息并包含堆栈跟踪的端点。'
- en: The code to implement this application is available at [https://github.com/PacktPublishing/Pentesting-APIs/blob/main/chapters/chapter08/api_sensitive_data.py](https://github.com/PacktPublishing/Pentesting-APIs/blob/main/chapters/chapter08/api_sensitive_data.py).
    It was written in Python since that’s one of the main languages we’ve been using
    in this book and since it’s quite trivial and straightforward to understand. The
    pandas framework is used to facilitate the reading of CSV files.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 实现此应用程序的代码可以在[https://github.com/PacktPublishing/Pentesting-APIs/blob/main/chapters/chapter08/api_sensitive_data.py](https://github.com/PacktPublishing/Pentesting-APIs/blob/main/chapters/chapter08/api_sensitive_data.py)找到。它是用
    Python 编写的，因为这是本书中我们使用的主要语言之一，而且它相对简单易懂。使用了 pandas 框架来简化 CSV 文件的读取。
- en: As you already know, this code listens on port TCP/5000 by default. Set it to
    run and let’s play with the endpoints. As this application is vulnerable to some
    threats, you don’t necessarily have to authenticate first to be able to talk to
    the endpoints.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你已经知道的，这段代码默认监听 TCP/5000 端口。启动它，让我们尝试与这些端点互动。由于这个应用程序容易受到一些威胁的攻击，你不一定需要先进行身份验证就能与这些端点交互。
- en: 'Without having access to the code, you’d obviously have to apply the reconnaissance
    techniques that we covered in the second part of this book. However, since you
    do have access to the code, even in a sloppy analysis of it, you will discover
    how weakly this API was purposefully implemented. Going top to down, we can see
    that:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果无法访问代码，你显然需要使用我们在本书第二部分中介绍的侦察技术。然而，由于你可以访问代码，即使只是粗略分析，你也会发现这个 API 的设计存在明显的弱点。自上而下地分析，我们可以看到：
- en: There’s an endpoint that sends back the whole data mass without any previous
    AuthN and AuthZ.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个端点会在没有任何先前的 AuthN 和 AuthZ 的情况下返回全部数据。
- en: The login endpoint is vulnerable to SQL Injection, even in the simplest forms.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 登录端点容易受到 SQL 注入攻击，哪怕是最简单的形式。
- en: The route that gives information about user profiles does not check whether
    the user is authorized to access such information.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供用户资料信息的路由没有检查用户是否有权限访问这些信息。
- en: The penultimate route tries to do some control by looking for an AuthZ token,
    but it’s so simple that the value could be guessed after a few attempts.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 倒数第二条路由试图通过寻找 AuthZ 令牌来进行一些控制，但它的设计过于简单，几次尝试后就可以猜到令牌的值。
- en: Finally, there’s even an endpoint that raises an internal exception, creating
    possibilities to disclose data about the internal infrastructure.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，甚至还有一个端点会引发内部异常，造成泄露内部基础设施数据的可能性。
- en: 'Let’s try them one by one:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一尝试：
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You just got all the users, organized in a JSON format to facilitate being categorized
    afterward. The login endpoint does not actually interface with a SQL database.
    Hence, we won’t be able to simulate an injection attack here, but the spirit remains.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚获得了所有用户的信息，以 JSON 格式组织，便于之后分类。登录端点实际上并未与 SQL 数据库进行交互。因此，我们无法在这里模拟注入攻击，但核心思想仍然存在。
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'What about the route that shows user profiles? It does not require any previous
    AuthZ to check a profile. Let’s try it:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，显示用户资料的路由怎么样呢？它不需要任何先前的 AuthZ 来查看用户资料。我们来试试看：
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You just got another API endpoint that discloses valid information without
    the correct AuthN or AuthZ. Let’s move on with the exercise and explore the one
    that tries to protect the application with an AuthZ token. In this case, we know
    that the token control is a simple Python condition that checks a trivial token
    content, but in a real-world scenario where some NoSQL or in-memory database would
    be in place, we could try a relevant injection attack to bypass the protection:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚发现了另一个 API 端点，在没有正确的 AuthN 或 AuthZ 的情况下暴露了有效的信息。接下来，我们继续练习，探索那个尝试用 AuthZ
    令牌保护应用程序的端点。在这种情况下，我们知道令牌控制是一个简单的 Python 条件判断，它检查一个简单的令牌内容，但在现实世界的场景中，如果使用 NoSQL
    或内存数据库，我们可以尝试进行相关的注入攻击来绕过保护：
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The final route is just there to push a detailed error message to reinforce
    the danger of not treating exceptions and errors when they happen. To know more
    about this, check [*Chapter 6*](B19657_06.xhtml#_idTextAnchor102), where we have
    deep coverage of the subject:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一条路由仅仅是为了发送一条详细的错误信息，强化在异常和错误发生时不加以处理的危险。想了解更多内容，请查阅[*第6章*](B19657_06.xhtml#_idTextAnchor102)，在该章节中我们有深入的讨论：
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: These are some ways to interact with APIs and get access to data that should
    not be directly accessible to a regular user. Moreover, unearthing unintentional
    information disclosure in an API involves a combination of passive and active
    probing methods. You can employ tools to craft diverse inquiries to the API and
    carefully examine the replies for potential leaks. This may encompass inspecting
    hidden data embedded within the response (metadata), error messages that might
    be overly revealing, or specific pieces of information that shouldn’t be readily
    available.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是与 API 交互并访问普通用户不应该直接访问的数据的一些方法。此外，发现 API 中无意泄露的信息需要结合被动和主动的探测方法。你可以使用工具向
    API 发起各种查询，并仔细检查响应，以寻找潜在的数据泄露。这可能包括检查响应中的隐藏数据（元数据）、可能过于泄露的错误信息，或本不应该轻易获取的特定信息。
- en: In a live environment, tools such as Wireshark (or its command-line equivalent,
    `tshark`) may be useful to detect hidden fields or unprotected payloads that,
    once discovered, will most likely reveal what you are looking for. Burp Suite
    or OWASP ZAP also play a part here, and that’s especially true when the traffic
    to or from the API endpoints is encrypted with TLS. In such cases, if you are
    not able to replace the target’s TLS certificate with your own, which would allow
    you to completely see the packets’ contents, you could struggle more to dig into
    the findings. Next, we are going to understand which techniques we can use to
    reduce the chances of data leakage in the world of APIs.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际环境中，像 Wireshark（或其命令行等效工具 `tshark`）这样的工具可能对检测隐藏字段或未保护的有效负载很有帮助。一旦发现这些字段，它们很可能会揭示你所寻找的信息。Burp
    Suite 或 OWASP ZAP 也在此过程中发挥了作用，尤其是在 API 端点的流量使用 TLS 加密时。在这种情况下，如果你不能将目标的 TLS 证书替换为自己的证书（这将允许你完全查看数据包内容），你可能会更难深入分析发现的问题。接下来，我们将了解在
    API 世界中，我们可以使用哪些技术来减少数据泄露的可能性。
- en: Preventing data leakage
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 防止数据泄露
- en: To eliminate or at least reduce the chances of suffering data leakage on your
    API or the application behind it, a multi-layered approach is possibly one of
    the best options. This involves secure coding practices, robust AuthN, and careful
    handling of sensitive information.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了消除或至少减少在 API 或其背后应用程序中发生数据泄露的可能性，采用多层次的方法可能是最佳选择之一。这包括安全的编码实践、强大的身份认证（AuthN）和对敏感信息的谨慎处理。
- en: The first line of defense is secure API design – only create the interfaces
    you need. In other words, only expose the data your API requires to function.
    Avoid open queries that could allow unauthorized access. In GraphQL, tools such
    as query whitelisting act as bouncers, restricting data requests and preventing
    the over-fetching of sensitive information.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 防线的第一步是安全的 API 设计——只创建你需要的接口。换句话说，只有暴露 API 功能所需的数据。避免开放查询，这可能会导致未经授权的访问。在 GraphQL
    中，像查询白名单这样的工具充当了守门员，限制数据请求并防止敏感信息的过度获取。
- en: 'Source code best practices are a vital topic too. When interacting with databases,
    one important point to keep in mind is to use parameterized queries instead of
    simply forwarding what the user provides as input to them. Think of these as pre-prepared
    invitations to the database – they prevent attackers from manipulating the query
    and potentially stealing data (often referred to as SQL injection attacks). An
    example of Python code implementing such queries is available here:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 源代码最佳实践也是一个至关重要的话题。在与数据库交互时，有一个重要的注意点是使用参数化查询，而不是简单地将用户输入的内容转发给数据库。可以将这些视为提前准备好的数据库邀请函——它们防止攻击者操控查询并可能窃取数据（通常被称为
    SQL 注入攻击）。一个实现这种查询的 Python 代码示例如下：
- en: '[PRE16]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Observe the user of a parameterized placeholder (`?`) for the `user_id` field.
    This prevents the possibility that input data provided by the API endpoint’s user
    affects the final SQL database, reducing the chances of injection attacks.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意在 `user_id` 字段上使用参数化占位符（`?`）。这样可以防止 API 端点的用户提供的输入数据影响最终的 SQL 数据库，从而降低注入攻击的可能性。
- en: 'The dynamic couple of AuthN and AuthZ must never be forgotten. APIs should
    use strong mechanisms such as OAuth 2.0 or OpenID Connect to ensure that only
    authorized users can access sensitive endpoints. **JSON Web Tokens** (**JWTs**)
    are like secure invitations – compact and protected, they allow developers to
    control who gets in. In the code block that follows, you can see an implementation
    of JWT in Python with the use of the Flask JWT Extended module:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 必须永远记住身份认证（AuthN）和授权（AuthZ）的动态组合。API 应该使用像 OAuth 2.0 或 OpenID Connect 这样的强大机制，确保只有授权用户可以访问敏感的端点。**JSON
    Web Tokens**（**JWTs**）就像是安全的邀请函——紧凑且受保护，它们允许开发者控制谁可以进入。在接下来的代码块中，你可以看到在 Python
    中使用 Flask JWT Extended 模块实现 JWT 的代码：
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: To be able to access the `/protected` API route, users must present a valid
    JWT token, which is required by the `@``jwt_required()` decorator.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要能够访问 `/protected` API 路由，用户必须提供有效的 JWT 令牌，这是 `@``jwt_required()` 装饰器所要求的。
- en: Data encryption is like a crown jewel. You must apply TLS as much as possible
    in your communication. As a matter of fact, Red Hat OpenStack, which is a private
    cloud offering, uses a concept called **TLS-e** (the **e** stands for **everywhere**),
    which means that internal and public endpoints of the product have TLS enabled,
    guaranteeing traffic encryption. For data at rest, encryption algorithms such
    as AES (with strong key sizes) act as the vault door, safeguarding stored data.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加密就像是皇冠上的宝石。你必须尽可能在通信中应用 TLS。事实上，Red Hat OpenStack（一种私有云解决方案）采用了一种叫做 **TLS-e**
    的概念（**e** 代表 ** everywhere**），这意味着该产品的内部和公共端点都启用了 TLS，从而保证了流量加密。对于静态数据，加密算法如 AES（带有强大的密钥长度）就像保险库的大门，保护存储的数据。
- en: 'Input validation and sanitization offer a subtle yet absolutely inevitable
    shield. Do not simply accept what comes in as valid. When designing or writing
    an API, you should always, always, always think with the mind of a criminal: every
    single line of code or implemented endpoint can be explored in a malicious way.
    Sanitizing user input helps prevent attacks such as SQL injection and **Cross-Site
    Scripting** (**XSS**) that could lead to data leakage if left unchecked. In such
    scenarios, the OWASP **Enterprise Security API** (**ESAPI**) gives a helping hand
    in enforcing security checks.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 输入验证和数据清理提供了一种微妙但绝对不可避免的保护盾。不要轻易接受任何输入为有效。在设计或编写 API 时，你应该始终始终始终以犯罪分子的思维方式来思考：每一行代码或已实现的端点都可能被恶意利用。清理用户输入有助于防止像
    SQL 注入和 **跨站脚本攻击** (**XSS**) 这样的攻击，这些攻击如果不加以控制，可能会导致数据泄露。在这种情况下，OWASP **企业安全 API**
    (**ESAPI**) 可以在执行安全检查时提供帮助。
- en: For GraphQL APIs, preventing the over-fetching of data is crucial. Techniques
    such as query whitelisting and query cost analysis act as portion control measures,
    ensuring that users only retrieve the data they need. The Apollo GraphQL platform
    offers additional security resources and tools for managing and analyzing queries.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 GraphQL API，防止数据过度获取至关重要。像查询白名单和查询成本分析等技术充当着份额控制措施，确保用户仅检索他们所需的数据。Apollo
    GraphQL 平台提供了额外的安全资源和工具，帮助管理和分析查询。
- en: Correct error handling means that you shouldn’t disclose anything that’s not
    strictly necessary to display that an error has happened. Also, catching all possible
    exceptions to avoid an unmapped error can inadvertently disclose internal data
    to the public.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的错误处理意味着你不应该泄露任何显示错误发生的细节之外的信息。此外，捕获所有可能的异常以避免未映射的错误，也可能无意中将内部数据公开给公众。
- en: Finally, logging and monitoring close our layered approach. Properly configured
    logging allows security teams to detect and respond to suspicious activity, while
    monitoring tools act as alarms, alerting administrators to potential breaches
    or unauthorized access. However, it’s important to ensure that logs don’t contain
    sensitive information. Rotate and encrypt them as needed.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，日志记录和监控完成了我们分层的安全策略。适当配置的日志记录可以让安全团队检测和应对可疑活动，而监控工具则充当警报，提醒管理员潜在的安全漏洞或未经授权的访问。然而，重要的是要确保日志中不包含敏感信息。根据需要旋转并加密这些日志。
- en: Summary
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter started the fourth part of the book, covering API advanced topics.
    We learned how to identify when sensitive data is exposed. We also discussed ways
    to test for information leakage on API endpoints (or routes) and finished the
    chapter with general recommendations on why and how such problems could be prevented.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始了本书的第四部分，涵盖了 API 高级话题。我们学习了如何识别敏感数据的暴露。我们还讨论了如何测试 API 端点（或路由）上的信息泄漏，并通过一些通用建议总结了为何以及如何防止此类问题的发生。
- en: At the end of the day, it doesn’t matter whether an API uses a modern programming
    language, has just a few endpoints, and only does specific tasks if the data that
    this API services is not well protected. Data leakage is one of the (if not the
    number-one) most feared problems in cyber incidents when they hit companies, regardless
    of their size.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 归根结底，如果 API 使用的是现代编程语言，端点少且只执行特定任务，但其服务的数据没有得到良好保护，那也毫无意义。在公司遭遇网络攻击时，数据泄露是（如果不是最严重的话）最令人担忧的问题之一，不论公司大小。
- en: In the next chapter, we will finish part four by talking about API abuse and
    general logic tests. It’s nothing less than better understanding the business
    logic behind an API implementation and how failures on it may lead to exploitations
    on the API itself. See you there!
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将完成第四部分的内容，讨论 API 滥用和常见的逻辑测试。这无非是更好地理解 API 实现背后的业务逻辑，以及失败时如何导致 API 本身的漏洞。下次见！
- en: Further reading
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Flog: [https://github.com/mingrammer/flog](https://github.com/mingrammer/flog)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Flog: [https://github.com/mingrammer/flog](https://github.com/mingrammer/flog)'
- en: 'The ELK stack: [https://www.elastic.co/elastic-stack](https://www.elastic.co/elastic-stack)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ELK 堆栈: [https://www.elastic.co/elastic-stack](https://www.elastic.co/elastic-stack)'
- en: 'The maximum map count check problem: [https://www.elastic.co/guide/en/elasticsearch/reference/8.13/_maximum_map_count_check.html](https://www.elastic.co/guide/en/elasticsearch/reference/8.13/_maximum_map_count_check.html)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '最大映射计数检查问题: [https://www.elastic.co/guide/en/elasticsearch/reference/8.13/_maximum_map_count_check.html](https://www.elastic.co/guide/en/elasticsearch/reference/8.13/_maximum_map_count_check.html)'
- en: 'Filebeat, an agent to send logs: [https://www.elastic.co/beats/filebeat](https://www.elastic.co/beats/filebeat)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Filebeat，一个日志发送代理: [https://www.elastic.co/beats/filebeat](https://www.elastic.co/beats/filebeat)'
- en: 'Installing Filebeat on Ubuntu: [https://www.elastic.co/guide/en/beats/filebeat/8.13/setup-repositories.html#_apt](https://www.elastic.co/guide/en/beats/filebeat/8.13/setup-repositories.html#_apt)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '在 Ubuntu 上安装 Filebeat: [https://www.elastic.co/guide/en/beats/filebeat/8.13/setup-repositories.html#_apt](https://www.elastic.co/guide/en/beats/filebeat/8.13/setup-repositories.html#_apt)'
- en: 'KQL: [https://www.elastic.co/guide/en/kibana/current/kuery-query.html](https://www.elastic.co/guide/en/kibana/current/kuery-query.html)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'KQL: [https://www.elastic.co/guide/en/kibana/current/kuery-query.html](https://www.elastic.co/guide/en/kibana/current/kuery-query.html)'
- en: 'Apache Lucene, an open source search engine: [https://lucene.apache.org/](https://lucene.apache.org/)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Apache Lucene，一个开源搜索引擎: [https://lucene.apache.org/](https://lucene.apache.org/)'
- en: 'Exploring regular expressions on Elasticsearch: [https://www.elastic.co/guide/en/elasticsearch/reference/current/regexp-syntax.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/regexp-syntax.html)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '在 Elasticsearch 上探索正则表达式: [https://www.elastic.co/guide/en/elasticsearch/reference/current/regexp-syntax.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/regexp-syntax.html)'
- en: 'Free official Elastic training: [https://www.elastic.co/training/free](https://www.elastic.co/training/free)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '免费的官方 Elastic 培训: [https://www.elastic.co/training/free](https://www.elastic.co/training/free)'
- en: '`rg` tool: [https://github.com/BurntSushi/ripgrep](https://github.com/BurntSushi/ripgrep)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rg` 工具: [https://github.com/BurntSushi/ripgrep](https://github.com/BurntSushi/ripgrep)'
- en: 'The Silver Searcher tool: [https://github.com/ggreer/the_silver_searcher](https://github.com/ggreer/the_silver_searcher)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Silver Searcher 工具: [https://github.com/ggreer/the_silver_searcher](https://github.com/ggreer/the_silver_searcher)'
- en: 'Red Hat OpenStack TLS-e: [https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.2/html/advanced_overcloud_customization/assembly_enabling-ssl-tls-on-overcloud-public-endpoints](https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.2/html/advanced_overcloud_customization/assembly_enabling-ssl-tls-on-overcloud-public-endpoints)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Red Hat OpenStack TLS-e: [https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.2/html/advanced_overcloud_customization/assembly_enabling-ssl-tls-on-overcloud-public-endpoints](https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.2/html/advanced_overcloud_customization/assembly_enabling-ssl-tls-on-overcloud-public-endpoints)'
- en: 'OWASP ESAPI: [https://owasp.org/www-project-enterprise-security-api/](https://owasp.org/www-project-enterprise-security-api/)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OWASP ESAPI: [https://owasp.org/www-project-enterprise-security-api/](https://owasp.org/www-project-enterprise-security-api/)'
