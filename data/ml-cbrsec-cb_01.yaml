- en: Machine Learning for Cybersecurity
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络安全中的机器学习
- en: Inthis chapter, we will cover the fundamental techniques of machine learning.
    We will use these throughout the book to solve interesting cybersecurity problems.
    We will cover both foundational algorithms, such as clustering and gradient boosting
    trees, and solutions to common data challenges, such as imbalanced data and false-positive
    constraints. A machine learning practitioner in cybersecurity is in a unique and
    exciting position to leverage enormous amounts of data and create solutions in
    a constantly evolving landscape.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍机器学习的基本技术。我们将在全书中使用这些技术来解决有趣的网络安全问题。我们将涵盖基础算法，如聚类和梯度提升树，并解决常见的数据挑战，如数据不平衡和假阳性约束。在网络安全领域，机器学习实践者处于一个独特且令人兴奋的位置，能够利用大量数据并在不断发展的环境中创造解决方案。
- en: 'This chapter covers the following recipes:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下内容：
- en: Train-test-splitting your data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练-测试分割你的数据
- en: Standardizing your data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化你的数据
- en: Summarizing large data using **principal component analysis** (**PCA**)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**主成分分析**（**PCA**）总结大型数据
- en: Generating text using Markov chains
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用马尔可夫链生成文本
- en: Performing clustering using scikit-learn
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用scikit-learn进行聚类
- en: Training an XGBoost classifier
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练XGBoost分类器
- en: Analyzing time series using statsmodels
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用statsmodels分析时间序列
- en: Anomaly detection using Isolation Forest
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Isolation Forest进行异常检测
- en: '**Natural language processing** (**NLP**) using hashing vectorizer and tf-idf
    with scikit-learn'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用哈希向量器和tf-idf与scikit-learn进行**自然语言处理**（**NLP**）
- en: Hyperparameter tuning with scikit-optimize
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用scikit-optimize进行超参数调整
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will be using the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下内容：
- en: scikit-learn
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn
- en: Markovify
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Markovify
- en: XGBoost
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost
- en: statsmodels
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: statsmodels
- en: The installation instructions and code can be found at [https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter01](https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter01).
    [](https://github.com/emmanueltsukerman/MLforCSCookbook)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 安装说明和代码可以在[https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter01](https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook/tree/master/Chapter01)找到。
    [](https://github.com/emmanueltsukerman/MLforCSCookbook)
- en: Train-test-splitting your data
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练-测试分割你的数据
- en: In machine learning, our goal is to create a program that is able to perform
    tasks it has never been explicitly taught to perform. The way we do that is to
    use data we have collected to *train* or *fit* a mathematical or statistical model.
    The data used to fit the model is referred to as *training data*. The resulting
    trained model is then used to predict future, previously-unseen data. In this
    way, the program is able to manage new situations without human intervention.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们的目标是创建一个能够执行从未被明确教过的任务的程序。我们实现这一目标的方法是利用我们收集的数据来*训练*或*拟合*一个数学或统计模型。用于拟合模型的数据称为*训练数据*。训练得到的模型随后用于预测未来的、以前未见过的数据。通过这种方式，程序能够在没有人工干预的情况下处理新情况。
- en: One of the major challenges for a machine learning practitioner is the danger
    of *overfitting* – creating a model that performs well on the training data but
    is not able to generalize to new, previously-unseen data. In order to combat the
    problem of overfitting, machine learning practitioners set aside a portion of
    the data, called *test data*, and use it only to assess the performance of the
    trained model, as opposed to including it as part of the training dataset. This
    careful setting aside of testing sets is key to training classifiers in cybersecurity,
    where overfitting is an omnipresent danger. One small oversight, such as using
    only benign data from one locale, can lead to a poor classifier.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习实践者来说，一个主要的挑战是*过拟合*的风险——即创建一个在训练数据上表现良好，但无法对新的、从未见过的数据进行推广的模型。为了应对过拟合问题，机器学习实践者会预留出一部分数据，称为*测试数据*，并仅用于评估训练模型的性能，而不是将其包含在训练数据集中。精心预留测试集是训练网络安全分类器的关键，在这里，过拟合是一个无处不在的危险。一个小小的疏忽，例如仅使用来自某一地区的良性数据，可能导致分类器效果差。
- en: There are various other ways to validate model performance, such as cross-validation.
    For simplicity, we will focus mainly on train-test splitting.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以验证模型性能，比如交叉验证。为了简化，我们将主要关注训练-测试分割。
- en: Getting ready
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Preparation for this recipe consists of installing the scikit-learn and `pandas`
    packages in `pip`. The command for this is as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程的准备工作包括在`pip`中安装scikit-learn和`pandas`包。安装命令如下：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In addition, we have included the `north_korea_missile_test_database.csv` dataset
    for use in this recipe.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还提供了`north_korea_missile_test_database.csv`数据集，以供本教程使用。
- en: How to do it...
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'The following steps demonstrate how to take a dataset, consisting of features
    `X` and labels `y`, and split these into a training and testing subset:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤演示了如何将一个数据集（包含特征`X`和标签`y`）拆分为训练集和测试集：
- en: 'Start by importing the `train_test_split` module and the `pandas` library,
    and read your features into `X` and labels into `y`:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先导入`train_test_split`模块和`pandas`库，并将特征读取到`X`中，将标签读取到`y`中：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, randomly split the dataset and its labels into a training set consisting
    80% of the size of the original dataset and a testing set 20% of the size:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，随机将数据集及其标签拆分为一个训练集（占原始数据集的80%）和一个测试集（占原始数据集的20%）：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We apply the `train_test_split` method once more, to obtain a validation set,
    `X_val` and `y_val`:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们再次应用`train_test_split`方法，以获得一个验证集，`X_val`和`y_val`：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We end up with a training set that's 60% of the size of the original data, a
    validation set of 20%, and a testing set of 20%.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们最终得到了一个训练集，占原始数据的60%，一个验证集占20%，一个测试集占20%。
- en: 'The following screenshot shows the output:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了输出结果：
- en: '![](assets/e07b58f8-015b-4ff0-b98f-3ada859d0136.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/e07b58f8-015b-4ff0-b98f-3ada859d0136.png)'
- en: How it works...
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: We start by reading in our dataset, consisting of historical and continuing
    missile experiments in North Korea. We aim to predict the type of missile based
    on remaining features, such as facility and time of launch. This concludes step
    1\. In step 2, we apply scikit-learn's `train_test_split` method to subdivide
    `X` and `y` into a training set, `X_train` and `y_train`, and also a testing set,
    `X_test` and `y_test`. The `test_size = 0.2` parameter means that the testing
    set consists of 20% of the original data, while the remainder is placed in the
    training set. The `random_state` parameter allows us to reproduce the same *randomly
    generated* split. Next, concerning step 3, it is important to note that, in applications,
    we often want to compare several different models. The danger of using the testing
    set to select the best model is that we may end up overfitting the testing set.
    This is similar to the statistical sin of data fishing. In order to combat this
    danger, we create an additional dataset, called the validation set. We train our
    models on the training set, use the validation set to compare them, and finally
    use the testing set to obtain an accurate indicator of the performance of the
    model we have chosen. So, in step 3, we choose our parameters so that, mathematically
    speaking, the end result consists of a training set of 60% of the original dataset,
    a validation set of 20%, and a testing set of 20%. Finally, we double-check our
    assumptions by employing the `len` function to compute the length of the arrays
    (step 4).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从读取数据集开始，数据集包含朝鲜的历史和持续的导弹实验。我们的目标是根据剩余特征（如设施和发射时间）预测导弹类型。这是步骤1的内容。在步骤2中，我们应用scikit-learn的`train_test_split`方法将`X`和`y`细分为一个训练集`X_train`和`y_train`，以及一个测试集`X_test`和`y_test`。`test_size
    = 0.2`参数表示测试集占原始数据的20%，其余部分放入训练集中。`random_state`参数允许我们复现相同的*随机生成*的拆分。接下来，关于步骤3，需要注意的是，在实际应用中，我们通常希望比较几个不同的模型。使用测试集选择最佳模型的危险在于，我们可能会过度拟合测试集。这类似于数据钓鱼的统计学错误。为了应对这一危险，我们创建了一个额外的数据集，称为验证集。我们在训练集上训练模型，使用验证集进行比较，最后使用测试集来获得我们选择的模型的准确性能指标。因此，在步骤3中，我们选择参数，使得从数学角度来看，最终结果包含60%的训练集，20%的验证集和20%的测试集。最后，我们通过使用`len`函数来计算数组的长度，来仔细检查我们的假设（步骤4）。
- en: Standardizing your data
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准化你的数据
- en: For many machine learning algorithms, performance is highly sensitive to the
    relative scale of features. For that reason, it is often important to *standardize *your
    features. To standardize a feature means to shift all of its values so that their
    mean = 0 and to scale them so that their variance = 1.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多机器学习算法，性能对特征的相对尺度非常敏感。因此，通常需要对特征进行*标准化*。标准化特征意味着将其所有值平移，使其均值为0，并对其进行缩放，使其方差为1。
- en: One instance when normalizing is useful is when featuring the PE header of a
    file. The PE header contains extremely large values (for example, the `SizeOfInitializedData`
    field) and also very small ones (for example, the number of sections). For certain
    ML models, such as neural networks, the large discrepancy in magnitude between
    features can reduce performance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化在某些情况下非常有用，特别是在处理文件的 PE 头信息时。PE 头信息包含极大的数值（例如，`SizeOfInitializedData` 字段），也包含非常小的数值（例如，节区的数量）。对于某些机器学习模型，如神经网络，特征间的巨大差异会降低模型的表现。
- en: Getting ready
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Preparation for this recipe consists of installing the `scikit-learn` and `pandas`
    packages in `pip`. Perform the following steps:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的准备工作包括在 `pip` 中安装 `scikit-learn` 和 `pandas` 包。请执行以下步骤：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In addition, you will find a dataset named `file_pe_headers.csv` in the repository for
    this recipe.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你将在本仓库中找到一个名为 `file_pe_headers.csv` 的数据集，供本示例使用。
- en: How to do it...
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In the following steps, we utilize scikit-learn''s `StandardScaler` method
    to standardize our data:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们使用 scikit-learn 的 `StandardScaler` 方法来标准化数据：
- en: 'Start by importing the required libraries and gathering a dataset, `X`:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入所需的库并收集数据集 `X`：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Dataset `X` looks as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 `X` 如下所示：
- en: '![](assets/dd02cfd2-e3d5-411d-9192-89815651d799.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dd02cfd2-e3d5-411d-9192-89815651d799.png)'
- en: 'Next, standardize `X` using a `StandardScaler` instance:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用 `StandardScaler` 实例对 `X` 进行标准化：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The standardized dataset looks like the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化后的数据集如下所示：
- en: '![](assets/d70b9c6a-48d9-459c-bb08-8f2c50afc2cf.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d70b9c6a-48d9-459c-bb08-8f2c50afc2cf.png)'
- en: How it works...
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We begin by reading in our dataset (step 1), which consists of the PE header
    information for a collection of PE files. These vary greatly, with some columns
    reaching hundreds of thousands of files, and others staying in the single digits.
    Consequently, certain models, such as neural networks, will perform poorly on
    such unstandardized data. In step 2, we instantiate `StandardScaler()` and then
    apply it to rescale `X` using `.fit_transform(X)`. As a result, we obtained a
    rescaled dataset, whose columns (corresponding to features) have a mean of 0 and
    a variance of 1.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从读取数据集开始（步骤 1），该数据集包含一组 PE 文件的 PE 头信息。不同的列差异很大，有些列的数据量达到数十万文件，而有些则只有个位数。因此，某些模型，如神经网络，在处理这些非标准化数据时表现不佳。在步骤
    2 中，我们实例化了`StandardScaler()`，然后应用`.fit_transform(X)`对 `X` 进行重新缩放。最终，我们获得了一个重新缩放的数据集，其中的列（对应特征）的均值为
    0，方差为 1。
- en: Summarizing large data using principal component analysis
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用主成分分析（PCA）对大数据进行总结
- en: 'Suppose that you would like to build a predictor for an individual''s expected
    net fiscal worth at age 45\. There are a huge number of variables to be considered:
    IQ, current fiscal worth, marriage status, height, geographical location, health,
    education, career state, age, and many others you might come up with, such as
    number of LinkedIn connections or SAT scores.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要构建一个预测模型，预测某人在 45 岁时的预期净资产。需要考虑的变量有很多：智商、当前净资产、婚姻状况、身高、地理位置、健康状况、教育背景、职业状态、年龄等等，甚至可能包括
    LinkedIn 连接数量或 SAT 分数等变量。
- en: The trouble with having so many features is several-fold. First, the amount
    of data, which will incur high storage costs and computational time for your algorithm.
    Second, with a large feature space, it is critical to have a large amount of data
    for the model to be accurate. That's to say, it becomes harder to distinguish
    the signal from the noise. For these reasons, when dealing with high-dimensional
    data such as this, we often employ dimensionality reduction techniques, such as
    PCA. More information on the topic can be found at [https://en.wikipedia.org/wiki/Principal_component_analysis](https://en.wikipedia.org/wiki/Principal_component_analysis).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有如此多特征的难题是多方面的。首先，数据量庞大，这将导致高存储成本和计算时间。其次，拥有庞大的特征空间时，模型准确性依赖于大量的数据。也就是说，信号与噪声之间的区别变得更加困难。因此，在处理像这样的高维数据时，我们通常会采用降维技术，例如
    PCA。关于该主题的更多信息，请参考 [https://en.wikipedia.org/wiki/Principal_component_analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)。
- en: PCA allows us to take our features and return a smaller number of new features,
    formed from our original ones, with maximal explanatory power. In addition, since
    the new features are linear combinations of the old features, this allows us to
    anonymize our data, which is very handy when working with financial information,
    for example.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: PCA使我们能够将原始特征转换为较少的新的特征，这些特征是由原始特征组成的，并且具有最大的解释能力。此外，由于新特征是旧特征的线性组合，这使得我们能够对数据进行匿名化处理，这在处理例如金融信息时非常方便。
- en: Getting ready
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The preparation for this recipe consists of installing the scikit-learn and
    `pandas` packages in `pip`. The command for this is as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实例的准备工作包括安装`scikit-learn`和`pandas`包，可以使用`pip`安装。命令如下：
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In addition, we will be utilizing the same dataset, `malware_pe_headers.csv`,
    as in the previous recipe.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将使用与前一个实例相同的数据集，`malware_pe_headers.csv`。
- en: How to do it...
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this section, we''ll walk through a recipe showing how to use PCA on data:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将演示如何在数据上使用PCA的一个实例：
- en: 'Start by importing the necessary libraries and reading in the dataset:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先导入必要的库并读取数据集：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Standardize the dataset, as is necessary before applying PCA:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在应用PCA之前，先标准化数据集：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Instantiate a `PCA` instance and use it to reduce the dimensionality of our
    data:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个`PCA`实例，并使用它来降低我们数据的维度：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Assess the effectiveness of your dimensionality reduction:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估降维的效果：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following screenshot shows the output:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了输出结果：
- en: '![](assets/31a84424-30b5-4159-83a6-ee81ae91fccb.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/31a84424-30b5-4159-83a6-ee81ae91fccb.png)'
- en: How it works...
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'We begin by reading in our dataset and then standardizing it, as in the recipe
    on standardizing data (steps 1 and 2). (It is necessary to work with standardized
    data before applying PCA). We now instantiate a new PCA transformer instance,
    and use it to both learn the transformation (fit) and also apply the transform
    to the dataset, using `fit_transform` (step 3). In step 4, we analyze our transformation.
    In particular, note that the elements of `pca.explained_variance_ratio_` indicate
    how much of the variance is accounted for in each direction. The sum is 1, indicating
    that all the variance is accounted for if we consider the full space in which
    the data lives. However, just by taking the first few directions, we can account
    for a large portion of the variance, while limiting our dimensionality. In our
    example, the first 40 directions account for 90% of the variance:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先读取数据集并进行标准化，方法参照标准化数据的步骤（步骤1和2）。 （在应用PCA之前，必须使用标准化数据）。接下来，我们实例化一个新的PCA转换器，并使用它来进行转换学习（fit）并将转换应用于数据集，使用`fit_transform`（步骤3）。在步骤4中，我们分析我们的转换。特别需要注意的是，`pca.explained_variance_ratio_`的元素表示在每个方向上所占的方差比例。总和为1，表示如果我们考虑数据所在的整个空间，所有的方差都已经被解释。然而，通过仅选择前几个方向，我们就能解释大部分的方差，同时减少维度。在我们的例子中，前40个方向就解释了90%的方差：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This produces the following output:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This means that we can reduce our number of features to 40 (from 78) while preserving
    90% of the variance. The implications of this are that many of the features of
    the PE header are closely correlated, which is understandable, as they are not
    designed to be independent.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们可以将特征的数量从78减少到40，同时保留90%的方差。这意味着PE头部的许多特征是高度相关的，这是可以理解的，因为这些特征并非设计为独立的。
- en: Generating text using Markov chains
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用马尔可夫链生成文本
- en: Markov chains are simple stochastic models in which a system can exist in a
    number of states. To know the probability distribution of where the system will
    be next, it suffices to know where it currently is. This is in contrast with a
    system in which the probability distribution of the subsequent state may depend
    on the past history of the system. This simplifying assumption allows Markov chains
    to be easily applied in many domains, surprisingly fruitfully.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫链是简单的随机模型，其中一个系统可以处于多个状态之一。要知道系统下一个状态的概率分布，只需知道系统当前的状态即可。这与一种系统不同，在这种系统中，后续状态的概率分布可能依赖于系统的过去历史。这个简化假设使得马尔可夫链能够轻松应用于许多领域，并且效果出奇的好。
- en: In this recipe, we will utilize Markov chains to generate fake reviews, which
    is useful for pen-testing a review system's spam detector. In a later recipe,
    you will upgrade the technology from Markov chains to RNNs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实例中，我们将使用马尔可夫链生成虚假评论，这对渗透测试评论系统的垃圾信息检测器非常有用。在后续的实例中，您将把技术从马尔可夫链升级到RNN。
- en: Getting ready
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Preparation for this recipe consists of installing the `markovify` and `pandas`
    packages in `pip`. The command for this is as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱的准备工作包括在`pip`中安装`markovify`和`pandas`包。命令如下：
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In addition, the directory in the repository for this chapter includes a CSV
    dataset, `airport_reviews.csv`, which should be placed alongside the code for
    the chapter.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本章的代码库中包含一个CSV数据集，`airport_reviews.csv`，它应与本章的代码放在一起。
- en: How to do it...
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Let''s see how to generate text using Markov chains by performing the following
    steps:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过执行以下步骤来看看如何使用马尔可夫链生成文本：
- en: 'Start by importing the `markovify` library and a text file whose style we would
    like to imitate:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从导入`markovify`库和我们希望模仿风格的文本文件开始：
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As an illustration, I have chosen a collection of airport reviews as my text:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，我选择了一组机场评论作为我的文本：
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, join the individual reviews into one large text string and build a Markov
    chain model using the airport review text:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将单独的评论合并成一个大的文本字符串，并使用机场评论文本构建一个马尔可夫链模型：
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Behind the scenes, the library computes the transition word probabilities from
    the text.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，库会根据文本计算过渡词的概率。
- en: 'Generate five sentences using the Markov chain model:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用马尔可夫链模型生成五个句子：
- en: '[PRE18]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Since we are using airport reviews, we will have the following as the output
    after executing the previous code:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们使用的是机场评论，执行前面的代码后，我们将得到以下输出：
- en: '[PRE19]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Surprisingly realistic! Although the reviews would have to be filtered down
    to the best ones.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的逼真！尽管这些评论需要筛选出最好的。
- en: 'Generate `3` sentences with a length of no more than `140` characters:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成`3`个句子，每个句子的长度不超过`140`个字符：
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With our running example, we will see the following output:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的示例，我们将看到以下输出：
- en: '[PRE21]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: How it works...
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'We begin the recipe by importing the Markovify library, a library for Markov
    chain computations, and reading in text, which will inform our Markov model (step
    1). In step 2, we create a Markov chain model using the text. The following is
    a relevant snippet from the text object''s initialization code:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从导入Markovify库开始，这个库用于马尔可夫链计算，并读取文本，这将为我们的马尔可夫模型提供信息（步骤1）。在步骤2中，我们使用文本创建马尔可夫链模型。以下是文本对象初始化代码中的相关片段：
- en: '[PRE22]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The most important parameter to understand is `state_size = 2`, which means
    that the Markov chains will be computing transitions between consecutive pairs
    of words. For more realistic sentences, this parameter can be increased, at the
    cost of making sentences appear less original. Next, we apply the Markov chains
    we have trained to generate a few example sentences (steps 3 and 4). We can see
    clearly that the Markov chains have captured the tone and style of the text. Finally,
    in step 5, we create a few `tweets` in the style of the airport reviews using
    our Markov chains.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的参数是`state_size = 2`，这意味着马尔可夫链将计算连续单词对之间的转换。为了生成更逼真的句子，可以增加该参数，但代价是句子看起来不那么原始。接下来，我们应用训练好的马尔可夫链生成一些示例句子（步骤3和4）。我们可以清楚地看到，马尔可夫链捕捉到了文本的语气和风格。最后，在步骤5中，我们使用我们的马尔可夫链生成一些模仿机场评论风格的`推文`。
- en: Performing clustering using scikit-learn
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用scikit-learn进行聚类
- en: '**Clustering** is a collection of unsupervised machine learning algorithms
    in which parts of the data are grouped based on similarity. For example, clusters
    might consist of data that is close together in n-dimensional Euclidean space.
    Clustering is useful in cybersecurity for distinguishing between normal and anomalous
    network activity, and for helping to classify malware into families.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚类**是一类无监督机器学习算法，其中数据的部分被基于相似性进行分组。例如，聚类可能由在n维欧几里得空间中紧密相邻的数据组成。聚类在网络安全中很有用，可以用来区分正常和异常的网络活动，并帮助将恶意软件分类为不同的家族。'
- en: Getting ready
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Preparation for this recipe consists of installing the `scikit-learn`, `pandas`,
    and `plotly` packages in `pip`. The command for this is as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱的准备工作包括在`pip`中安装`scikit-learn`、`pandas`和`plotly`包。命令如下：
- en: '[PRE23]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In addition, a dataset named `file_pe_header.csv` is provided in the repository
    for this recipe.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，仓库中为本食谱提供了一个名为`file_pe_header.csv`的数据集。
- en: How to do it...
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'In the following steps, we will see a demonstration of how scikit-learn''s
    K-means clustering algorithm performs on a toy PE malware classification:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将看到scikit-learn的K-means聚类算法在玩具PE恶意软件分类上的演示：
- en: 'Start by importing and plotting the dataset:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先导入并绘制数据集：
- en: '[PRE24]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following screenshot shows the output:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了输出：
- en: '![](assets/d5b81582-3439-462a-b2fd-90c0cb515aa0.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d5b81582-3439-462a-b2fd-90c0cb515aa0.png)'
- en: 'Extract the features and target labels:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取特征和目标标签：
- en: '[PRE25]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, import scikit-learn''s clustering module and fit a K-means model with
    two clusters to the data:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，导入scikit-learn的聚类模块，并将K-means模型（包含两个聚类）拟合到数据：
- en: '[PRE26]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Predict the cluster using our trained algorithm:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们训练好的算法预测聚类：
- en: '[PRE27]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'To see how the algorithm did, plot the algorithm''s clusters:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了查看算法的表现，绘制算法的聚类结果：
- en: '[PRE28]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The following screenshot shows the output:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了输出：
- en: '![](assets/869390eb-e597-455c-801f-844687dc056e.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/869390eb-e597-455c-801f-844687dc056e.png)'
- en: The results are not perfect, but we can see that the clustering algorithm captured
    much of the structure in the dataset.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 结果虽然不完美，但我们可以看到聚类算法捕捉到了数据集中的大部分结构。
- en: How it works...
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'We start by importing our dataset of PE header information from a collection
    of samples (step 1). This dataset consists of two classes of PE files: malware
    and benign. We then use plotly to create a nice-looking interactive 3D graph (step
    1). We proceed to prepare our dataset for machine learning. Specifically, in step
    2, we set `X` as the features and y as the classes of the dataset. Based on the
    fact that there are two classes, we aim to cluster the data into two groups that
    will match the sample classification. We utilize the K-means algorithm (step 3),
    about which you can find more information at: [https://en.wikipedia.org/wiki/K-means_clustering](https://en.wikipedia.org/wiki/K-means_clustering).
    With a thoroughly trained clustering algorithm, we are ready to predict on the
    testing set. We apply our clustering algorithm to predict to which cluster each
    of the samples should belong (step 4). Observing our results in step 5, we see
    that clustering has captured a lot of the underlying information, as it was able
    to fit the data well.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从一组样本中导入PE头部信息的数据集（步骤1）。该数据集包含两类PE文件：恶意软件和良性文件。然后，我们使用plotly创建一个漂亮的交互式3D图（步骤1）。接着，我们准备好将数据集用于机器学习。具体来说，在步骤2中，我们将`X`设为特征，将y设为数据集的类别。由于数据集有两个类别，我们的目标是将数据分成两个组，以便与样本分类相匹配。我们使用K-means算法（步骤3），有关此算法的更多信息，请参阅：[https://en.wikipedia.org/wiki/K-means_clustering](https://en.wikipedia.org/wiki/K-means_clustering)。在经过充分训练的聚类算法下，我们准备好对测试集进行预测。我们应用聚类算法来预测每个样本应该属于哪个聚类（步骤4）。在步骤5中观察结果时，我们发现聚类捕捉到了大量的潜在信息，因为它能够很好地拟合数据。
- en: Training an XGBoost classifier
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练XGBoost分类器
- en: Gradient boosting is widely considered the most reliable and accurate algorithm
    for generic machine learning problems. We will utilize XGBoost to create malware
    detectors in future recipes.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升被广泛认为是解决一般机器学习问题时最可靠、最准确的算法。我们将在未来的食谱中利用XGBoost来创建恶意软件检测器。
- en: Getting ready
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'The preparation for this recipe consists of installing the scikit-learn, `pandas`,
    and `xgboost` packages in `pip`. The command for this is as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱的准备工作包括在`pip`中安装scikit-learn、`pandas`和`xgboost`包。安装命令如下：
- en: '[PRE29]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In addition, a dataset named `file_pe_header.csv` is provided in the repository
    for this recipe.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，仓库中提供了名为`file_pe_header.csv`的数据集，供本食谱使用。
- en: How to do it...
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In the following steps, we will demonstrate how to instantiate, train, and
    test an XGBoost classifier:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将演示如何实例化、训练和测试XGBoost分类器：
- en: 'Start by reading in the data:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始读取数据：
- en: '[PRE30]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, train-test-split a dataset:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，进行训练-测试数据集划分：
- en: '[PRE31]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Create one instance of an XGBoost model and train it on the training set:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个XGBoost模型实例，并在训练集上训练它：
- en: '[PRE32]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, assess its performance on the testing set:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，评估它在测试集上的表现：
- en: '[PRE33]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The following screenshot shows the output:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了输出：
- en: '![](assets/0697316c-043c-48d1-9a3f-4eee7867f25f.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/0697316c-043c-48d1-9a3f-4eee7867f25f.png)'
- en: How it works...
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We begin by reading in our data (step 1). We then create a train-test split
    (step 2). We proceed to instantiate an XGBoost classifier with default parameters
    and fit it to our training set (step 3). Finally, in step 4, we use our XGBoost
    classifier to predict on the testing set. We then produce the measured accuracy
    of our XGBoost model's predictions.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先读取数据（步骤1）。然后，我们创建一个训练-测试分割（步骤2）。接着，我们实例化一个带有默认参数的XGBoost分类器，并将其拟合到训练集（步骤3）。最后，在步骤4中，我们使用XGBoost分类器对测试集进行预测。然后，我们计算XGBoost模型预测的准确性。
- en: Analyzing time series using statsmodels
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用statsmodels分析时间序列
- en: A time series is a series of values obtained at successive times. For example,
    the price of the stock market sampled every minute forms a time series. In cybersecurity,
    time series analysis can be very handy for predicting a cyberattack, such as an
    insider employee exfiltrating data, or a group of hackers colluding in preparation
    for their next hit.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列是指在连续的时间点上获取的数值序列。例如，股市每分钟的价格构成了一个时间序列。在网络安全领域，时间序列分析对于预测网络攻击非常有用，例如内部员工窃取数据，或一群黑客在为下一次攻击做准备时的行为模式。
- en: Let's look at several techniques for making predictions using time series.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看使用时间序列进行预测的几种技术。
- en: Getting ready
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Preparation for this recipe consists of installing the `matplotlib`, `statsmodels`,
    and `scipy` packages in `pip`. The command for this is as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的准备工作包括在 `pip` 中安装 `matplotlib`、`statsmodels` 和 `scipy` 包。安装命令如下：
- en: '[PRE34]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: How to do it...
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In the following steps, we demonstrate several methods for making predictions
    using time series data:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们展示了几种使用时间序列数据进行预测的方法：
- en: 'Begin by generating a time series:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先生成一个时间序列：
- en: '[PRE35]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Plot your data:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制你的数据：
- en: '[PRE36]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following screenshot shows the output:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了输出结果：
- en: '![](assets/1d04a34d-bfe3-441d-b431-a92458457502.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/1d04a34d-bfe3-441d-b431-a92458457502.png)'
- en: 'There is a large variety of techniques we can use to predict the consequent
    value of a time series:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用多种技术来预测时间序列的后续值：
- en: '**Autoregression** (**AR**):'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自回归**（**AR**）：'
- en: '[PRE37]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '**Moving average** (**MA**):'
  id: totrans-177
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移动平均**（**MA**）：'
- en: '[PRE38]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '**Simple exponential smoothing** (**SES**):'
  id: totrans-179
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单指数平滑**（**SES**）：'
- en: '[PRE39]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The resulting predictions are as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 结果预测如下：
- en: '![](assets/b35f14f9-0bb6-425d-bbc3-c32e1b3be02c.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/b35f14f9-0bb6-425d-bbc3-c32e1b3be02c.png)'
- en: How it works...
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In the first step, we generate a simple toy time series. The series consists
    of values on a line sprinkled with some added noise. Next, we plot our time series
    in step 2\. You can see that it is very close to a straight line and that a sensible
    prediction for the value of the time series at time ![](assets/bb728aba-f0a1-493f-a6d3-c8a52662bd65.png)
    is ![](assets/f8365b1e-32c3-4f44-8eba-a235d3e09405.png). To create a forecast
    of the value of the time series, we consider three different schemes (step 3)
    for predicting the future values of the time series. In an autoregressive model,
    the basic idea is that the value of the time series at time *t* is a linear function
    of the values of the time series at the previous times. More precisely, there
    are some constants, ![](assets/78f5feb2-6573-41aa-a3e9-ace1a6660c99.png), and
    a number, ![](assets/039e0190-78eb-4e7a-ad1c-e30f1f0e6473.png), such that:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我们生成了一个简单的时间序列。该序列由一条线上的值组成，并添加了一些噪声。接下来，在第2步中我们绘制了时间序列。你可以看到它非常接近一条直线，而且对时间点
    ![](assets/bb728aba-f0a1-493f-a6d3-c8a52662bd65.png) 处的时间序列值做出的合理预测是 ![](assets/f8365b1e-32c3-4f44-8eba-a235d3e09405.png)。为了创建时间序列值的预测，我们考虑了三种不同的方案（第3步）来预测时间序列的未来值。在自回归模型中，基本思想是时间序列在时间
    *t* 的值是该时间序列在之前时刻值的线性函数。更准确地说，有一些常数 ![](assets/78f5feb2-6573-41aa-a3e9-ace1a6660c99.png)，以及一个数字
    ![](assets/039e0190-78eb-4e7a-ad1c-e30f1f0e6473.png)，使得：
- en: '![](assets/e6d48f5b-64b0-444d-9eee-4a7e31a2b667.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/e6d48f5b-64b0-444d-9eee-4a7e31a2b667.png)'
- en: As a hypothetical example, ![](assets/dba51c21-ae4d-485d-b878-0f7f04b88c3c.png)
    may be *3*, meaning that the value of the time series can be easily computed from
    knowing its last *3* values.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个假设的例子，![](assets/dba51c21-ae4d-485d-b878-0f7f04b88c3c.png) 可能是 *3*，意味着可以通过知道时间序列的最后
    *3* 个值来轻松计算其值。
- en: 'In the moving-average model, the time series is modeled as fluctuating about
    a mean. More precisely, let ![](assets/176f6dfd-d61f-4a5d-a8bf-7d1447efd15e.png)
    be a sequence of i.i.d normal variables and let ![](assets/1b146731-b7ad-42a1-9901-80df10e48329.png)
    be a constant. Then, the time series is modeled by the following formula:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在移动平均模型中，时间序列被建模为围绕均值波动。更准确地说，设 ![](assets/176f6dfd-d61f-4a5d-a8bf-7d1447efd15e.png)
    是一组独立同分布的正态变量，且设 ![](assets/1b146731-b7ad-42a1-9901-80df10e48329.png) 是常数。那么，时间序列可以通过以下公式建模：
- en: '![](assets/c4c6352b-d94c-4d62-90fe-99519dbea008.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/c4c6352b-d94c-4d62-90fe-99519dbea008.png)'
- en: For that reason, it performs poorly in predicting the noisy linear time series
    we have generated.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它在预测我们生成的嘈杂线性时间序列时表现较差。
- en: 'Finally, in simple exponential smoothing, we propose a smoothing parameter, ![](assets/fe2653e2-e6f5-4665-b6cb-cd0585722359.png).
    Then, our model''s estimate, ![](assets/0b46364e-73ee-499f-8a9a-4ab7013880b4.png),
    is computed from the following equations:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在简单指数平滑中，我们提出一个平滑参数，![](assets/fe2653e2-e6f5-4665-b6cb-cd0585722359.png)。然后，我们模型的估计值![](assets/0b46364e-73ee-499f-8a9a-4ab7013880b4.png)是根据以下公式计算得出的：
- en: '![](assets/5590c81b-0f15-4fd5-bf0b-b9b4dc074eb2.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/5590c81b-0f15-4fd5-bf0b-b9b4dc074eb2.png)'
- en: '![](assets/962f5ffe-7714-4e71-94ea-36e8b861cdfe.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/962f5ffe-7714-4e71-94ea-36e8b861cdfe.png)'
- en: In other words, we keep track of an estimate, ![](assets/bd3be59f-8df1-4602-9425-552842dbd7c4.png),
    and adjust it slightly using the current time series value, ![](assets/9fe675db-c5a5-4b58-9190-28199fb71327.png).
    How strongly the adjustment is made is regulated by the ![](assets/91976144-4a0c-46e8-96e0-9d996cca4b39.png) parameter.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们跟踪一个估计值，![](assets/bd3be59f-8df1-4602-9425-552842dbd7c4.png)，并使用当前的时间序列值![](assets/9fe675db-c5a5-4b58-9190-28199fb71327.png)略微调整它。调整的强度由![](assets/91976144-4a0c-46e8-96e0-9d996cca4b39.png)参数控制。
- en: Anomaly detection with Isolation Forest
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用隔离森林进行异常检测
- en: Anomaly detection is the identification of events in a dataset that do not conform
    to the expected pattern. In applications, these events may be of critical importance.
    For instance, they may be occurrences of a network intrusion or of fraud. We will
    utilize Isolation Forest to detect such anomalies. Isolation Forest relies on
    the observation that it is easy to isolate an outlier, while more difficult to
    describe a normal data point.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测是识别数据集中不符合预期模式的事件。在应用中，这些事件可能至关重要。例如，它们可能是网络入侵或欺诈的发生。我们将利用隔离森林来检测此类异常。隔离森林依赖于一个观察结果：隔离异常值很容易，而描述一个正常数据点则更为困难。
- en: Getting ready
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The preparation for this recipe consists of installing the `matplotlib`, `pandas`,
    and `scipy` packages in `pip`. The command for this is as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 该配方的准备工作包括在`pip`中安装`matplotlib`、`pandas`和`scipy`包。命令如下：
- en: '[PRE40]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: How to do it...
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'In the next steps, we demonstrate how to apply the Isolation Forest algorithm
    to detecting anomalies:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们演示如何应用隔离森林算法来检测异常：
- en: 'Import the required libraries and set a random seed:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库并设置随机种子：
- en: '[PRE41]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Generate a set of normal observations, to be used as training data:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一组正常观测数据，用作训练数据：
- en: '[PRE42]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Generate a testing set, also consisting of normal observations:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个测试集，仍然由正常观测数据组成：
- en: '[PRE43]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Generate a set of outlier observations. These are generated from a different
    distribution than the normal observations:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一组异常观测数据。这些数据来自与正常观测数据不同的分布：
- en: '[PRE44]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Let''s take a look at the data we have generated:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看我们生成的数据：
- en: '[PRE45]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The following screenshot shows the output:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了输出结果：
- en: '![](assets/62e1ed8c-6c95-4fbc-bf15-8f69ca89cb4b.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/62e1ed8c-6c95-4fbc-bf15-8f69ca89cb4b.png)'
- en: 'Now train an Isolation Forest model on our training data:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在在我们的训练数据上训练一个隔离森林模型：
- en: '[PRE46]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s see how the algorithm performs. Append the labels to `X_outliers`:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看算法的表现。将标签附加到`X_outliers`：
- en: '[PRE47]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The following is the output:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '|  | x | y | pred |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  | x | y | pred |'
- en: '| --- | --- | --- | --- |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 3.947504 | 2.891003 | 1 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 3.947504 | 2.891003 | 1 |'
- en: '| 1 | 0.413976 | -2.025841 | -1 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.413976 | -2.025841 | -1 |'
- en: '| 2 | -2.644476 | -3.480783 | -1 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 2 | -2.644476 | -3.480783 | -1 |'
- en: '| 3 | -0.518212 | -3.386443 | -1 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 3 | -0.518212 | -3.386443 | -1 |'
- en: '| 4 | 2.977669 | 2.215355 | 1 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 2.977669 | 2.215355 | 1 |'
- en: 'Let''s plot the Isolation Forest predictions on the outliers to see how many
    it caught:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们绘制隔离森林预测的异常值，看看它捕获了多少：
- en: '[PRE48]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The following screenshot shows the output:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了输出结果：
- en: '![](assets/724ae20b-2325-4b22-8324-849089f48d95.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/724ae20b-2325-4b22-8324-849089f48d95.png)'
- en: 'Now let''s see how it performed on the normal testing data. Append the predicted
    label to `X_test`:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们看看它在正常测试数据上的表现。将预测标签附加到`X_test`：
- en: '[PRE49]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The following is the output:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '|  | x | y | pred |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | x | y | pred |'
- en: '| --- | --- | --- | --- |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 3.944575 | 3.866919 | -1 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 3.944575 | 3.866919 | -1 |'
- en: '| 1 | 2.984853 | 3.142150 | 1 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2.984853 | 3.142150 | 1 |'
- en: '| 2 | 3.501735 | 2.168262 | 1 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 3.501735 | 2.168262 | 1 |'
- en: '| 3 | 2.906300 | 3.233826 | 1 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2.906300 | 3.233826 | 1 |'
- en: '| 4 | 3.273225 | 3.261790 | 1 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 3.273225 | 3.261790 | 1 |'
- en: 'Now let''s plot the results to see whether our classifier labeled the normal
    testing data correctly:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们绘制结果，看看我们的分类器是否正确地标记了正常的测试数据：
- en: '[PRE50]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The following screenshot shows the output:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了输出结果：
- en: '![](assets/60609e2c-972c-4f26-ad58-4000736e81d6.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/60609e2c-972c-4f26-ad58-4000736e81d6.png)'
- en: Evidently, our Isolation Forest model performed quite well at capturing the
    anomalous points. There were quite a few false negatives (instances where normal
    points were classified as outliers), but by tuning our model's parameters, we
    may be able to reduce these.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们的 Isolation Forest 模型在捕捉异常点方面表现得相当不错。尽管存在一些假阴性（正常点被错误分类为异常点），但通过调整模型的参数，我们或许能减少这些问题。
- en: How it works...
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: The first step involves simply loading the necessary libraries that will allow
    us to manipulate data quickly and easily. In steps 2 and 3, we generate a training
    and testing set consisting of normal observations. These have the same distributions.
    In step 4, on the other hand, we generate the remainder of our testing set by
    creating outliers. This anomalous dataset has a different distribution from the
    training data and the rest of the testing data. Plotting our data, we see that
    some outlier points look indistinguishable from normal points (step 5). This guarantees
    that our classifier will have a significant percentage of misclassifications,
    due to the nature of the data, and we must keep this in mind when evaluating its
    performance. In step 6, we fit an instance of Isolation Forest with default parameters
    to the training data.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步简单地加载必要的库，这些库将使我们能够快速、轻松地操作数据。在步骤 2 和 3 中，我们生成一个由正常观察值组成的训练集和测试集。这些数据具有相同的分布。而在步骤
    4 中，我们通过创建异常值来生成其余的测试集。这个异常数据集的分布与训练数据和其余的测试数据不同。绘制我们的数据时，我们看到一些异常点与正常点看起来无法区分（步骤
    5）。这保证了由于数据的性质，我们的分类器将有相当大比例的误分类，在评估其性能时，我们必须记住这一点。在步骤 6 中，我们使用默认参数拟合一个 Isolation
    Forest 实例到训练数据。
- en: Note that the algorithm is fed no information about the anomalous data. We use
    our trained instance of Isolation Forest to predict whether the testing data is
    normal or anomalous, and similarly to predict whether the anomalous data is normal
    or anomalous. To examine how the algorithm performs, we append the predicted labels
    to `X_outliers` (step 7) and then plot the predictions of the Isolation Forest
    instance on the outliers (step 8). We see that it was able to capture most of
    the anomalies. Those that were incorrectly labeled were indistinguishable from
    normal observations. Next, in step 9, we append the predicted label to `X_test`
    in preparation for analysis and then plot the predictions of the Isolation Forest
    instance on the normal testing data (step 10). We see that it correctly labeled
    the majority of normal observations. At the same time, there was a significant
    number of incorrectly classified normal observations (shown in red).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，算法并未接收到任何关于异常数据的信息。我们使用训练好的 Isolation Forest 实例来预测测试数据是正常的还是异常的，类似地，也预测异常数据是正常的还是异常的。为了检查算法的表现，我们将预测标签附加到
    `X_outliers`（步骤 7），然后绘制 Isolation Forest 实例在异常值上的预测（步骤 8）。我们看到它能够捕捉到大部分的异常值。那些被错误标记的异常值与正常观察值无法区分。接下来，在步骤
    9，我们将预测标签附加到 `X_test`，为分析做准备，然后绘制 Isolation Forest 实例在正常测试数据上的预测（步骤 10）。我们看到它正确地标记了大多数正常观察值。与此同时，也有相当数量的正常观察值被错误分类（用红色显示）。
- en: Depending on how many false alarms we are willing to tolerate, we may need to
    fine-tune our classifier to reduce the number of false positives.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们愿意容忍多少误报，我们可能需要对分类器进行微调，以减少假阳性的数量。
- en: Natural language processing using a hashing vectorizer and tf-idf with scikit-learn
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用哈希向量化器和 tf-idf 以及 scikit-learn 进行自然语言处理
- en: We often find in data science that the objects we wish to analyze are textual.
    For example, they might be tweets, articles, or network logs. Since our algorithms
    require numerical inputs, we must find a way to convert such text into numerical
    features. To this end, we utilize a sequence of techniques.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在数据科学中常常发现，我们希望分析的对象是文本。例如，它们可能是推文、文章或网络日志。由于我们的算法需要数值输入，我们必须找到一种方法将这些文本转化为数值特征。为此，我们使用了一系列技术。
- en: A *token* is a unit of text. For example, we may specify that our tokens are
    words, sentences, or characters. A count vectorizer takes textual input and then
    outputs a vector consisting of the counts of the textual tokens. A **hashing vectorizer**
    is a variation on the count vectorizer that sets out to be faster and more scalable,
    at the cost of interpretability and hashing collisions. Though it can be useful,
    just having the counts of the words appearing in a document corpus can be misleading.
    The reason is that, often, unimportant words, such as *the* and *a* (known as
    *stop words*) have a high frequency of occurrence, and hence little informative
    content. For reasons such as this, we often give words different weights to offset
    this. The main technique for doing so is **tf-idf**, which stands for **Term-Frequency,
    Inverse-Document-Frequency**. The main idea is that we account for the number
    of times a term occurs, but discount it by the number of documents it occurs in.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*词元*是文本的一个单位。例如，我们可以指定我们的词元是单词、句子或字符。计数向量化器接受文本输入，然后输出一个包含文本词元计数的向量。**哈希向量化器**是计数向量化器的一种变体，旨在以更快和更可扩展的方式实现，但牺牲了可解释性和哈希冲突。尽管它很有用，仅仅获取文档语料库中出现的单词计数可能会误导。原因是，通常，像*the*和*a*这样的不重要词（称为*停用词*）频繁出现，因此信息含量较低。正因如此，我们通常会为词语赋予不同的权重以抵消这个问题。主要的技术是**tf-idf**，即**词频-逆文档频率**。其主要思想是我们考虑某个词出现的次数，但根据它在多少个文档中出现过来进行折扣。
- en: In cybersecurity, text data is omnipresent; event logs, conversational transcripts,
    and lists of function names are just a few examples. Consequently, it is essential
    to be able to work with such data, something you'll learn in this recipe.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络安全领域，文本数据无处不在；事件日志、对话记录以及函数名列表只是其中的一些例子。因此，能够处理此类数据至关重要，这是你在本食谱中将要学习的内容。
- en: Getting ready
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The preparation for this recipe consists of installing the scikit-learn package
    in `pip`. The command for this is as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 该食谱的准备工作包括在`pip`中安装scikit-learn包。安装命令如下：
- en: '[PRE51]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: In addition, a log file, `anonops_short.log`, consisting of an excerpt of conversations
    taking place on the IRC channel, `#Anonops`, is included in the repository for
    this chapter.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，包含`#Anonops` IRC 频道中对话摘录的日志文件`anonops_short.log`也包含在本章的代码库中。
- en: How to do it…
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何进行…
- en: 'In the next steps, we will convert a corpus of text data into numerical form,
    amenable to machine learning algorithms:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将把一组文本数据转换为数值形式，以便于机器学习算法使用：
- en: 'First, import a textual dataset:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入一个文本数据集：
- en: '[PRE52]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Next, count the words in the text using the hash vectorizer and then perform
    weighting using tf-idf:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用哈希向量化器计算文本中的单词数量，然后使用tf-idf进行加权：
- en: '[PRE53]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The end result is a sparse matrix with each row being a vector representing
    one of the texts:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终结果是一个稀疏矩阵，每一行是一个表示文本之一的向量：
- en: '[PRE54]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The following is the output:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](assets/84d753ae-9201-4079-aaad-ead4c340cd34.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/84d753ae-9201-4079-aaad-ead4c340cd34.png)'
- en: How it works...
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'We started by loading in the #Anonops text dataset (step 1). The Anonops IRC
    channel has been affiliated with the Anonymous hacktivist group. In particular,
    chat participants have in the past planned and announced their future targets
    on Anonops. Consequently, a well-engineered ML system would be able to predict
    cyber attacks by training on such data. In step 2, we instantiated a hashing vectorizer.
    The hashing vectorizer gave us counts of the 1- and 2-grams in the text, in other
    words, singleton and consecutive pairs of words (tokens) in the articles. We then
    applied a tf-idf transformer to give appropriate weights to the counts that the
    hashing vectorizer gave us. Our final result is a large, sparse matrix representing
    the occurrences of 1- and 2-grams in the texts, weighted by importance. Finally,
    we examined the frontend of a sparse matrix representation of our featured data
    in Scipy.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从加载#Anonops文本数据集开始（第1步）。Anonops IRC 频道与匿名黑客活动组织有关。特别地，聊天参与者曾在过去通过Anonops计划和宣布他们未来的目标。因此，一个经过精心设计的机器学习系统，通过对这类数据进行训练，可以预测网络攻击。在第2步中，我们实例化了一个哈希向量化器。哈希向量化器为我们提供了文本中1-gram和2-gram的计数，换句话说，就是单个单词和相邻的两个单词（词元）。然后，我们应用了一个tf-idf转换器，为哈希向量化器提供的计数赋予适当的权重。我们的最终结果是一个大型稀疏矩阵，表示文本中1-gram和2-gram的出现次数，并根据重要性加权。最后，我们检查了在Scipy中展示的稀疏矩阵前端。
- en: Hyperparameter tuning with scikit-optimize
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用scikit-optimize进行超参数调优
- en: In machine learning, a **hyperparameter** is a parameter whose value is set
    before the training process begins. For example, the choice of learning rate of
    a gradient boosting model and the size of the hidden layer of a multilayer perceptron,
    are both examples of hyperparameters. By contrast, the values of other parameters
    are derived via training. Hyperparameter selection is important because it can
    have a huge effect on the model's performance.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，**超参数**是指在训练过程开始之前就已设定的参数。例如，梯度提升模型的学习率选择和多层感知机的隐藏层大小都是超参数的例子。与之对比，其他参数的值是通过训练过程中学习得出的。超参数选择非常重要，因为它可能对模型的表现产生巨大影响。
- en: The most basic approach to hyperparameter tuning is called a **grid search**.
    In this method, you specify a range of potential values for each hyperparameter,
    and then try them all out, until you find the best combination. This brute-force
    approach is comprehensive but computationally intensive. More sophisticated methods
    exist. In this recipe, you will learn how to use *Bayesian optimization* over
    hyperparameters using `scikit-optimize`. In contrast to a basic grid search, in
    Bayesian optimization, not all parameter values are tried out, but rather a fixed
    number of parameter settings is sampled from specified distributions. More details
    can be found at [https://scikit-optimize.github.io/notebooks/bayesian-optimization.html](https://scikit-optimize.github.io/notebooks/bayesian-optimization.html).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的超参数调优方法叫做**网格搜索**。在这种方法中，你为每个超参数指定一组潜在的值，然后尝试所有的组合，直到找到最佳的组合。这个暴力法虽然全面，但计算量大。也有更为复杂的方法。在这个食谱中，你将学习如何使用`scikit-optimize`进行超参数的*贝叶斯优化*。与基本的网格搜索不同，在贝叶斯优化中，并不是尝试所有参数值，而是从指定的分布中抽取一个固定数量的参数设置。更多细节请参见[https://scikit-optimize.github.io/notebooks/bayesian-optimization.html](https://scikit-optimize.github.io/notebooks/bayesian-optimization.html)。
- en: Getting ready
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'The preparation for this recipe consists of installing a specific version of
    `scikit-learn`, installing `xgboost`, and installing `scikit-optimize` in `pip`.
    The command for this is as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱的准备工作包括安装特定版本的`scikit-learn`，安装`xgboost`，以及通过`pip`安装`scikit-optimize`。相关命令如下：
- en: '[PRE55]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: How to do it...
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In the following steps, you will load the standard `wine` dataset and use Bayesian
    optimization to tune the hyperparameters of an XGBoost model:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，你将加载标准的`wine`数据集，并使用贝叶斯优化来调优XGBoost模型的超参数：
- en: 'Load the `wine` dataset from scikit-learn:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从scikit-learn加载`wine`数据集：
- en: '[PRE56]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Import XGBoost and stratified K-fold:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入XGBoost和分层K折交叉验证：
- en: '[PRE57]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Import `BayesSearchCV` from `scikit-optimize` and specify the number of parameter
    settings to test:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`scikit-optimize`导入`BayesSearchCV`并指定要测试的参数设置数量：
- en: '[PRE58]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Specify your estimator. In this case, we select XGBoost and set it to be able
    to perform multi-class classification:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定你的估计器。在这种情况下，我们选择XGBoost，并将其设置为能够执行多类别分类：
- en: '[PRE59]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Specify a parameter search space:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定参数搜索空间：
- en: '[PRE60]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Specify the type of cross-validation to perform:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定要执行的交叉验证类型：
- en: '[PRE61]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Define `BayesSearchCV` using the settings you have defined:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你定义的设置来定义`BayesSearchCV`：
- en: '[PRE62]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Define a `callback` function to print out the progress of the parameter search:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个`callback`函数来输出参数搜索的进度：
- en: '[PRE63]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Perform the parameter search:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行参数搜索：
- en: '[PRE64]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'As you can see, the following shows the output:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，以下显示了输出结果：
- en: '[PRE65]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: How it works...
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: In steps 1 and 2, we import a standard dataset, the `wine` dataset, as well
    as the libraries needed for classification. A more interesting step follows, in
    which we specify how long we would like the hyperparameter search to be, in terms
    of a number of combinations of parameters to try. The longer the search, the better
    the results, at the risk of overfitting and extending the computational time.
    In step 4, we select XGBoost as the model, and then specify the number of classes,
    the type of problem, and the evaluation metric. This part will depend on the type
    of problem. For instance, for a regression problem, we might set `eval_metric
    = 'rmse'` and drop `num_class` together.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤1和2中，我们导入了一个标准数据集——`wine`数据集，以及分类所需的库。接下来的步骤更为有趣，我们指定了想要进行超参数搜索的时间，具体来说，就是指定我们希望尝试多少种参数组合。搜索时间越长，结果通常越好，但也有过拟合和延长计算时间的风险。在步骤4中，我们选择XGBoost作为模型，然后指定类别数量、问题类型和评估指标。这个部分将取决于问题的类型。例如，对于回归问题，我们可能会设置`eval_metric
    = 'rmse'`，并且一同去掉`num_class`。
- en: Other models than XGBoost can be selected with the hyperparameter optimizer
    as well. In the next step, (step 5), we specify a probability distribution over
    each parameter that we will be exploring. This is one of the advantages of using
    `BayesSearchCV` over a simple grid search, as it allows you to explore the parameter
    space more intelligently. Next, we specify our cross-validation scheme (step 6).
    Since we are performing a classification problem, it makes sense to specify a
    stratified fold. However, for a regression problem, `StratifiedKFold` should be
    replaced with `KFold`.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 除了XGBoost之外，超参数优化器还可以选择其他模型。在下一步（第5步）中，我们指定了每个参数的概率分布，这些参数将会被探索。这也是使用`BayesSearchCV`优于简单网格搜索的一个优势，因为它允许你更智能地探索参数空间。接下来，我们指定交叉验证方案（第6步）。由于我们正在进行分类问题，因此指定分层折叠是合理的。然而，对于回归问题，`StratifiedKFold`应该被替换为`KFold`。
- en: Also note that a larger splitting number is preferred for the purpose of measuring
    results, though it will come at a computational price. In step 7, you can see
    additional settings that can be changed. In particular, `n_jobs` allows you to
    parallelize the task. The verbosity and the method used for scoring can be altered
    as well. To monitor the search process and the performance of our hyperparameter
    tuning, we define a callback function to print out the progress in step 8\. The
    results of the grid search are also saved in a CSV file. Finally, we run the hyperparameter
    search (step 9). The output allows us to observe the parameters and the performance
    of each iteration of the hyperparameter search.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，为了测量结果的准确性，较大的划分数更为理想，尽管这会带来计算上的开销。在第7步中，你可以看到一些可以更改的额外设置。特别地，`n_jobs`允许你并行化任务。输出的详细程度以及评分方法也可以进行调整。为了监控搜索过程和我们超参数调优的性能，我们在第8步定义了一个回调函数，用于打印出进度。网格搜索的结果也会保存在CSV文件中。最后，我们运行超参数搜索（第9步）。输出结果让我们能够观察每次超参数搜索迭代的参数和性能。
- en: In this book, we will refrain from tuning the hyperparameters of classifiers.
    The reason is in part brevity, and in part because hyperparameter tuning here
    would be *premature optimization*, as there is no specified requirement or goal
    for the performance of the algorithm from the end user. Having seen how to perform
    it here, you can easily adapt this recipe to the application at hand.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将避免调整分类器的超参数。原因部分是为了简洁，部分原因是因为在这里进行超参数调优会是*过早优化*，因为从最终用户的角度来看，算法的性能并没有特定的要求或目标。既然我们已经展示了如何进行调优，你可以轻松地将这个方法应用于当前的任务。
- en: Another prominent library for hyperparameter tuning to keep in mind is `hyperopt`.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要记住的用于超参数调优的著名库是`hyperopt`。
